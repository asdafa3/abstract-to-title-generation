{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFG36yZqFvVq"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "%cd \"/content/drive/MyDrive/DL4NLP/abstract-to-title-generation/\"\n",
    "\n",
    "from config import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUMCr2qkFvVt"
   },
   "outputs": [],
   "source": [
    "!pip install -r \"{PROJECT_ROOT}/requirements.txt\" &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzuxREQMFvVu"
   },
   "outputs": [],
   "source": [
    "sys.path.append(f'{PROJECT_ROOT}')\n",
    "sys.path.append(f'{PROJECT_ROOT}/src')\n",
    "sys.path.append(f'{PROJECT_ROOT}/deps/BARTScore')\n",
    "sys.path.append(f'{PROJECT_ROOT}/deps/bert_score')\n",
    "sys.path.append(f'{PROJECT_ROOT}/RL_bart_xsum/trl/trl')\n",
    "sys.path.append(f'{PROJECT_ROOT}/deps/emnlp19-moverscore-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNtE5_DvFvVv"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, BartTokenizer, BartModel, BartForConditionalGeneration, BartConfig, GPT2Config,GPT2LMHeadModel\n",
    "from transformers import top_k_top_p_filtering, GPT2Model\n",
    "from transformers import GPT2Tokenizer, AutoModel, BartTokenizer, AutoTokenizer\n",
    "from transformers import BertModel, BertPreTrainedModel, AutoConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from bart_xsum import ValueHead\n",
    "import ppo\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from threading import active_count\n",
    "import sys\n",
    "from bart_score import BARTScorer\n",
    "import bert_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import model_utils\n",
    "import dataset_utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-yglkeYFvVw"
   },
   "outputs": [],
   "source": [
    "# pull data only pulls changed data\n",
    "#!dvc pull -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACriYR8EFvVx"
   },
   "source": [
    "## Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3674kouFvVz"
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.backends.cudnn.deteministic = True\n",
    "setup_seed(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVxc6GA-FvV0"
   },
   "outputs": [],
   "source": [
    "config = BartConfig('facebook/bart-large-xsum', output_hidden_states=True)\n",
    "model_name = f\"{MODEL_DIR}/BART-XSum-humor/\"\n",
    "\n",
    "print(DEVICE_ID)\n",
    "\n",
    "#load preptrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_hidden_states=True).to(DEVICE_ID)\n",
    "#load reference model\n",
    "ref_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_hidden_states=True).to(DEVICE_ID)\n",
    "\n",
    "#load reward tokenizer\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "#load model tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "\n",
    "#load reward model\n",
    "reward_model_dir, reward_model_name, generate_humor, gamma, lam, cliprange = {\n",
    "\n",
    "    0: (\"evaluation_models/reward_model\", \"finetuned_size140_lr4.0638e-05_ep16_2022-08-18__11_34_24_final\", 2, 0.99, 0.85, 0.5),\n",
    "    1: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 0.99, 0.85, 0.5),\n",
    "\n",
    "    2: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 0, 0.99, 0.85, 0.5),\n",
    "    3: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 1, 0.99, 0.85, 0.5),\n",
    "\n",
    "    4: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 1.0, 0.85, 5.0),\n",
    "    5: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", [0, 1, 2], 0.99, 0.85, 3.5),\n",
    "    6: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 0.99, 0.85, 5.0),\n",
    "    7: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 0.98, 0.85, 5.0),\n",
    "    8: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 0.99, 0.85, 0.5),\n",
    "    \n",
    "    9: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 0.99, 0.9, 0.5),\n",
    "    10: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 0.995, 0.85, 1.0),\n",
    "    11: (\"evaluation_models/reward_model\", \"finetuned_size230_lr4.0638e-05_ep16_2022-08-17__16_44_40_final\", 2, 0.99, 0.85, 0.5),\n",
    "    12: (\"evaluation_models/reward_model\", \"only_quality_finetuned_size80_lr4.0638e-05_ep20_2022-08-18__21_29_20_final\", None, 0.985, 0.85, 3.5),\n",
    "    13: (\"evaluation_models/reward_model\", \"only_quality_finetuned_size140_lr4.0638e-05_ep20_2022-08-18__15_05_43_final\", None, 0.985, 0.85, 3.5),\n",
    "    14: (\"evaluation_models/reward_model\", \"only_quality_finetuned_size230_lr4.0638e-05_ep20_2022-08-18__15_17_48_final\", None, 0.985, 0.85, 3.5),\n",
    "    15: (\"evaluation_models/reward_model\", \"only_quality_finetuned_size230_lr4.0638e-05_ep20_2022-08-18__15_17_48_final\", None, 0.985, 0.85, 5.0)\n",
    "}[14]\n",
    "\n",
    "reward_aconfig = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "if generate_humor is not None:\n",
    "  reward_model = model_utils.HumorBertRegresser.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "  model_state = torch.load(f\"{PROJECT_ROOT}/{reward_model_dir}/{reward_model_name}/model.pth\")\n",
    "\n",
    "  reward_tokenizer, reward_model = dataset_utils.add_humor_token(reward_tokenizer, reward_model)\n",
    "  tokenizer, model = dataset_utils.add_humor_token(tokenizer, model)\n",
    "\n",
    "  ref_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "  _, ref_model = dataset_utils.add_humor_token(ref_tokenizer, ref_model)\n",
    "else:\n",
    "  reward_model = model_utils.BertRegresser.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "  model_state = torch.load(f\"{PROJECT_ROOT}/reward_model/checkpoint\")[0]\n",
    "  \n",
    "reward_model.load_state_dict(model_state)\n",
    "reward_model.to(DEVICE_ID)\n",
    "#load Valuehead\n",
    "hmodel = ValueHead(config).to(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txz_sYwEFvV2"
   },
   "outputs": [],
   "source": [
    "# load annotated data\n",
    "anno_sample = pd.read_json(f\"{DATA_DIR}/annotated/dataset_230samples.json\")\n",
    "\n",
    "def create_tupel(sample_np, humor):\n",
    "    ab = [\n",
    "        (f\"[humor={humor}]\" if humor is not None else \"\") + sample_np[0]\n",
    "    ]*6\n",
    "    ht = [sample_np[1]]*6\n",
    "    indices = np.array([2, 5, 8, 11, 14, 17])\n",
    "    sysms = list(sample_np[2].keys())\n",
    "    sysms = ['original'] + sysms\n",
    "    gents = list(sample_np[2].values())\n",
    "    gents = [sample_np[1]] + gents\n",
    "    gents_scores = list(sample_np[3].values())\n",
    "    gents_scores = [gents_scores[i] for i in indices]\n",
    "    max_idx = gents_scores.index(max(gents_scores))\n",
    "    res = np.transpose(np.array([ab, ht, gents, gents_scores, sysms]))\n",
    "    return res[max_idx,:].reshape(1,5).tolist(), res.tolist()\n",
    "\n",
    "\n",
    "res = []\n",
    "res1 = []\n",
    "for row in anno_sample.to_numpy():\n",
    "  \n",
    "  if type(generate_humor) == list:\n",
    "    humor = random.choice(generate_humor)\n",
    "  elif generate_humor is not None:\n",
    "    humor = generate_humor\n",
    "  else:\n",
    "    humor = None\n",
    "\n",
    "  r1, r2 = create_tupel(row, humor)\n",
    "  res += r1\n",
    "  res1 += r2\n",
    "gen_title_score_pairs = np.array(res1)\n",
    "gen_title_score_pairs_bestone = np.array(res)\n",
    "gen_title_score_pairs_bestone[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_QE1f_jFvV3"
   },
   "outputs": [],
   "source": [
    "gen_title_score_pairs_bad = []\n",
    "scores = gen_title_score_pairs_bestone[:,-2]\n",
    "scores = np.array([float(s) for s in scores])\n",
    "for row in gen_title_score_pairs_bestone:\n",
    "  if float(row[-2]) <= np.mean(scores):\n",
    "    gen_title_score_pairs_bad.append(row)\n",
    "\n",
    "gen_title_score_pairs_bad = np.array(gen_title_score_pairs_bad)\n",
    "\n",
    "\n",
    "gen_title_score_pairs_xsum = []\n",
    "for row in gen_title_score_pairs:\n",
    "  if row[-1] == 'bart_xsum':\n",
    "    gen_title_score_pairs_xsum.append(row)\n",
    "\n",
    "gen_title_score_pairs_xsum = np.array(gen_title_score_pairs_xsum)\n",
    "\n",
    "gen_title_score_pairs_good = []\n",
    "scores = gen_title_score_pairs_bestone[:,-2]\n",
    "scores = np.array([float(s) for s in scores])\n",
    "for row in gen_title_score_pairs_bestone:\n",
    "  if float(row[-2]) >= np.mean(scores) or row[-1] == 'bart_xsum':\n",
    "    gen_title_score_pairs_good.append(row)\n",
    "\n",
    "gen_title_score_pairs_good = np.array(gen_title_score_pairs_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BsO2vRPfOGK"
   },
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVDXDgcEFvV4"
   },
   "outputs": [],
   "source": [
    "\"\"\"# **2 Train Mode (Cross learning/ Default RL)**\"\"\"\n",
    "gen_kwargs = {\n",
    "    \"min_length\":-1,\n",
    "    \"top_k\": 2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    #\"length_penalty\" : -20.0,\n",
    "    #\"num_return_sequences\" : 5,\n",
    "    #\"repetition_penalty\" : 1.5,\n",
    "}\n",
    "\n",
    "def generate_reward(tokenizer, query):\n",
    "    reward_encode = reward_tokenizer(query, return_tensors='pt').to('cuda')\n",
    "    reward = reward_model(reward_encode['input_ids'], reward_encode['attention_mask']).squeeze()\n",
    "    return reward.norm().to('cuda')\n",
    "\n",
    "def ACT_step(title_score_pairs, tokenizer, model, ppo_trainer):\n",
    "  titles = []\n",
    "  for i in range(len(title_score_pairs)):\n",
    "    row = title_score_pairs[i]\n",
    "    #model_name = row[4]\n",
    "    ref_title = row[2]\n",
    "    abstract = row[0]\n",
    "    #print(query_txt)\n",
    "    query = '[CLS]' + abstract + '[SEP]'\n",
    "    \n",
    "    human_score = float(row[3])\n",
    "    human_score_tensor = torch.tensor([human_score]).to('cuda')\n",
    "    ab_tensor = tokenizer(abstract, return_tensors=\"pt\").to('cuda')\n",
    "    #ref_title_tensor = tokenizer(ref_title, return_tensors='pt').to('cuda')\n",
    "\n",
    "    ## define a reward for response\n",
    "    ref_reward = generate_reward(reward_tokenizer, query + ref_title).cpu().item()\n",
    "\n",
    "    ## train model with ppo\n",
    "    #train_stats = ppo_trainer.step(ab_tensor[\"input_ids\"], ref_title_tensor[\"input_ids\"], ref_reward)#human_score_tensor)\n",
    "\n",
    "    # generate title\n",
    "    gen_title_tensor = model.generate(\n",
    "        input_ids = ab_tensor[\"input_ids\"],\n",
    "        attention_mask = ab_tensor[\"attention_mask\"],\n",
    "        max_new_tokens=40,\n",
    "        **gen_kwargs\n",
    "    )[-40:].to('cuda')\n",
    "\n",
    "    gen_title = tokenizer.batch_decode(\n",
    "        gen_title_tensor,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    #test ref model response(generated title from fine-tunde bart-xsum)\n",
    "    # define a reward for response\n",
    "    \n",
    "    reward = generate_reward(reward_tokenizer, query + gen_title).cpu().item()\n",
    "\n",
    "    # train model with ppo\n",
    "    train_stats = ppo_trainer.step(ab_tensor[\"input_ids\"], gen_title_tensor, reward)\n",
    "\n",
    "    if gen_title.strip() != ref_title.strip():\n",
    "        print(f'Human title:                                     {row[1]}')\n",
    "        print(f'Reference:                                       {ref_title}\\nHuman Score: {human_score}\\nReward: {ref_reward}')\n",
    "        print(f'Generated reference title:                       {gen_title}\\n({reward})')\n",
    "        print('- - - -'*20 + '>')\n",
    "    \n",
    "    titles.append((gen_title, reward))\n",
    "  return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMWMRxTCD8J4"
   },
   "outputs": [],
   "source": [
    "def generate_titles(title_score_pairs, tokenizer, model, titles):\n",
    "  scores = []\n",
    "  gen_titles = []\n",
    "  for i in range(len(title_score_pairs)):\n",
    "    title, reward = titles[i]\n",
    "    row = title_score_pairs[i]\n",
    "    #model_name = row[4]\n",
    "    ref_title = row[2]\n",
    "    abstract = row[0]\n",
    "    #print(query_txt)\n",
    "    query = '[CLS]' + abstract + '[SEP]'\n",
    "    \n",
    "    human_score = float(row[3])\n",
    "    human_score_tensor = torch.tensor([human_score]).to('cuda')\n",
    "    ab_tensor = tokenizer(abstract, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    gen_title_ids = model.generate(\n",
    "        input_ids = ab_tensor[\"input_ids\"],\n",
    "        attention_mask = ab_tensor[\"attention_mask\"],\n",
    "        max_new_tokens=40,\n",
    "        **gen_kwargs\n",
    "    )[-40:].to('cuda')\n",
    "\n",
    "    gen_title_ids = gen_title_ids.cpu().data.numpy()\n",
    "    gen_title_txt = tokenizer.batch_decode(\n",
    "        gen_title_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    gen_reward = generate_reward(reward_tokenizer, query + gen_title_txt).cpu().item()\n",
    "\n",
    "    if gen_title_txt.strip() != title.strip():\n",
    "        print(f'Generated reference title:                       {title}\\n({reward})')\n",
    "        print(f'RL-rewarded-after-step bart_xsum generated title: {gen_title_txt}\\nReward: {gen_reward}')\n",
    "        #print([li for li in difflib.ndiff(gen_title, new_title_txt) if li[0] != ' '])\n",
    "        print('- - - -'*20 + '>')\n",
    "    gen_titles.append((i, abstract, title, gen_title_txt))\n",
    "    scores.append((reward, gen_reward))\n",
    "  return gen_titles, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt5PrB67FvV8"
   },
   "source": [
    "# test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqAPG6e0FvV8"
   },
   "outputs": [],
   "source": [
    "\"\"\"# **Train Settings and Do Train**\"\"\"\n",
    "\n",
    "# initialize trainer\n",
    "#ppo_config = {'batch_size': 1, 'forward_batch_size': 1}\n",
    "new_titles = None\n",
    "gen_titles = None\n",
    "scores = None\n",
    "torch.cuda.empty_cache()\n",
    "ppo_config = {\n",
    "    \"lr\": 2e-6,#1.41e-5,#6e-7,#\"lr\": 3e-6,#,\n",
    "    \"adap_kl_ctrl\": True,\n",
    "    \"init_kl_coef\": 0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\": 10000,\n",
    "    \"gamma\": gamma, #0.99,\n",
    "    \"lam\": lam, #0.95,\n",
    "    \"cliprange\": cliprange, #0.5,\n",
    "    \"cliprange_value\": cliprange, #0.5,\n",
    "    \"vf_coef\": .1,\n",
    "    \"batch_size\": 1,\n",
    "    \"forward_batch_size\": 1,\n",
    "    \"ppo_epochs\": 4,\n",
    "}\n",
    "ppo_trainer = ppo.PPOTrainer(model, ref_model, hmodel, **ppo_config)\n",
    "new_titles = ACT_step(gen_title_score_pairs_bestone, tokenizer, model, ppo_trainer)\n",
    "\n",
    "#res = RL_steps(gen_title_score_pairs_bad, 0, len(gen_title_score_pairs_bad), tokenizer, model, ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVYZjMVIF2TL"
   },
   "outputs": [],
   "source": [
    "gen_titles, scores = generate_titles(gen_title_score_pairs_bestone, tokenizer, model, new_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Bx5OQfbEhXf"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "\n",
    "dir_path = f\"{OUTPUT_DIR}/generated_titles/{reward_model_name}_humor_{generate_humor}_ppo_{gamma}_{lam}_{cliprange}_/\"\n",
    "if os.path.isdir(dir_path):\n",
    "  for p, _, fs in os.walk(dir_path, topdown=False):\n",
    "    for f in fs: # delete files anywhere in dirs tree\n",
    "      os.remove(f\"{p}/{f}\")\n",
    "else:\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "def accum(acc, s):\n",
    "  new = acc[0] + s[1] - s[0]\n",
    "  return (new, acc[1] + [new])\n",
    "  \n",
    "plt.plot(functools.reduce(accum, scores, (0.0, []))[1])\n",
    "plt.savefig(f\"{dir_path}/learning_curve.png\")\n",
    "\n",
    "df = pd.DataFrame(scores, columns=[\"ref_reward\", \"new_reward\"])\n",
    "df.to_csv(f\"{dir_path}/scores.csv\")\n",
    "\n",
    "df = pd.DataFrame(gen_titles, columns=[\"index\", \"abstract\", \"ref_title\", \"new_title\"])\n",
    "df.to_csv(f\"{dir_path}/titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itRDqTm7FvV9"
   },
   "outputs": [],
   "source": [
    "query_txt = \"This bachelor thesis explores the generation of title based on a given abstract using neural language model. Recently, neural language models have been used in many scenarios with practical applications. For example, in scientific writing, automatic summary generation from long texts is used to assist in the reading and selection of relevant scientific articles. Title is an important part of scientific article, but the title generation using neural language and optimization for neural language model based on human preferences are less studied. This thesis addresses this gap and presents an optimized model based on state-of-the-art pre-trained neural language model which generate human-preferred titles from a given abstract. The model is fine-tuned on datasets of scientific article and optimized from human preferences using the novel learning perspective in reinforcement learning environment. The result shows that, the neural language model have powerful capabilities on the abstract-to-title task and the reinforcement learning approach is effective in scalable learning of neural language model.\"\n",
    "\n",
    "query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86kCE3HTFvV9"
   },
   "outputs": [],
   "source": [
    "ref_tensor=model.generate(\n",
    "    input_ids = query_tensor[\"input_ids\"],\n",
    "    attention_mask = query_tensor[\"attention_mask\"],\n",
    "    max_new_tokens=40,\n",
    "    **gen_kwargs\n",
    ")[-40:].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOn2nZbDFvV-"
   },
   "outputs": [],
   "source": [
    "ref_tensor = model.generate(\n",
    "      input_ids = query_tensor[\"input_ids\"],\n",
    "      attention_mask = query_tensor[\"attention_mask\"],\n",
    "      max_length = 40,\n",
    "      num_beams = 5,\n",
    "      num_return_sequences = 1,\n",
    "      repetition_penalty=2.0,\n",
    "      length_penalty=10.0,\n",
    "      early_stopping = True,\n",
    "    ).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIYUpjnuFvV-"
   },
   "outputs": [],
   "source": [
    "ref_txt = tokenizer.batch_decode(ref_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "ref_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSJRDQazFvV-"
   },
   "source": [
    "\n",
    "# **Generation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6D-TPqFFvV_"
   },
   "outputs": [],
   "source": [
    "outres = []\n",
    "for row in gen_title_score_pairs_bestone:\n",
    "  ab = row[0]\n",
    "  ht = row[1]\n",
    "  ogt = row[2]\n",
    "  query_txt = ab\n",
    "  query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')\n",
    "  #get ref model response\n",
    "  '''ref_tensor = ref_model.generate(\n",
    "              input_ids = query_tensor[\"input_ids\"],\n",
    "              attention_mask = query_tensor[\"attention_mask\"],\n",
    "              max_length = 30,\n",
    "              num_beams = 5,\n",
    "              num_return_sequences = 1,\n",
    "              repetition_penalty=2.0, \n",
    "              length_penalty=10.0,\n",
    "              early_stopping = True,\n",
    "              ).to('cuda')'''\n",
    "\n",
    "  ref_tensor=ref_model.generate(input_ids = query_tensor[\"input_ids\"], \n",
    "                          attention_mask = query_tensor[\"attention_mask\"], \n",
    "                          max_new_tokens=40,\n",
    "                          **gen_kwargs)[-40:].to('cuda')\n",
    "  ref_txt = tokenizer.batch_decode(ref_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "  \n",
    "  # get model response\n",
    "  '''response_tensor = model.generate(\n",
    "              input_ids = query_tensor[\"input_ids\"],\n",
    "              attention_mask = query_tensor[\"attention_mask\"],\n",
    "              max_length = 30,\n",
    "              num_beams = 5,\n",
    "              num_return_sequences = 1,\n",
    "              repetition_penalty=2.0, \n",
    "              length_penalty=10.0,\n",
    "              early_stopping = True,\n",
    "              ).to('cuda')'''\n",
    "  response_tensor = model.generate(\n",
    "      input_ids = query_tensor[\"input_ids\"],\n",
    "      attention_mask = query_tensor[\"attention_mask\"],\n",
    "      max_new_tokens=40,\n",
    "      **gen_kwargs\n",
    "    )[-40:].to('cuda')\n",
    "\n",
    "  response_txt = tokenizer.batch_decode(response_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "  outres.append([ab, ht,ogt, ref_txt, response_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vx3d_VpdFvV_"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(outres)\n",
    "df.columns = ['abstract', 'original title','best title' , 'generated title before RL', 'generated title after RL']\n",
    "path=f'{OUTPUT_DIR}\\act_'+ \"{:.9f}\".format(ppo_config['lr'])+'_'+str(ppo_config['ppo_epochs'])+'.csv'\n",
    "#df = df.drop(['original title'], axis=1)\n",
    "#df.columns = ['abstract', 'best title', 'title-xsum', 'title-xsum-reward']\n",
    "df_np = df.to_numpy()\n",
    "\n",
    "n_np = []\n",
    "for row in df_np:\n",
    "  ab = row[0]\n",
    "  ts = row[1:]\n",
    "  #ts[0] = ts[0]\n",
    "  #ts[0] = ts[0]\n",
    "  #ts = np.random.permutation(ts)\n",
    "  n_np.append([ab, ts[1], ts[2]])\n",
    "n_np = np.array(n_np)\n",
    "se = pd.DataFrame(n_np[:30])\n",
    "se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_f1xafrFvV_"
   },
   "source": [
    "# **Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozQBafmgFvWA"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/Thesis/BARTScore\n",
    "\n",
    "bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-xsum')\n",
    "\n",
    "\n",
    "from moverscore_v2 import get_idf_dict, word_mover_score\n",
    "from typing import List, Union, Iterable\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def sentence_score(hypothesis: str, references: List[str], trace=0):\n",
    "    \n",
    "    idf_dict_hyp = defaultdict(lambda: 1.)\n",
    "    idf_dict_ref = defaultdict(lambda: 1.)\n",
    "    \n",
    "    hypothesis = [hypothesis] * len(references)\n",
    "    \n",
    "    sentence_score = 0 \n",
    "\n",
    "    scores = word_mover_score(references, hypothesis, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=False)\n",
    "    \n",
    "    sentence_score = np.mean(scores)\n",
    "    \n",
    "    if trace > 0:\n",
    "        print(hypothesis, references, sentence_score)\n",
    "            \n",
    "    return sentence_score\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoPws2EwFvWA"
   },
   "outputs": [],
   "source": [
    "abs = df['abstract'].to_list()\n",
    "ots = df['original title'].to_list()\n",
    "bts = df['best title'].to_list()\n",
    "ogts = df['generated title before RL'].to_list()\n",
    "rlts = df['generated title after RL'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B_HQFflFvWA"
   },
   "outputs": [],
   "source": [
    "#BartScore\n",
    "bart_scores = [bart_scorer.score(abs, ts, batch_size=1) for ts in [ots, ogts, rlts]]\n",
    "\n",
    "avg_bart = np.array(bart_scores).mean(axis=1)\n",
    "print('abs_ots: ', avg_bart[0])\n",
    "print('abs_ogts: ', avg_bart[1])\n",
    "print('abs_rlts: ', avg_bart[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2x_jzV_pfT5"
   },
   "outputs": [],
   "source": [
    "#MoverScore\n",
    "mover_scores = [[sentence_score(t, [ab]) for t,ab in zip(ts, abs)] for ts in [ots, ogts, rlts]]\n",
    "avg_mover = np.array(mover_scores).mean(axis=1)\n",
    "print('abs_ots: ', avg_mover[0])\n",
    "print('abs_ogts: ', avg_mover[1])\n",
    "print('abs_rlts: ', avg_mover[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9paukKEGFvWB"
   },
   "outputs": [],
   "source": [
    "#BertScore\n",
    "bert_scores = [bert_score.score(ts, abs, lang=\"en\")[2].tolist() for ts in [ots, ogts, rlts]]\n",
    "print(bert_scores)\n",
    "avg_bert = np.array(bert_scores).mean(axis=1)\n",
    "\n",
    "print('abs_ots: ', avg_bert[0])\n",
    "print('abs_ogts: ', avg_bert[1])\n",
    "print('abs_rlts: ', avg_bert[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YD8L2V7fZAv"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array(bart_scores).T, columns=[\"original title\", \"generated title before RL\", \"generated title before RL\"])\n",
    "df.to_csv(f\"{dir_path}/bart_scores.csv\")\n",
    "df = pd.DataFrame(np.array(bert_scores).T, columns=[\"original title\", \"generated title before RL\", \"generated title before RL\"])\n",
    "df.to_csv(f\"{dir_path}/bert_scores.csv\")\n",
    "df = pd.DataFrame(np.array(mover_scores).T, columns=[\"original title\", \"generated title before RL\", \"generated title before RL\"])\n",
    "df.to_csv(f\"{dir_path}/mover_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSPkfg_hFvWC"
   },
   "outputs": [],
   "source": [
    "r1 = [np.exp(-0.654991332689921), 0.8518536269664765, 0.5175741747308048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktV3TSDhFvWC"
   },
   "outputs": [],
   "source": [
    "r2 = [np.exp(-0.6468217780192693), 0.8508123060067495, 0.5174440166876286]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9OZm-CdFvWC"
   },
   "outputs": [],
   "source": [
    "#r3 = [np.exp(-0.6468217780192693), 0.5174440166876286, 0.8508123060067495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-xnjnTrFvWC"
   },
   "outputs": [],
   "source": [
    "normalized_metrics = normalize(np.array([r1, r2]), axis=0, norm='l1')\n",
    "normalized_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZfAlZEZFvWC"
   },
   "outputs": [],
   "source": [
    "normalized_metrics = normalize(np.array([r1, r2]), axis=0, norm='l1')\n",
    "normalized_metrics"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von RL_Optimization.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('abstract-to-title')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "846c63257c8068d961f9bc7a1cc6d5c293f004d697fb3a71f59faad032bcda7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
