{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4XjWOY_rrsW"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "sys.path.append(\"/content/drive/MyDrive/DL4NLP/abstract-to-title-generation\")\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dqpfdl9a6sUx"
   },
   "outputs": [],
   "source": [
    "!cd \"{PROJECT_ROOT}\"\n",
    "sys.path.append(f\"{PROJECT_ROOT}/src\")\n",
    "sys.path.append(f\"{PROJECT_ROOT}/deps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_s7deoS1rrsa"
   },
   "outputs": [],
   "source": [
    "!pip install -r \"requirements.txt\" &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXY9zP1Orrsa"
   },
   "outputs": [],
   "source": [
    "#!dvc pull -f # <- uncomment to pull data from dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXU6DlgHrrsb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset\n",
    "from tqdm import trange \n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from transformers import BertModel,BertPreTrainedModel\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import model_utils\n",
    "import dataset_utils\n",
    "from fast_soft_sort.pytorch_ops import soft_rank\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from scipy.interpolate import make_interp_spline, BSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFuaCESNrrsb"
   },
   "outputs": [],
   "source": [
    "## Model Configurations\n",
    "p = {\n",
    "    'max_len': 512,\n",
    "    'batch_size': 6,\n",
    "    'lr': 4.0638e-05, #4.0638e-06,\n",
    "    'epochs': 16, #18,\n",
    "    'humor_epochs': 18, #18,\n",
    "    'train_runs': 3,\n",
    "    'humor_train_runs': 4,\n",
    "    'dropout': 0.5,\n",
    "    'num_threads': 1,\n",
    "    'model_name': 'allenai/scibert_scivocab_uncased',\n",
    "    'train_quality': False,\n",
    "    'train_humor': True,\n",
    "    'random_seed': 24\n",
    "}\n",
    "\n",
    "model_utils.setup_seed(p['random_seed'])\n",
    "df_size = 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuvJpJMlZE0Q"
   },
   "outputs": [],
   "source": [
    "model_name = f\"finetuned_size{df_size}_lr{p['lr']}_ep{p['epochs']}_{datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d__%H_%M_%S')}\"\n",
    "model_save_path = f\"{PROJECT_ROOT}/evaluation_models/reward_model/{model_name}\"\n",
    "\n",
    "def save_training_stats(run, phase_title, training_stats, save_path):\n",
    "  Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "  stats_df = pd.DataFrame(np.array(training_stats))\n",
    "  stats_df.columns = [\"episode\", \"accuracy\", \"correlation\"]\n",
    "  stats_df.to_csv(f\"{save_path}/{run}_{phase_title}_stats.csv\")\n",
    "\n",
    "def save_model(model, save_path):\n",
    "  save_path = f\"{save_path}\"\n",
    "  Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "  torch.save(model.state_dict(), f\"{save_path}/model.pth\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwsPsv89rrsf"
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEm9xCBXrrsh"
   },
   "outputs": [],
   "source": [
    "## Configuration loaded from AutoConfig \n",
    "aconfig = AutoConfig.from_pretrained(p['model_name'])\n",
    "## Tokenizer loaded from AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(p['model_name'])\n",
    "## Creating the model from the desired transformer model\n",
    "model = model_utils.HumorBertRegresser.from_pretrained(p['model_name'], config=aconfig)\n",
    "\n",
    "#freeze all layers except regression head\n",
    "\n",
    "unfreeze_layers = ['bert.pooler', 'regressor.1']\n",
    "for name, params in model.named_parameters():\n",
    "  params.requires_grad = False\n",
    "  for ele in unfreeze_layers:\n",
    "    if ele in name:\n",
    "      params.requires_grad = True\n",
    "      break\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "  if params.requires_grad:\n",
    "    print(name, params.size())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## Putting model to device\n",
    "model = model.to(device)\n",
    "## Optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=p['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DWfdWmZj_ir"
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "criterion = lambda a, b: 1.0 - cos(a, b) + mse(a, b)\n",
    "print(criterion(torch.as_tensor(1.0), torch.as_tensor(1.0)))\n",
    "print(criterion(torch.as_tensor([0.0, 0.1]), torch.as_tensor([0.0, 1.0])))\n",
    "print(criterion(torch.as_tensor(-1.0), torch.as_tensor(1.0)))\n",
    "print(criterion(torch.as_tensor(0.1), torch.as_tensor(1.0)))\n",
    "#print(cos(torch.as_tensor(0.0), torch.as_tensor(0.0)))\n",
    "#print(cos(torch.as_tensor(1.0), torch.as_tensor(1.0)))\n",
    "#print(cos(torch.as_tensor(0.1), torch.as_tensor(1.0)))\n",
    "print(mse(torch.as_tensor([0.0, 0.1]), torch.as_tensor([0.0, 1.0])))\n",
    "criterion = mse\n",
    "\n",
    "\n",
    "def corrcoef(target, pred):\n",
    "    # np.corrcoef in torch from @mdo\n",
    "    # https://forum.numer.ai/t/custom-loss-functions-for-xgboost-using-pytorch/960\n",
    "    pred_n = pred - pred.mean()\n",
    "    target_n = target - target.mean()\n",
    "    pred_n = pred_n / pred_n.norm()\n",
    "    target_n = target_n / target_n.norm()\n",
    "    return (pred_n * target_n).sum()\n",
    "\n",
    "def spearman(\n",
    "    pred,\n",
    "    target,\n",
    "    regularization=\"l2\",\n",
    "    regularization_strength=1.0,\n",
    "):\n",
    "    # fast_soft_sort uses 1-based indexing, divide by len to compute percentage of rank\n",
    "    pred = soft_rank(\n",
    "        pred,\n",
    "        regularization=regularization,\n",
    "        regularization_strength=regularization_strength,\n",
    "    )\n",
    "    return corrcoef(target, pred / pred.shape[-1])\n",
    "\n",
    "\n",
    "def corr_loss(a, b):\n",
    "  assert a.shape == b.shape, f\"{a.shape} != {b.shape}\"\n",
    "  return 1.0 - spearman(a, b) + mse(a, b)\n",
    "print(corr_loss(torch.as_tensor([[0.1, 1.0], [1.0, 0.1]]), torch.as_tensor([[1.0, 0.1], [0.1, 1.0]])))\n",
    "print(corr_loss(torch.as_tensor([[0.1, 1.0]]), torch.as_tensor([[1.0, 0.1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1RqSFXRrrsi"
   },
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QslYO9JF6sU0"
   },
   "outputs": [],
   "source": [
    "annotations = pd.read_json(f'{DATA_DIR}/annotated/dataset_{df_size}samples.json')\n",
    "annotations.columns = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "quality_train_loader, quality_dev_loader, quality_test_loader = dataset_utils.gen_datasets(\n",
    "    tokenizer,\n",
    "    annotations,\n",
    "    p[\"max_len\"],\n",
    "    p[\"batch_size\"],\n",
    "    p[\"num_threads\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UyUMhVerrsk"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_mc55zlwLi7"
   },
   "outputs": [],
   "source": [
    "def map_quality(output, target):\n",
    "  target = target\n",
    "  return (output[:, 0], target[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsUTc-GF6sU3"
   },
   "outputs": [],
   "source": [
    "# Load annotated humor data\n",
    "annotations = pd.read_csv(f'{DATA_DIR}/humor/quirky_annotated.csv')\n",
    "aconfig = AutoConfig.from_pretrained(p['model_name'])\n",
    "\n",
    "# add tokens [humor=0][humor=1][humor=2]\n",
    "tokenizer, model = dataset_utils.add_humor_token(tokenizer, model)\n",
    "\n",
    "# create humor dataset and auto annotate for quality\n",
    "def create_humor_dataset(tokenizer, model, annotations):\n",
    "\n",
    "  # annotate quality score with quality_model\n",
    "  df = dataset_utils.gen_humor_dataframe(\n",
    "      tokenizer,\n",
    "      model,\n",
    "      lambda output, target: (output[:, 0], target),\n",
    "      device,\n",
    "      annotations,\n",
    "      p[\"max_len\"],\n",
    "      p[\"num_threads\"]\n",
    "  )\n",
    "\n",
    "  return dataset_utils.gen_humor_datasets(\n",
    "    tokenizer,\n",
    "    df,\n",
    "    p[\"max_len\"],\n",
    "    p[\"num_threads\"],\n",
    "    batch_size=3\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu8A0q5HR1Zz"
   },
   "source": [
    "## Train Quality model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxObZk7YBX2L"
   },
   "outputs": [],
   "source": [
    "maps = {\n",
    "    \"map_quality\": (\n",
    "        map_quality,\n",
    "        quality_train_loader,\n",
    "        quality_dev_loader,\n",
    "        p['epochs'],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvVB6gSjR6jY"
   },
   "outputs": [],
   "source": [
    "if p['train_quality']:\n",
    "  for e in range(p['train_runs']):\n",
    "    map_sample, train_loader, dev_loader, epochs = list(maps.values())[0]\n",
    "    \n",
    "    training_stats = model_utils.train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            map_sample=map_sample,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=dev_loader,\n",
    "            epochs = epochs,\n",
    "            device = device\n",
    "        )\n",
    "    \n",
    "    save_training_stats(e, \"quality\", training_stats, save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PXwJN5U6sU3"
   },
   "source": [
    "## Train Humor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3G503Mz-n2L"
   },
   "outputs": [],
   "source": [
    "# training pipeline for humor and quality learning\n",
    "humor_maps = {\n",
    "    \"map_quality\": (\n",
    "        map_quality,\n",
    "        quality_train_loader,\n",
    "        quality_dev_loader,\n",
    "        p['epochs'],\n",
    "    ),\n",
    "    \"map_mock_quality\": (\n",
    "        lambda output, target: (output[:, 0], torch.full([target.shape[0]], 1.0)),\n",
    "        None,\n",
    "        None,\n",
    "        p['epochs']\n",
    "    ),\n",
    "    \"map_humor\": (\n",
    "        lambda output, target: (output[:, 1], target[:, 1]),\n",
    "        None,\n",
    "        None,\n",
    "        p['humor_epochs']\n",
    "    ),\n",
    "    \"map_id\": (\n",
    "        lambda output, target: (output, target),\n",
    "        None,\n",
    "        None,\n",
    "        p['epochs']\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqlH0CTC6sU3"
   },
   "outputs": [],
   "source": [
    "if p['train_humor']:\n",
    "  for e in range(p['train_runs']):\n",
    "\n",
    "    map_sample, train_loader, dev_loader, epochs = list(humor_maps.values())[0]\n",
    "    \n",
    "    training_stats = model_utils.train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            map_sample=map_sample,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=dev_loader,\n",
    "            epochs = epochs,\n",
    "            device = device\n",
    "        )\n",
    "    \n",
    "    save_training_stats(e, \"quality\", training_stats, save_path=model_save_path)\n",
    "    \n",
    "    train_loader, dev_loader, test_loader = create_humor_dataset(tokenizer, model, annotations)\n",
    "\n",
    "    for phase_name, (map_sample, _, _, epochs) in list(humor_maps.items())[1:4]:\n",
    "\n",
    "        training_stats = model_utils.train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            map_sample=map_sample,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=dev_loader,\n",
    "            epochs = epochs,\n",
    "            device = device\n",
    "        )\n",
    "        save_training_stats(e, phase_name, training_stats, save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAy_0w2cXROl"
   },
   "outputs": [],
   "source": [
    "save_model(model, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je7iG4Fvrrsl"
   },
   "source": [
    "\n",
    "### Evaluate Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "th5WSUdMrrsl"
   },
   "outputs": [],
   "source": [
    "def print_quality_stats(model):\n",
    "  print(\"Train\")\n",
    "  model_utils.display_correlation(model, quality_train_loader, maps['map_quality'][0], device)\n",
    "  print(\"Dev\")\n",
    "  model_utils.display_correlation(model, quality_dev_loader, maps['map_quality'][0], device)\n",
    "  print(\"Test\")\n",
    "  model_utils.display_correlation(model, quality_test_loader, maps['map_quality'][0], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5hMDN6f6sU3"
   },
   "source": [
    "### Evaluate Humor & Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaBj5YRgxvPv"
   },
   "outputs": [],
   "source": [
    "def print_humor_quality_stats(model, humor_train_loader, humor_dev_loader, humor_test_loader):\n",
    "  print(\"Train\")\n",
    "  model_utils.display_correlation(model, humor_train_loader, humor_maps['map_id'][0], device)\n",
    "  print(\"Dev\")\n",
    "  model_utils.display_correlation(model, humor_dev_loader, humor_maps['map_id'][0], device)\n",
    "  print(\"Test\")\n",
    "  model_utils.display_correlation(model, humor_test_loader, humor_maps['map_id'][0], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2axQHWgTrrsl"
   },
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCURJRfOrrsl"
   },
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "  model.load_state_dict(torch.load(path))\n",
    "  model.to(device)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QQS4M8gP_VM"
   },
   "source": [
    "### Plotting & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1SaHkaBXDdo"
   },
   "outputs": [],
   "source": [
    "humor_model_paths = [\n",
    "    f\"{PROJECT_ROOT}/evaluation_models/reward_model/finetuned_size80_lr4.0638e-05_ep16_2022-08-20__13_17_24_final_new\",\n",
    "    f\"{PROJECT_ROOT}/evaluation_models/reward_model/finetuned_size140_lr4.0638e-05_ep16_2022-08-18__11_34_24_final\",\n",
    "    f\"{PROJECT_ROOT}/evaluation_models/reward_model/finetuned_size230_lr4.0638e-05_ep16_2022-08-20__13_03_57_final_new\"\n",
    "]\n",
    "\n",
    "quality_model_paths = [\n",
    "    f\"{PROJECT_ROOT}/evaluation_models/reward_model/only_quality_finetuned_size80_lr4.0638e-05_ep20_2022-08-18__21_29_20_final\",\n",
    "    f\"{PROJECT_ROOT}/evaluation_models/reward_model/only_quality_finetuned_size140_lr4.0638e-05_ep20_2022-08-18__15_05_43_final\",\n",
    "    f\"{PROJECT_ROOT}/evaluation_models/reward_model/only_quality_finetuned_size230_lr4.0638e-05_ep20_2022-08-18__15_17_48_final\"\n",
    "]\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('axes', linewidth=0.5)\n",
    "\n",
    "plt.rcParams[\"axes.labelweight\"] = \"light\"\n",
    "plt.rcParams[\"font.weight\"] = \"light\"\n",
    "\n",
    "def plot_model_quality_stats(runs, phase, paths):\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 3))\n",
    "  for train_size, df_len, path in zip([54,96,162], [80,140,230], paths):\n",
    "    stats_df = pd.DataFrame(np.array([]))\n",
    "    stats.columns = [\"accuracy\", \"correlation\"]\n",
    "    for run in range(runs):\n",
    "      tmp_df = pd.read_csv(f\"{path}/{run}_{phase}.csv\", index_col=0) \n",
    "      tmp_df = tmp_df.drop(columns=[\"episode\"])\n",
    "      stats_df = pd.concat([stats_df, tmp_df])\n",
    "      stats_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    stats_df[\"accuracy\"] = stats_df[\"accuracy\"].apply(lambda x: x / train_size)\n",
    "    stats_df[\"accuracy\"].interpolate(method='cubic')\n",
    "    stats_df[\"accuracy\"].plot(ax=axes[0], label=f\"{df_len}\")\n",
    "    axes[0].legend(loc=\"upper right\")\n",
    "    axes[0].title.set_text('(a) MSE loss for reward model trained on title quality')\n",
    "    stats_df[\"correlation\"].plot(ax=axes[1], label=f\"{df_len}\")\n",
    "    axes[1].legend(loc=\"lower right\")\n",
    "    axes[1].title.set_text('(b) Spearman correlation for title quality')\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "      ax.set(xlabel='epochs', ylabel='mean mse loss' if idx == 0 else 'spearman correlation')\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def plot_model_quality_humor_stats(runs, phase, paths):\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "  for train_size, df_len, path in zip([54,96,162], [80,140,230], paths):\n",
    "    stats_df = pd.DataFrame(np.array([]))\n",
    "    stats.columns = [\"accuracy\", \"correlation_quality\", \"correlation_humor\"]\n",
    "    for run in range(runs):\n",
    "      tmp_df = pd.read_csv(f\"{path}/{run}_{phase}.csv\", index_col=0, converters={'correlation': lambda x: x[1:-1].strip().split(' ', 1)})\n",
    "      tmp_df = tmp_df.drop(columns=[\"episode\"])\n",
    "      split_df = pd.DataFrame(tmp_df['correlation'].tolist(), columns=['correlation_quality', 'correlation_humor'])\n",
    "      tmp_df = pd.concat([tmp_df, split_df], axis=1)\n",
    "      tmp_df = tmp_df.drop(columns=[\"correlation\"])\n",
    "      tmp_df = tmp_df.astype({'correlation_quality':'float','correlation_humor':'float'})\n",
    "      #tmp_df = tmp_df.astype({'correlation_quality': 'float', 'correlation_humor':'float'}).dtypes\n",
    "      stats_df = pd.concat([stats_df, tmp_df])\n",
    "      stats_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    stats_df[\"accuracy\"] = stats_df[\"accuracy\"].apply(lambda x: x / 138)\n",
    "    stats_df[\"accuracy\"].interpolate(method='cubic')\n",
    "    stats_df[\"accuracy\"].plot(ax=axes[0], label=f\"{df_len}\")\n",
    "    axes[0].legend(loc=\"upper right\")\n",
    "    axes[0].title.set_text(\"(a) MSE loss / epoch\")\n",
    "    stats_df[\"correlation_quality\"].plot(ax=axes[1], label=f\"{df_len} quality\")\n",
    "    stats_df[\"correlation_humor\"].plot(ax=axes[2], label=f\"{df_len} humor\")\n",
    "    axes[1].legend(loc=\"lower right\")\n",
    "    axes[1].title.set_text('(b) title quality correlation / epoch')\n",
    "    axes[2].legend(loc=\"lower left\")\n",
    "    axes[2].title.set_text('(c) title humor correlation / epoch')\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "      ax.set(xlabel='epochs', ylabel='mean mse loss' if idx == 0 else 'spearman correlation')\n",
    "\n",
    "  x = [16, 32]\n",
    "  ymin1, ymax1 = axes[0].get_ylim()\n",
    "  ymin2, ymax2 = axes[1].get_ylim()\n",
    "  ymin3, ymax3 = axes[2].get_ylim()\n",
    "  axes[0].vlines(x=x, ymin=ymin1, ymax=ymax1, colors='black', ls='--', lw=2)\n",
    "  axes[1].vlines(x=x, ymin=ymin2, ymax=ymax2, colors='black', ls='--', lw=2)\n",
    "  axes[2].vlines(x=x, ymin=ymin3, ymax=ymax3, colors='black', ls='--', lw=2)\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_model_quality_humor_stats(3, \"map_id_stats\", humor_model_paths)\n",
    "plot_model_quality_stats(1, \"quality_stats\", quality_model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oufcYONl-5Lw"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model = model_utils.HumorBertRegresser.from_pretrained(p[\"model_name\"])\n",
    "model.to(device)\n",
    "tokenizer, model = dataset_utils.add_humor_token(tokenizer, model)\n",
    "humor_train_loader, humor_dev_loader, humor_test_loader = create_humor_dataset(tokenizer, model, annotations)\n",
    "\n",
    "for path in humor_model_paths:\n",
    "  model_path = f\"{path}/model.pth\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(p['model_name'])\n",
    "  model = model_utils.HumorBertRegresser.from_pretrained(p[\"model_name\"])\n",
    "  tokenizer, model = dataset_utils.add_humor_token(tokenizer, model)\n",
    "  model.load_state_dict(torch.load(model_path))\n",
    "  model.to(device)\n",
    "  humor_train_loader, humor_dev_loader, humor_test_loader = create_humor_dataset(tokenizer, model, annotations)\n",
    "  print_humor_quality_stats(model, humor_train_loader, humor_dev_loader, humor_test_loader)\n",
    "  print(\"-\"*20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "reward_model-tmp.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('3.9.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c039eee1f421c1324b6f0bc8cbf23673c3e07f040312fef7bf30f2aec2f39f37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
