{"cells":[{"cell_type":"markdown","metadata":{"id":"tOLZ4dV6scRB"},"source":["## Configuration"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5394,"status":"ok","timestamp":1653214696679,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"1rDxT1Ifsqgd","outputId":"cc02c24f-6aa2-4ad3-caa2-b88457a70058"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: git_root in /usr/local/lib/python3.7/dist-packages (0.1)\n","Running on CoLab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/DL4NLP/abstract-to-title-generation\n"]}],"source":["!pip install git_root\n","\n","PROJECT_ROOT = None\n","in_colab = 'google.colab' in str(get_ipython())\n","\n","if in_colab:\n","  print('Running on CoLab')\n","  PROJECT_ROOT = \"/content/drive/MyDrive/DL4NLP/abstract-to-title-generation\"\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","else:\n","  print('Running on local machine')\n","  from git_root import git_root\n","  PROJECT_ROOT = git_root()\n","\n","%cd {PROJECT_ROOT}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":42861,"status":"ok","timestamp":1653211908716,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"xLTKACAPEj84","outputId":"3e9d3441-a66f-4c79-d75e-73e097674f74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n","\u001b[K     |████████████████████████████████| 346 kB 5.6 MB/s \n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 50.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.3.5)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.64.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 50.3 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.11.0+cu113)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.4.1)\n","Requirement already satisfied: git_root in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.1)\n","Collecting dvc[gdrive]\n","  Downloading dvc-2.10.2-py3-none-any.whl (401 kB)\n","\u001b[K     |████████████████████████████████| 401 kB 49.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 1)) (4.11.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: dill<0.3.5 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 50.4 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 44.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 1)) (21.3)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 51.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 957 kB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 1)) (3.7.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 1)) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 1)) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets->-r requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 58.9 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 53.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 2)) (2019.12.20)\n","Collecting pyyaml\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 46.0 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 3)) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 3)) (1.15.0)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 53.5 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.12)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (21.4.0)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 68.9 MB/s \n","\u001b[?25hCollecting funcy>=1.14\n","  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n","Collecting python-benedict>=0.24.2\n","  Downloading python_benedict-0.25.1-py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 366 kB/s \n","\u001b[?25hCollecting grandalf==0.6\n","  Downloading grandalf-0.6-py3-none-any.whl (31 kB)\n","Collecting dvc-render==0.0.5\n","  Downloading dvc_render-0.0.5-py3-none-any.whl (15 kB)\n","Collecting aiohttp-retry>=2.4.5\n","  Downloading aiohttp_retry-2.4.6-py3-none-any.whl (7.7 kB)\n","Collecting shtab<2,>=1.3.4\n","  Downloading shtab-1.5.4-py2.py3-none-any.whl (13 kB)\n","Collecting dpath<3,>=2.0.2\n","  Downloading dpath-2.0.6-py3-none-any.whl (15 kB)\n","Requirement already satisfied: importlib-resources>=5.2.2 in /usr/local/lib/python3.7/dist-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (5.7.1)\n","Collecting flufl.lock>=5\n","  Downloading flufl.lock-7.0-py3-none-any.whl (11 kB)\n","Collecting dictdiffer>=0.8.1\n","  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n","Collecting flatten-dict<1,>=0.4.1\n","  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n","Collecting scmrepo==0.0.19\n","  Downloading scmrepo-0.0.19-py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 6.8 MB/s \n","\u001b[?25hCollecting zc.lockfile>=1.2.1\n","  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n","Collecting colorama>=0.3.9\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting toml>=0.10.1\n","  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Collecting rich>=10.13.0\n","  Downloading rich-12.4.1-py3-none-any.whl (231 kB)\n","\u001b[K     |████████████████████████████████| 231 kB 68.0 MB/s \n","\u001b[?25hCollecting nanotime>=0.5.2\n","  Downloading nanotime-0.5.2.tar.gz (3.2 kB)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.4.4)\n","Collecting configobj>=5.0.6\n","  Downloading configobj-5.0.6.tar.gz (33 kB)\n","Collecting psutil>=5.8.0\n","  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n","\u001b[K     |████████████████████████████████| 281 kB 63.5 MB/s \n","\u001b[?25hCollecting ruamel.yaml>=0.17.11\n","  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n","\u001b[K     |████████████████████████████████| 109 kB 68.2 MB/s \n","\u001b[?25hCollecting distro>=1.3.0\n","  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.7/dist-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (2.6.3)\n","Collecting diskcache>=5.2.1\n","  Downloading diskcache-5.4.0-py3-none-any.whl (44 kB)\n","\u001b[K     |████████████████████████████████| 44 kB 3.7 MB/s \n","\u001b[?25hCollecting pathspec<0.10.0,>=0.9.0\n","  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n","Collecting dvclive>=0.7.3\n","  Downloading dvclive-0.8.1-py2.py3-none-any.whl (31 kB)\n","Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.8.9)\n","Collecting voluptuous>=0.11.7\n","  Downloading voluptuous-0.13.1-py3-none-any.whl (29 kB)\n","Requirement already satisfied: pydot>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.3.0)\n","Collecting pygtrie>=2.3.2\n","  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n","Collecting pydrive2[fsspec]>=1.10.1\n","  Downloading PyDrive2-1.10.1-py3-none-any.whl (39 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from grandalf==0.6->dvc[gdrive]->-r requirements.txt (line 11)) (0.16.0)\n","Collecting gitpython>3\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 70.2 MB/s \n","\u001b[?25hCollecting dulwich>=0.20.34\n","  Downloading dulwich-0.20.40.tar.gz (423 kB)\n","\u001b[K     |████████████████████████████████| 423 kB 61.9 MB/s \n","\u001b[?25hCollecting pygit2>=1.7.2\n","  Downloading pygit2-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[K     |████████████████████████████████| 4.5 MB 45.3 MB/s \n","\u001b[?25hCollecting asyncssh<3,>=2.7.1\n","  Downloading asyncssh-2.10.1-py3-none-any.whl (335 kB)\n","\u001b[K     |████████████████████████████████| 335 kB 66.1 MB/s \n","\u001b[?25hCollecting cryptography>=3.1\n","  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 41.5 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.1->asyncssh<3,>=2.7.1->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.1->asyncssh<3,>=2.7.1->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (2.21)\n","Collecting atpublic>=2.3\n","  Downloading atpublic-3.0.1-py3-none-any.whl (4.8 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets->-r requirements.txt (line 1)) (3.8.0)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (4.1.3)\n","Requirement already satisfied: google-api-python-client>=1.12.5 in /usr/local/lib/python3.7/dist-packages (from pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (1.12.11)\n","Collecting pyOpenSSL>=19.1.0\n","  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.17.4)\n","Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (1.35.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (3.0.1)\n","Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (1.31.5)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.0.4)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (3.17.3)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (57.4.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (1.56.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (4.2.4)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.4.8)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from pygit2>=1.7.2->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (1.5.2)\n","Collecting python-fsutil<1.0.0,>=0.6.0\n","  Downloading python_fsutil-0.6.1-py3-none-any.whl (12 kB)\n","Collecting mailchecker<5.0.0,>=4.1.0\n","  Downloading mailchecker-4.1.17.tar.gz (232 kB)\n","\u001b[K     |████████████████████████████████| 232 kB 65.0 MB/s \n","\u001b[?25hCollecting requests>=2.19.0\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n","\u001b[?25hCollecting xmltodict<1.0.0,>=0.12.0\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Collecting ftfy<7.0.0,>=6.0.0\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n","\u001b[?25hCollecting phonenumbers<9.0.0,>=8.12.0\n","  Downloading phonenumbers-8.12.48-py2.py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 40.6 MB/s \n","\u001b[?25hRequirement already satisfied: python-slugify<7.0.0,>=6.0.1 in /usr/local/lib/python3.7/dist-packages (from python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (6.1.2)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0.0,>=6.0.0->python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (0.2.5)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify<7.0.0,>=6.0.1->python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (1.3)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.13.0->dvc[gdrive]->-r requirements.txt (line 11)) (2.6.1)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 6.7 MB/s \n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n","  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n","\u001b[K     |████████████████████████████████| 546 kB 65.0 MB/s \n","\u001b[?25hBuilding wheels for collected packages: configobj, dulwich, nanotime, pygtrie, mailchecker\n","  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34547 sha256=404fa597206a34b2648f45b33ab986171f7f8938fc0bd02d7c108f50da325fc9\n","  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n","  Building wheel for dulwich (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dulwich: filename=dulwich-0.20.40-cp37-cp37m-linux_x86_64.whl size=535045 sha256=ace29b86731debebabc8a5114e11c2394a0e2507c58d7066e130a22c8c73823f\n","  Stored in directory: /root/.cache/pip/wheels/10/c3/15/ce1e5edb6c5cf20ed248f659c054a69492a7bac4d71cd9b035\n","  Building wheel for nanotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nanotime: filename=nanotime-0.5.2-py3-none-any.whl size=2441 sha256=3ce2cd814add933c8db97d5b5c31f060641b4d0ccb6c330016a78b7dfa841c85\n","  Stored in directory: /root/.cache/pip/wheels/b8/92/aa/456d462c908b4e210c3928f778d28f94049fc9e47af8b191c9\n","  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19063 sha256=683e54ebd4b57ab635e24b228f3a86ecfd454fc88d9b67537357af3ad2cfbd31\n","  Stored in directory: /root/.cache/pip/wheels/d3/f8/ba/1d828b1603ea422686eb694253a43cb3a5901ea4696c1e0603\n","  Building wheel for mailchecker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mailchecker: filename=mailchecker-4.1.17-py3-none-any.whl size=232921 sha256=fb8ded1c1fb19824874b805f7c9cd612abd976b0701fb7a4a00eda5730c3c3b6\n","  Stored in directory: /root/.cache/pip/wheels/62/34/d5/21a0829a95aec844a36606f113b607b6ed82f7148cb3f4a5a9\n","Successfully built configobj dulwich nanotime pygtrie mailchecker\n","Installing collected packages: urllib3, smmap, requests, multidict, frozenlist, yarl, gitdb, funcy, cryptography, asynctest, async-timeout, aiosignal, xmltodict, toml, ruamel.yaml.clib, pyyaml, python-fsutil, pyOpenSSL, pygtrie, pygit2, psutil, phonenumbers, pathspec, mailchecker, gitpython, ftfy, fsspec, dvc-render, dulwich, commonmark, atpublic, asyncssh, aiohttp, zc.lockfile, voluptuous, shtab, shortuuid, scmrepo, ruamel.yaml, rich, python-benedict, pydrive2, nanotime, grandalf, flufl.lock, flatten-dict, dvclive, dpath, distro, diskcache, dictdiffer, configobj, colorama, aiohttp-retry, xxhash, tokenizers, responses, huggingface-hub, dvc, transformers, sentencepiece, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiohttp-retry-2.4.6 aiosignal-1.2.0 async-timeout-4.0.2 asyncssh-2.10.1 asynctest-0.13.0 atpublic-3.0.1 colorama-0.4.4 commonmark-0.9.1 configobj-5.0.6 cryptography-37.0.2 datasets-2.2.2 dictdiffer-0.9.0 diskcache-5.4.0 distro-1.7.0 dpath-2.0.6 dulwich-0.20.40 dvc-2.10.2 dvc-render-0.0.5 dvclive-0.8.1 flatten-dict-0.4.2 flufl.lock-7.0 frozenlist-1.3.0 fsspec-2022.5.0 ftfy-6.1.1 funcy-1.17 gitdb-4.0.9 gitpython-3.1.27 grandalf-0.6 huggingface-hub-0.6.0 mailchecker-4.1.17 multidict-6.0.2 nanotime-0.5.2 pathspec-0.9.0 phonenumbers-8.12.48 psutil-5.9.1 pyOpenSSL-22.0.0 pydrive2-1.10.1 pygit2-1.9.1 pygtrie-2.4.2 python-benedict-0.25.1 python-fsutil-0.6.1 pyyaml-6.0 requests-2.27.1 responses-0.18.0 rich-12.4.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 scmrepo-0.0.19 sentencepiece-0.1.96 shortuuid-1.0.9 shtab-1.5.4 smmap-5.0.0 tokenizers-0.12.1 toml-0.10.2 transformers-4.19.2 urllib3-1.25.11 voluptuous-0.13.1 xmltodict-0.13.0 xxhash-3.0.0 yarl-1.7.2 zc.lockfile-2.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["psutil"]}}},"metadata":{},"output_type":"display_data"}],"source":["# install requirements\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97665,"status":"ok","timestamp":1653062861129,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"n1v5lcHeEiUv","outputId":"1cbe1cef-6d7d-47d2-9c4b-2e4537a4ee22"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/DL4NLP/abstract-to-title-generation\n","Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |40d83206bbedf13326333bb63cbbc9.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","40d83206bbedf13326333bb63cbbc9.dir:   0% 0.00/148 [00:00<?, ?B/s{'info': ''}]   \u001b[A\n","100% 148/148 [00:01<00:00, 81.8B/s{'info': ''}]                              \u001b[A\n","Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |ca4e70571a46a8cd5b14cebf93cc30.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","ca4e70571a46a8cd5b14cebf93cc30.dir:   0% 0.00/148 [00:00<?, ?B/s{'info': ''}]   \u001b[A\n","100% 148/148 [00:01<00:00, 87.0B/s{'info': ''}]                              \u001b[A\n","Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |79fb1e3826d48e27806105d0f9683d.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","79fb1e3826d48e27806105d0f9683d.dir:   0% 0.00/217 [00:00<?, ?B/s{'info': ''}]   \u001b[A\n","100% 217/217 [00:01<00:00, 128B/s{'info': ''}]                               \u001b[A\n","Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |e15b8604517e9a03e8ba9be6b0ef4e.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","e15b8604517e9a03e8ba9be6b0ef4e.dir:   0% 0.00/148 [00:00<?, ?B/s{'info': ''}]   \u001b[A\n","100% 148/148 [00:01<00:00, 115B/s{'info': ''}]                               \u001b[A\n","Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |97b4fb6099b1112907f12e43462b46.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","97b4fb6099b1112907f12e43462b46.dir:   0% 0.00/148 [00:00<?, ?B/s{'info': ''}]   \u001b[A\n","100% 148/148 [00:01<00:00, 122B/s{'info': ''}]                               \u001b[A\n","Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |e34871bf342788e72b9e7906c82751.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","e34871bf342788e72b9e7906c82751.dir:   0% 0.00/148 [00:00<?, ?B/s{'info': ''}]   \u001b[A\n","100% 148/148 [00:01<00:00, 139B/s{'info': ''}]                               \u001b[A\n","Transferring:   0% 0/6 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |56c9e674a2bbda95c40722f9dd2fc8     0.00/? [00:00<?,        ?B/s]\u001b[A\n","\n","!\u001b[A\u001b[A\n","\n","  0%|          |846262a74111ab90e5eced9d5b5da7     0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\n","\n","\n","!\u001b[A\u001b[A\u001b[A\n","\n","\n","  0%|          |22244ad467cf9d50c4987ccae04c99     0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","!\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  0%|          |54a9bf6f5a97a480caed8c03cb468c     0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","!\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  0%|          |c004998452c58302e18d0ff5256293     0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","!\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  0%|          |7da3b149ef3d3a0051c319a8615365     0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","56c9e674a2bbda95c40722f9dd2fc8:   0% 0.00/1.67k [00:00<?, ?B/s{'info': ''}]     \u001b[A\n","\n","\n","\n","54a9bf6f5a97a480caed8c03cb468c:   0% 0.00/1.35k [00:00<?, ?B/s{'info': ''}]     \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","c004998452c58302e18d0ff5256293:   0% 0.00/907 [00:00<?, ?B/s{'info': ''}]       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","7da3b149ef3d3a0051c319a8615365:   0% 0.00/1.55k [00:00<?, ?B/s{'info': ''}]     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","846262a74111ab90e5eced9d5b5da7:   0% 0.00/1.62k [00:00<?, ?B/s{'info': ''}]     \u001b[A\u001b[A\n","\n","\n","22244ad467cf9d50c4987ccae04c99:   0% 0.00/1.40k [00:00<?, ?B/s{'info': ''}]     \u001b[A\u001b[A\u001b[A\n","100% 1.67k/1.67k [00:01<00:00, 1.33kB/s{'info': ''}]                       \u001b[A\n","Transferring:  17% 1/6 [00:01<00:06,  1.33s/file{'info': ''}]\n","\n","\n","\n","100% 1.35k/1.35k [00:01<00:00, 1.01kB/s{'info': ''}]                       \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Transferring:  33% 2/6 [00:01<00:02,  1.63file/s{'info': ''}]\n","\n","\n","\n","\n","c004998452c58302e18d0ff5256293: 100% 907/907 [00:01<00:00, 625B/s{'info': ''}]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","                                                                              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","100% 1.62k/1.62k [00:01<00:00, 908B/s{'info': ''}]                         \u001b[A\u001b[A\n","\n","\n","\n","\n","\n","100% 1.55k/1.55k [00:01<00:00, 874B/s{'info': ''}]                         \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","                                                  \u001b[A\u001b[A\n","\n","\n","\n","\n","\n","Transferring:  67% 4/6 [00:01<00:00,  2.72file/s{'info': ''}]\n","\n","\n","100% 1.40k/1.40k [00:02<00:00, 650B/s{'info': ''}]                         \u001b[A\u001b[A\u001b[A\n","\n","\n","Checkout:   0% 0/19 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |.Z9MpCRdjUG5CKgAiK3pBjB.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n",".Z9MpCRdjUG5CKgAiK3pBjB.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]         \u001b[A\n","                                                                       \u001b[A\n","!\u001b[A\n","  0%|          |c004998452c58302e18d0ff5256293     0.00/? [00:00<?,        ?B/s]\u001b[A\n","c004998452c58302e18d0ff5256293:   0% 0.00/907 [00:00<?, ?B/s{'info': ''}]       \u001b[A\n","  5% 1/19 [00:00<00:08,  2.08file/s{'info': ' /content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/GPT2/config.json |'}]\n","!\u001b[A\n","  0%|          |.a99oQWwdZ7sJeBqv4P99tD.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n",".a99oQWwdZ7sJeBqv4P99tD.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]         \u001b[A\n","                                                                       \u001b[A\n","!\u001b[A\n","  0%|          |22244ad467cf9d50c4987ccae04c99     0.00/? [00:00<?,        ?B/s]\u001b[A\n","22244ad467cf9d50c4987ccae04c99:   0% 0.00/1.40k [00:00<?, ?B/s{'info': ''}]     \u001b[A\n"," 21% 4/19 [00:01<00:03,  4.32file/s{'info': ' /content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/PEGASUS-XSum/config.json |'}]\n","!\u001b[A\n","  0%|          |.8osvgJSJ7kGoxoyaBoAULA.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n",".8osvgJSJ7kGoxoyaBoAULA.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]         \u001b[A\n","                                                                       \u001b[A\n","!\u001b[A\n","  0%|          |7da3b149ef3d3a0051c319a8615365     0.00/? [00:00<?,        ?B/s]\u001b[A\n","7da3b149ef3d3a0051c319a8615365:   0% 0.00/1.55k [00:00<?, ?B/s{'info': ''}]     \u001b[A\n"," 26% 5/19 [00:01<00:04,  3.10file/s{'info': ' /content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/BART-XSum/config.json |'}]   \n","!\u001b[A\n","  0%|          |.Qx2ZfTWTMpP7hEewcz3kcW.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n",".Qx2ZfTWTMpP7hEewcz3kcW.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]         \u001b[A\n","                                                                       \u001b[A\n","!\u001b[A\n","  0%|          |56c9e674a2bbda95c40722f9dd2fc8     0.00/? [00:00<?,        ?B/s]\u001b[A\n","56c9e674a2bbda95c40722f9dd2fc8:   0% 0.00/1.67k [00:00<?, ?B/s{'info': ''}]     \u001b[A\n"," 53% 10/19 [00:02<00:01,  6.27file/s{'info': ' /content/drive/MyDrive/DL4NLP/abstract-to-title-generation/data/annotated |'}]            \n","!\u001b[A\n","  0%|          |.ffYFp3sutNUZ48A6NsPMvD.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n",".ffYFp3sutNUZ48A6NsPMvD.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]         \u001b[A\n","                                                                       \u001b[A\n","!\u001b[A\n","  0%|          |54a9bf6f5a97a480caed8c03cb468c     0.00/? [00:00<?,        ?B/s]\u001b[A\n","54a9bf6f5a97a480caed8c03cb468c:   0% 0.00/1.35k [00:00<?, ?B/s{'info': ''}]     \u001b[A\n","                                                                           \u001b[A\n","!\u001b[A\n","  0%|          |.R9kb7ubQA5SYSznJWoBEBr.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n",".R9kb7ubQA5SYSznJWoBEBr.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]         \u001b[A\n","                                                                       \u001b[A\n","!\u001b[A\n","  0%|          |846262a74111ab90e5eced9d5b5da7     0.00/? [00:00<?,        ?B/s]\u001b[A\n","846262a74111ab90e5eced9d5b5da7:   0% 0.00/1.62k [00:00<?, ?B/s{'info': ''}]     \u001b[A\n","\u001b[33mM\u001b[0m       model/GPT2/\n","\u001b[33mM\u001b[0m       model/PEGASUS-XSum/\n","\u001b[33mM\u001b[0m       model/BART-XSum/\n","\u001b[33mM\u001b[0m       model/BART-base/\n","\u001b[33mM\u001b[0m       model/T5/\n","\u001b[33mM\u001b[0m       model/BART-CNN/\n","6 files modified and 6 files fetched\n","\u001b[0m"]}],"source":["# pull data only pulls changed data\n","%cd {PROJECT_ROOT}\n","!dvc pull"]},{"cell_type":"markdown","metadata":{"id":"AZQTdOKhfc39"},"source":["## Code section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pglc_wgIaky2","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# imports\n","from datasets import Dataset\n","from datasets import load_dataset, load_metric\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxdV7I5uVe3y","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["model_checkpoint = 'facebook/bart-base'"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3410,"status":"ok","timestamp":1653213931269,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"VisOC8KZaxI3","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# create tokenizer from checkopoint\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","# create pretrained model from checkpoint\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1653213933105,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"c3FO11232ShQ","outputId":"d8a68b69-d3bd-4d48-8b8e-38d81ccbb8a5","pycharm":{"name":"#%%\n"}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DL4NLP/abstract-to-title-generation/\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                   title  \\\n","0      Natural Image Bases to Represent Neuroimaging ...   \n","1      Sluice Resolution without Hand-Crafted Feature...   \n","2      Learning Translation Models from Monolingual C...   \n","3           Sentiment Adaptive End-to-End Dialog Systems   \n","4          User-Friendly Text Prediction For Translators   \n","...                                                  ...   \n","21437  Arabic Tokenization, Part-of-Speech Tagging an...   \n","21438  Using Semantically Motivated Estimates to Help...   \n","21439  A Mathematical Exploration of Why Language Mod...   \n","21440  Do You Know That Florence Is Packed with Visit...   \n","21441  LDA Topic Model with Soft Assignment of Descri...   \n","\n","                                                abstract  title_length  \\\n","0      Visual inspection of neuroimagery is susceptib...             7   \n","1      Sluice resolution in English is the problem of...             9   \n","2      Translation models often fail to generate good...             7   \n","3      End-to-end learning framework is useful for bu...             5   \n","4      Text prediction is a form of interactive machi...             5   \n","...                                                  ...           ...   \n","21437  We present an approach to using a morphologica...            11   \n","21438  Research into the automatic acquisition of sub...             8   \n","21439  Autoregressive language models, pretrained usi...            11   \n","21440  When a speaker, Mary, asks \"Do you know that F...            15   \n","21441  The LDA topic model is being used to model cor...            10   \n","\n","       abstract_length  \n","0                  132  \n","1                  110  \n","2                  152  \n","3                  119  \n","4                  134  \n","...                ...  \n","21437               58  \n","21438              530  \n","21439              194  \n","21440              174  \n","21441              198  \n","\n","[21442 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-687bb894-35de-4c7e-84c1-f57fc44f7c4c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>title_length</th>\n","      <th>abstract_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Natural Image Bases to Represent Neuroimaging ...</td>\n","      <td>Visual inspection of neuroimagery is susceptib...</td>\n","      <td>7</td>\n","      <td>132</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sluice Resolution without Hand-Crafted Feature...</td>\n","      <td>Sluice resolution in English is the problem of...</td>\n","      <td>9</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Learning Translation Models from Monolingual C...</td>\n","      <td>Translation models often fail to generate good...</td>\n","      <td>7</td>\n","      <td>152</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentiment Adaptive End-to-End Dialog Systems</td>\n","      <td>End-to-end learning framework is useful for bu...</td>\n","      <td>5</td>\n","      <td>119</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>User-Friendly Text Prediction For Translators</td>\n","      <td>Text prediction is a form of interactive machi...</td>\n","      <td>5</td>\n","      <td>134</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21437</th>\n","      <td>Arabic Tokenization, Part-of-Speech Tagging an...</td>\n","      <td>We present an approach to using a morphologica...</td>\n","      <td>11</td>\n","      <td>58</td>\n","    </tr>\n","    <tr>\n","      <th>21438</th>\n","      <td>Using Semantically Motivated Estimates to Help...</td>\n","      <td>Research into the automatic acquisition of sub...</td>\n","      <td>8</td>\n","      <td>530</td>\n","    </tr>\n","    <tr>\n","      <th>21439</th>\n","      <td>A Mathematical Exploration of Why Language Mod...</td>\n","      <td>Autoregressive language models, pretrained usi...</td>\n","      <td>11</td>\n","      <td>194</td>\n","    </tr>\n","    <tr>\n","      <th>21440</th>\n","      <td>Do You Know That Florence Is Packed with Visit...</td>\n","      <td>When a speaker, Mary, asks \"Do you know that F...</td>\n","      <td>15</td>\n","      <td>174</td>\n","    </tr>\n","    <tr>\n","      <th>21441</th>\n","      <td>LDA Topic Model with Soft Assignment of Descri...</td>\n","      <td>The LDA topic model is being used to model cor...</td>\n","      <td>10</td>\n","      <td>198</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21442 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-687bb894-35de-4c7e-84c1-f57fc44f7c4c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-687bb894-35de-4c7e-84c1-f57fc44f7c4c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-687bb894-35de-4c7e-84c1-f57fc44f7c4c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}],"source":["# load train pairs\n","!echo $PROJECT_ROOT\n","df = pd.read_csv(f\"{PROJECT_ROOT}/data/filtered/Kopie von train_pairs.csv\", index_col=0)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lm97OV9JNgx4","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1653213936105,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"r5eGRlj3Po9D","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["df = df.drop(columns=[\"title_length\", \"abstract_length\"])"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":298,"status":"ok","timestamp":1653213947765,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"mp4cQi3LQsPm","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["df_train = df[:11864]\n","df_valid = df[11864:]"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1653213961784,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"k43zz_yzRLYt","outputId":"9da4d8b5-b402-476c-8b07-d40bc79a4407"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   title  \\\n","11864               Spherical CNNs on Unstructured Grids   \n","11865   Soft Representation Learning for Sparse Transfer   \n","11866  Explicit and Implicit Syntactic Features for T...   \n","11867  Adaptive Gradient Methods with Dynamic Bound o...   \n","11868  Using Fast Weights to Improve Persistent Contr...   \n","...                                                  ...   \n","21437  Arabic Tokenization, Part-of-Speech Tagging an...   \n","21438  Using Semantically Motivated Estimates to Help...   \n","21439  A Mathematical Exploration of Why Language Mod...   \n","21440  Do You Know That Florence Is Packed with Visit...   \n","21441  LDA Topic Model with Soft Assignment of Descri...   \n","\n","                                                abstract  \n","11864  We present an efficient convolution kernel for...  \n","11865  Transfer learning is effective for improving t...  \n","11866  Syntactic features are useful for many text cl...  \n","11867  Adaptive optimization methods such as ADAGRAD,...  \n","11868  The most commonly used learning algorithm for ...  \n","...                                                  ...  \n","21437  We present an approach to using a morphologica...  \n","21438  Research into the automatic acquisition of sub...  \n","21439  Autoregressive language models, pretrained usi...  \n","21440  When a speaker, Mary, asks \"Do you know that F...  \n","21441  The LDA topic model is being used to model cor...  \n","\n","[9578 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-11e71278-0d4c-4bf6-9b17-f68934637b57\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>abstract</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>11864</th>\n","      <td>Spherical CNNs on Unstructured Grids</td>\n","      <td>We present an efficient convolution kernel for...</td>\n","    </tr>\n","    <tr>\n","      <th>11865</th>\n","      <td>Soft Representation Learning for Sparse Transfer</td>\n","      <td>Transfer learning is effective for improving t...</td>\n","    </tr>\n","    <tr>\n","      <th>11866</th>\n","      <td>Explicit and Implicit Syntactic Features for T...</td>\n","      <td>Syntactic features are useful for many text cl...</td>\n","    </tr>\n","    <tr>\n","      <th>11867</th>\n","      <td>Adaptive Gradient Methods with Dynamic Bound o...</td>\n","      <td>Adaptive optimization methods such as ADAGRAD,...</td>\n","    </tr>\n","    <tr>\n","      <th>11868</th>\n","      <td>Using Fast Weights to Improve Persistent Contr...</td>\n","      <td>The most commonly used learning algorithm for ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21437</th>\n","      <td>Arabic Tokenization, Part-of-Speech Tagging an...</td>\n","      <td>We present an approach to using a morphologica...</td>\n","    </tr>\n","    <tr>\n","      <th>21438</th>\n","      <td>Using Semantically Motivated Estimates to Help...</td>\n","      <td>Research into the automatic acquisition of sub...</td>\n","    </tr>\n","    <tr>\n","      <th>21439</th>\n","      <td>A Mathematical Exploration of Why Language Mod...</td>\n","      <td>Autoregressive language models, pretrained usi...</td>\n","    </tr>\n","    <tr>\n","      <th>21440</th>\n","      <td>Do You Know That Florence Is Packed with Visit...</td>\n","      <td>When a speaker, Mary, asks \"Do you know that F...</td>\n","    </tr>\n","    <tr>\n","      <th>21441</th>\n","      <td>LDA Topic Model with Soft Assignment of Descri...</td>\n","      <td>The LDA topic model is being used to model cor...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9578 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11e71278-0d4c-4bf6-9b17-f68934637b57')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-11e71278-0d4c-4bf6-9b17-f68934637b57 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-11e71278-0d4c-4bf6-9b17-f68934637b57');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":24}],"source":["df_valid"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1653213971491,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"P8wMDe8RRKSr","outputId":"0a4a8c19-c6a6-4048-d40e-ec3588148cc6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   title  \\\n","0      Natural Image Bases to Represent Neuroimaging ...   \n","1      Sluice Resolution without Hand-Crafted Feature...   \n","2      Learning Translation Models from Monolingual C...   \n","3           Sentiment Adaptive End-to-End Dialog Systems   \n","4          User-Friendly Text Prediction For Translators   \n","...                                                  ...   \n","11859  On Fast Adversarial Robustness Adaptation in M...   \n","11860  Syntactical Analysis of the Weaknesses of Sent...   \n","11861  CEM-RL: Combining evolutionary and gradient-ba...   \n","11862  Semi-Markov Conditional Random Fields for Info...   \n","11863  Learning to Correlate in Multi-Player General-...   \n","\n","                                                abstract  \n","0      Visual inspection of neuroimagery is susceptib...  \n","1      Sluice resolution in English is the problem of...  \n","2      Translation models often fail to generate good...  \n","3      End-to-end learning framework is useful for bu...  \n","4      Text prediction is a form of interactive machi...  \n","...                                                  ...  \n","11859  Model-agnostic meta-learning (MAML) has emerge...  \n","11860  We carry out a syntactic analysis of two state...  \n","11861  Deep neuroevolution and deep reinforcement lea...  \n","11862  We describe semi-Markov conditional random fie...  \n","11863  In the context of multi-player, general-sum ga...  \n","\n","[11864 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-f42d8c37-35c5-4dd4-9669-a21a6deaba3c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>abstract</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Natural Image Bases to Represent Neuroimaging ...</td>\n","      <td>Visual inspection of neuroimagery is susceptib...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sluice Resolution without Hand-Crafted Feature...</td>\n","      <td>Sluice resolution in English is the problem of...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Learning Translation Models from Monolingual C...</td>\n","      <td>Translation models often fail to generate good...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentiment Adaptive End-to-End Dialog Systems</td>\n","      <td>End-to-end learning framework is useful for bu...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>User-Friendly Text Prediction For Translators</td>\n","      <td>Text prediction is a form of interactive machi...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11859</th>\n","      <td>On Fast Adversarial Robustness Adaptation in M...</td>\n","      <td>Model-agnostic meta-learning (MAML) has emerge...</td>\n","    </tr>\n","    <tr>\n","      <th>11860</th>\n","      <td>Syntactical Analysis of the Weaknesses of Sent...</td>\n","      <td>We carry out a syntactic analysis of two state...</td>\n","    </tr>\n","    <tr>\n","      <th>11861</th>\n","      <td>CEM-RL: Combining evolutionary and gradient-ba...</td>\n","      <td>Deep neuroevolution and deep reinforcement lea...</td>\n","    </tr>\n","    <tr>\n","      <th>11862</th>\n","      <td>Semi-Markov Conditional Random Fields for Info...</td>\n","      <td>We describe semi-Markov conditional random fie...</td>\n","    </tr>\n","    <tr>\n","      <th>11863</th>\n","      <td>Learning to Correlate in Multi-Player General-...</td>\n","      <td>In the context of multi-player, general-sum ga...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11864 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f42d8c37-35c5-4dd4-9669-a21a6deaba3c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f42d8c37-35c5-4dd4-9669-a21a6deaba3c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f42d8c37-35c5-4dd4-9669-a21a6deaba3c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}],"source":["df_train"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":7146,"status":"ok","timestamp":1653214022949,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"eLws3YI2NyQ5","pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a152c307-919c-4681-bf37-c97b04e35495"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge_score\n","  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.21.6)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.0.0)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.0.4\n"]}],"source":["train_dataset = Dataset.from_pandas(df_train)\n","valid_dataset = Dataset.from_pandas(df_valid)\n","metric = load_metric(\"rouge\")\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":297,"status":"ok","timestamp":1653214032928,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"KRyNcZ_hThOm","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["max_input_length = 1024\n","max_target_length = 512\n","\n","def preprocess_function(examples):\n","    model_inputs = tokenizer(examples[\"abstract\"], max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"title\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1653214041899,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"oVj7bC9qZRLz","outputId":"c3c8af37-2f7d-4fe2-c97e-3e84f27725b0","pycharm":{"name":"#%%\n"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[0, 170, 1455, 41, 5693, 15380, 23794, 34751, 13, 30505, 23794, 337, 44304, 14641, 36, 16256, 29, 43, 15, 542, 25384, 4075, 38446, 634, 43797, 1538, 25406, 5990, 150, 5650, 15, 44787, 8724, 215, 25, 5730, 29762, 3156, 50, 33417, 8724, 4, 598, 42, 253, 6, 52, 3190, 9164, 15380, 23794, 45256, 19, 26956, 21092, 9, 25406, 5990, 14, 32, 19099, 30, 1532, 868, 17294, 4, 22248, 2617, 5990, 64, 28, 14146, 2319, 15, 542, 25384, 4075, 38446, 634, 65, 12, 4506, 6611, 6, 8, 1532, 868, 17294, 64, 28, 29854, 149, 2526, 124, 12, 27128, 1073, 1258, 4, 287, 10, 898, 6, 52, 6925, 2778, 5693, 26739, 4836, 14, 914, 50, 9980, 3899, 194, 12, 1116, 12, 627, 12, 2013, 1546, 41885, 11, 1110, 9, 819, 53, 19, 10, 3625, 2735, 346, 9, 1546, 17294, 4, 166, 10516, 84, 17194, 11, 41, 4935, 651, 9, 15491, 15, 10, 3143, 9, 3034, 3360, 8, 2147, 2866, 8558, 6, 217, 3989, 20257, 6, 2147, 6184, 2835, 1258, 6, 8, 32442, 808, 43606, 337, 2274, 46195, 2835, 1258, 4, 7806, 6, 52, 36, 134, 43, 1455, 10, 5808, 3480, 1548, 15, 542, 25384, 4075, 38446, 634, 43797, 1538, 25406, 5990, 13, 44787, 8724, 6, 8, 36, 176, 43, 311, 14, 84, 2216, 34751, 43797, 1938, 2386, 84, 1421, 7, 3042, 5, 276, 50, 723, 8611, 19, 3625, 4163, 1546, 17294, 4, 2], [0, 46552, 2239, 16, 2375, 13, 3927, 5, 819, 9, 8558, 14, 32, 1330, 6, 8, 19268, 12, 45025, 2239, 36, 11674, 574, 43, 8, 4415, 12, 1527, 5564, 2239, 36, 347, 6006, 43, 32, 505, 10960, 4, 152, 2225, 10648, 14, 543, 12, 46669, 5906, 3565, 6, 9, 543, 12, 438, 19519, 13171, 1373, 420, 430, 8558, 50, 11991, 6, 1395, 937, 2072, 157, 6, 77, 3565, 19, 10, 29213, 1330, 3685, 4, 5598, 403, 6, 61, 52, 486, 28593, 2937, 6, 429, 888, 2581, 819, 6, 10, 10632, 684, 25, 2430, 2937, 4, 1541, 5883, 16, 634, 37930, 27774, 1058, 420, 8558, 6, 7, 22, 24810, 12, 20414, 113, 1373, 8, 940, 5938, 6, 7, 1877, 5, 1373, 980, 1516, 350, 28593, 4, 96, 230, 6006, 6, 84, 1850, 9437, 9857, 277, 1539, 9, 4098, 19, 614, 12, 8634, 8135, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[0, 12582, 48152, 3480, 29, 15, 1890, 25384, 4075, 2974, 7823, 2], [0, 38805, 27893, 1258, 13807, 13, 2064, 21284, 18853, 2]]}"]},"metadata":{},"execution_count":30}],"source":["preprocess_function(valid_dataset[:2])\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["4286678b73c642bb97e935f5d2c4600e","1eb4cd407dcf4e959c6064be82705066","bb06931a52b847078d475fc87f6d5b42","abcdf66b1e2046bda0ab97ae859d19ce","da9a4d5bd5f6482e8021a554808fc1ce","2ed0daaa897442d68802772baf3e9cdc","9524577ab6684baa81c7a3925554e918","d1e002f26e2f4eb9a12d228ba3c71ac0","0a8ef3565d6845168f35f28d176e57ff","2bc0a0b92a4a48c89fbe2e1f780bd91f","2a144abbd4a84fbe997b000b865b0e62","59d645cf0b1746f2b37c5fa5d1b25ded","e3a85409ab9e42569a9d84f0b09d4edb","ff17f85d96a24f5eb4772dd4cf1ef448","2509f7f16cbf4feab214723bdda1b449","8211b8732eee4354b029a8ec0665d4d1","f1982962923c49ce8d0c51e98a3f086f","d402abbdd4684704bb3afb9d929927e3","26c579a0c3e14d36856156129fdfb31b","ea0222a71aa447f2b23230c2772ed9d7","f2e5f393c0e44c94ad4f33c578d60baa","c0e335a075564465a7866ddf04270886"]},"executionInfo":{"elapsed":17783,"status":"ok","timestamp":1653214062283,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"WsGePsUcSrkO","outputId":"6c22c113-dcc3-4292-b6db-95199eb7304e","pycharm":{"name":"#%%\n"}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4286678b73c642bb97e935f5d2c4600e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d645cf0b1746f2b37c5fa5d1b25ded"}},"metadata":{}}],"source":["train_dataset = train_dataset.map(preprocess_function, batched=True)\n","valid_dataset = valid_dataset.map(preprocess_function, batched=True)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":237,"status":"ok","timestamp":1653214066357,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"btKZ3RhjZ4Vx","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["batch_size = 8\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-lm_al_paper\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=3e-4,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=4,\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    num_train_epochs=3,\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=False,\n","    gradient_accumulation_steps=8,\n","    save_steps = 500,\n","    logging_steps = 185,\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":217,"status":"ok","timestamp":1653214077123,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"j6RYke5GbaYN","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"]},{"cell_type":"markdown","metadata":{"id":"dduVZjJZcLUa","pycharm":{"name":"#%% md\n"}},"source":["\n","\n","\n","```\n","Examples:\n","    >>> predictions = [\"hello there\", \"general kenobi\"]\n","    >>> references = [\"hello there\", \"general kenobi\"]\n","    >>> bertscore = datasets.load_metric(\"bertscore\")\n","    >>> results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n","    >>> print([round(v, 2) for v in results[\"f1\"]])\n","    [1.0, 1.0]\n","  \n","```\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":758,"status":"ok","timestamp":1653214083224,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"cCYda7Z3bgUC","outputId":"4fa0a62f-5e13-4f16-ec59-2bbfa0371552","pycharm":{"name":"#%%\n"}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import nltk\n","import numpy as np\n","nltk.download('punkt')\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    \n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14269,"status":"ok","timestamp":1653214113222,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"l7PrIlIVfXtX","outputId":"6ac8f5dd-f3a1-422b-9713-69322ee9191b","pycharm":{"name":"#%%\n"}},"outputs":[{"output_type":"stream","name":"stderr","text":["Using amp half precision backend\n"]}],"source":["trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=valid_dataset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"executionInfo":{"elapsed":64656,"status":"error","timestamp":1653214180495,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"CimnB8COfle1","outputId":"48ed45ca-6523-41e4-db95-b8b3a3c6e913","pycharm":{"name":"#%%\n"}},"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: abstract, __index_level_0__, title. If abstract, __index_level_0__, title are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 11864\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 8\n","  Total optimization steps = 555\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='43' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 43/555 01:01 < 12:51, 0.66 it/s, Epoch 0.23/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         )\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m         )\n\u001b[1;32m   1365\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m         )\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         attention_mask = self._prepare_decoder_attention_mask(\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m         )\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m_prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    908\u001b[0m             combined_attention_mask = _make_causal_mask(\n\u001b[1;32m    909\u001b[0m                 \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             ).to(self.device)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1915,"status":"ok","timestamp":1643553208387,"user":{"displayName":"yichen xie","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05370341903943803742"},"user_tz":-60},"id":"eXx-uoz8fuhf","outputId":"5ef1c3e3-852a-4caa-94b4-8ea034ce2846","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stderr","output_type":"stream","text":["Configuration saved in ./output/bart/config.json\n","Model weights saved in ./output/bart/pytorch_model.bin\n"]}],"source":["model.save_pretrained(f\"{PROJECT_ROOT}/model/BART-base/\")\n"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":285,"status":"error","timestamp":1653215326300,"user":{"displayName":"Linus Schwarz","userId":"05638686941640029327"},"user_tz":-120},"id":"lYGz849ime6l","outputId":"303790ab-f2a1-42fe-f3cd-1ca0f418dd99","pycharm":{"name":"#%%\n"}},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/BART-base/config.json\n","Model config BartConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/BART-base/config.json\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.1,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 12,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 12,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 128,\n","      \"min_length\": 12,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_cnn\": {\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 62,\n","      \"min_length\": 11,\n","      \"num_beams\": 6\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/BART-base/config.json\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    405\u001b[0m                         \u001b[0;34m\"model. Make sure you have saved the model properly.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                     ) from e\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unable to locate the file /content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/BART-base/config.json which is necessary to load this pretrained model. Make sure you have saved the model properly.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-bd4221724b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{PROJECT_ROOT}/model/BART-base/config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{PROJECT_ROOT}/model/BART-base/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[1;32m    448\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             raise OSError(\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0;34mf\"at '{checkpoint_file}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;34m\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/BART-base/config.json' at '/content/drive/MyDrive/DL4NLP/abstract-to-title-generation/model/BART-base/config.json'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."]}],"source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(f\"{PROJECT_ROOT}/model/BART-base/config.json\")\n","#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(f\"{PROJECT_ROOT}/model/BART-base/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3NmNvjfnevO","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import pandas as pd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":1152,"status":"ok","timestamp":1644160749389,"user":{"displayName":"yichen xie","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05370341903943803742"},"user_tz":-60},"id":"uda8rS7ES7Su","outputId":"4e7e8a2c-7b26-4973-9e28-5155d36e0969","pycharm":{"name":"#%%\n"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-4c84c594-7b7a-4e88-9b91-929a3e0385ea\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>title_length</th>\n","      <th>abstract_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Learning Latent Semantic Annotations for Groun...</td>\n","      <td>Previous work on grounded language learning di...</td>\n","      <td>11</td>\n","      <td>121</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Partially Supervised Sense Disambiguation by L...</td>\n","      <td>Supervised and semi-supervised sense disambigu...</td>\n","      <td>13</td>\n","      <td>140</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hawkes Processes for Continuous Time Sequence ...</td>\n","      <td>Classification of temporal textual data sequen...</td>\n","      <td>15</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A Unified Single Scan Algorithm for Japanese B...</td>\n","      <td>We describe an algorithm for Japanese analysis...</td>\n","      <td>13</td>\n","      <td>62</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Generating Coherent Event Schemas at Scale</td>\n","      <td>Chambers and Jurafsky (2009) demonstrated that...</td>\n","      <td>6</td>\n","      <td>127</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5356</th>\n","      <td>Bridging Information-Seeking Human Gaze and Ma...</td>\n","      <td>In this work, we analyze how human gaze during...</td>\n","      <td>8</td>\n","      <td>118</td>\n","    </tr>\n","    <tr>\n","      <th>5357</th>\n","      <td>Quantum-inspired Neural Network for Conversati...</td>\n","      <td>We provide a novel perspective on conversation...</td>\n","      <td>7</td>\n","      <td>116</td>\n","    </tr>\n","    <tr>\n","      <th>5358</th>\n","      <td>The BQ Corpus: A Large-scale Domain-specific C...</td>\n","      <td>This paper introduces the Bank Question (BQ) c...</td>\n","      <td>13</td>\n","      <td>174</td>\n","    </tr>\n","    <tr>\n","      <th>5359</th>\n","      <td>Doc2hash: Learning Discrete Latent variables f...</td>\n","      <td>Learning to hash via generative model has beco...</td>\n","      <td>8</td>\n","      <td>131</td>\n","    </tr>\n","    <tr>\n","      <th>5360</th>\n","      <td>Provable Benefits of Overparameterization in M...</td>\n","      <td>Deep networks are typically trained with many ...</td>\n","      <td>14</td>\n","      <td>228</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5361 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c84c594-7b7a-4e88-9b91-929a3e0385ea')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4c84c594-7b7a-4e88-9b91-929a3e0385ea button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4c84c594-7b7a-4e88-9b91-929a3e0385ea');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                  title  ... abstract_length\n","0     Learning Latent Semantic Annotations for Groun...  ...             121\n","1     Partially Supervised Sense Disambiguation by L...  ...             140\n","2     Hawkes Processes for Continuous Time Sequence ...  ...              68\n","3     A Unified Single Scan Algorithm for Japanese B...  ...              62\n","4            Generating Coherent Event Schemas at Scale  ...             127\n","...                                                 ...  ...             ...\n","5356  Bridging Information-Seeking Human Gaze and Ma...  ...             118\n","5357  Quantum-inspired Neural Network for Conversati...  ...             116\n","5358  The BQ Corpus: A Large-scale Domain-specific C...  ...             174\n","5359  Doc2hash: Learning Discrete Latent variables f...  ...             131\n","5360  Provable Benefits of Overparameterization in M...  ...             228\n","\n","[5361 rows x 4 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["test_samples = pd.read_csv(f\"{PROJECT_ROOT}/data/filtered/test_pairs.csv\", index_col=0)\n","test_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJkJ7UsNUR8E","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["abstracts = test_samples.abstract.to_list()\n","titles = test_samples.title.to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1644160798032,"user":{"displayName":"yichen xie","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05370341903943803742"},"user_tz":-60},"id":"aDHNxyi3nrFY","outputId":"3215f900-9cf7-45c5-d0e4-b6e71b23de1c","pycharm":{"name":"#%%\n"}},"outputs":[{"data":{"text/plain":["BartForConditionalGeneration(\n","  (model): BartModel(\n","    (shared): Embedding(50265, 768, padding_idx=1)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["model.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kt3IloOwZ_uY","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["def creat_eval_pairs(model, tokenizer, abstracts, titles):\n","  preds = []\n","  for abstract, title in zip(abstracts, titles):\n","    encoding = tokenizer.encode_plus(abstract, return_tensors = \"pt\")\n","    inputs = encoding[\"input_ids\"].to(\"cuda\")\n","    attention_masks = encoding[\"attention_mask\"].to(\"cuda\")\n","    title_ids = model.generate(\n","            input_ids = inputs,\n","            attention_mask = attention_masks,\n","            max_length = 30,\n","            num_beams = 5,\n","            num_return_sequences = 5,\n","            repetition_penalty=2.0, \n","            length_penalty=10.0,\n","            early_stopping = True,\n","            )\n","    result = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in title_ids]\n","    s=\"\"\n","    for t in result:\n","      s = s + \"<TITLE>\" + t\n","    preds.append(s)\n","    if len(preds) % 500 == 0:\n","      print(\"original title: \", title)\n","      print(\"generated title: \", preds[-1:])\n","  return preds, titles"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416116,"status":"ok","timestamp":1644165358168,"user":{"displayName":"yichen xie","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05370341903943803742"},"user_tz":-60},"id":"HKhygWb3bZMx","outputId":"1ac905e3-2db1-4df0-e5df-a994816e1bbc","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["original title:  Paraphrase-Driven Learning for Open Question Answering\n","generated title:  ['<TITLE>Learning a Semantic Lexicon and Linear Ranking Function for Question Answering<TITLE>Learning Semantic Lexicons and Linear Ranking Functions for Question Answering<TITLE>Learning a Semantic Lexicon and Linear Ranking Function for Open-Domain Questions<TITLE>Learning Semantic Lexicon and Linear Ranking Functions for Question Answering<TITLE>Learning Semantic Lexicons for Question Answering']\n","original title:  Robustness and Generalization of Role Sets: PropBank vs. VerbNet\n","generated title:  ['<TITLE>Robustness and Generalization of PropBank and VerbNet Roles for Semantic Role Labeling<TITLE>Robustness and Generalization of Two Alternative Role Sets for Semantic Role Labeling<TITLE>Robustness and Generalization of Alternative Role Sets for Semantic Role Labeling<TITLE>Robustness and Generalization of PropBank Roles for Semantic Role Labeling<TITLE>Robustness and Generalization of Alternative Roles for Semantic Role Labeling']\n","original title:  Analyzing the Persuasive Effect of Style in News Editorial Argumentation\n","generated title:  ['<TITLE>The Effect of Style in News Editorials on Persuasion<TITLE>Exploring the Effect of Style in News Editorials<TITLE>Style and Persuasion in News Editorials<TITLE>Style and Persuasion of News Editorials<TITLE>The Effect of Style in News Editorials']\n","original title:  Noise-Robust Morphological Disambiguation for Dialectal Arabic\n","generated title:  ['<TITLE>Neural Morphological Tagging and Disambiguation for Dialectal Arabic<TITLE>Neural Morphological Tagging and Disambiguation of Dialectal Text<TITLE>Neural Morphological Tagging and Disambiguation for Dialectal Text<TITLE>A Neural Morphological Tagging and Disambiguation Model for Arabic<TITLE>Neural Morphological Tagging and Disambiguation for Arabic']\n","original title:  Improving Interactive Machine Translation via Mouse Actions\n","generated title:  ['<TITLE>Mouse Actions as an Efficient Interface between Phrase-Based MT and InteractivePredictive Engine<TITLE>Mouse Actions as an Efficient Interface between Phrase-Based MT and InteractivePredictive Systems<TITLE>Mouse Actions as an Efficient Interface between Phrase-Based MT System and Interactive Prediction Engine<TITLE>Mouse Actions as an Efficient Interface for InteractivePredictive Machine Translation<TITLE>Microwave: Interactive Prediction for Machine Translation']\n","original title:  SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization\n","generated title:  ['<TITLE>Supervised Multi-Document Summarization Evaluation Metrics with Pseudo Reference Summaries<TITLE>Supervised Multi-Document Summarization Evaluation with Pseudo Reference Summaries and Rewards<TITLE>Supervised Multi-Document Summarization Evaluation with Pseudo Reference Summaries<TITLE>Supervised Multi-Document Summarization Evaluation Using Pseudo Reference Summaries<TITLE>Supervised Multi-Document Summarization Evaluation using Pseudo Reference Summaries']\n"]}],"source":["preds, titles = creat_eval_pairs(model, tokenizer, abstracts, titles)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"haa7O2kag0zL","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["pred_target_pairs = pd.DataFrame(list(zip(preds, titles)), columns=['predictions', 'targets'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8ra-1D0hfmj","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["pred_target_pairs.to_csv(f\"{PROJECT_ROOT}/output/preds_targets_pairs/bart-base.csv\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"bart_huggingface_trainer.ipynb","provenance":[{"file_id":"1DlV2tsDOx5o4ZQRKCAWsxHcUYcURFRHm","timestamp":1643234729469}]},"interpreter":{"hash":"fdc44dd37d6dc877b27f0694258de5d51e88dfff0bfeb39cc88c3996a865c9e8"},"kernelspec":{"display_name":"Python 3.10.3 64-bit ('dl4nlp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4286678b73c642bb97e935f5d2c4600e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1eb4cd407dcf4e959c6064be82705066","IPY_MODEL_bb06931a52b847078d475fc87f6d5b42","IPY_MODEL_abcdf66b1e2046bda0ab97ae859d19ce"],"layout":"IPY_MODEL_da9a4d5bd5f6482e8021a554808fc1ce"}},"1eb4cd407dcf4e959c6064be82705066":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ed0daaa897442d68802772baf3e9cdc","placeholder":"​","style":"IPY_MODEL_9524577ab6684baa81c7a3925554e918","value":"100%"}},"bb06931a52b847078d475fc87f6d5b42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1e002f26e2f4eb9a12d228ba3c71ac0","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a8ef3565d6845168f35f28d176e57ff","value":12}},"abcdf66b1e2046bda0ab97ae859d19ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bc0a0b92a4a48c89fbe2e1f780bd91f","placeholder":"​","style":"IPY_MODEL_2a144abbd4a84fbe997b000b865b0e62","value":" 12/12 [00:11&lt;00:00,  1.04ba/s]"}},"da9a4d5bd5f6482e8021a554808fc1ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed0daaa897442d68802772baf3e9cdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9524577ab6684baa81c7a3925554e918":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1e002f26e2f4eb9a12d228ba3c71ac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a8ef3565d6845168f35f28d176e57ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bc0a0b92a4a48c89fbe2e1f780bd91f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a144abbd4a84fbe997b000b865b0e62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59d645cf0b1746f2b37c5fa5d1b25ded":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3a85409ab9e42569a9d84f0b09d4edb","IPY_MODEL_ff17f85d96a24f5eb4772dd4cf1ef448","IPY_MODEL_2509f7f16cbf4feab214723bdda1b449"],"layout":"IPY_MODEL_8211b8732eee4354b029a8ec0665d4d1"}},"e3a85409ab9e42569a9d84f0b09d4edb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1982962923c49ce8d0c51e98a3f086f","placeholder":"​","style":"IPY_MODEL_d402abbdd4684704bb3afb9d929927e3","value":"100%"}},"ff17f85d96a24f5eb4772dd4cf1ef448":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26c579a0c3e14d36856156129fdfb31b","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea0222a71aa447f2b23230c2772ed9d7","value":10}},"2509f7f16cbf4feab214723bdda1b449":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2e5f393c0e44c94ad4f33c578d60baa","placeholder":"​","style":"IPY_MODEL_c0e335a075564465a7866ddf04270886","value":" 10/10 [00:05&lt;00:00,  2.29ba/s]"}},"8211b8732eee4354b029a8ec0665d4d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1982962923c49ce8d0c51e98a3f086f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d402abbdd4684704bb3afb9d929927e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26c579a0c3e14d36856156129fdfb31b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea0222a71aa447f2b23230c2772ed9d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2e5f393c0e44c94ad4f33c578d60baa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0e335a075564465a7866ddf04270886":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}