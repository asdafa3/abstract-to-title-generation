{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from config import *\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{PROJECT_ROOT}\"\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r \"{PROJECT_ROOT}/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f'{PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc pull -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset\n",
    "from tqdm import trange \n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from transformers import BertModel,BertPreTrainedModel\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title map for annotated sample-selection\n",
    "map80 = [[3, 2, 4, 6, 0, 5],\n",
    " [1, 5, 6, 3, 0, 4],\n",
    " [3, 2, 6, 1, 0, 4],\n",
    " [2, 1, 6, 5, 4, 0],\n",
    " [0, 3, 6, 5, 2, 4],\n",
    " [4, 3, 1, 2, 0, 6],\n",
    " [6, 1, 0, 4, 5, 3],\n",
    " [6, 0, 5, 1, 3, 2],\n",
    " [0, 3, 4, 1, 2, 5],\n",
    " [0, 3, 2, 6, 5, 4],\n",
    " [4, 3, 0, 5, 1, 6],\n",
    " [0, 1, 4, 6, 5, 2],\n",
    " [4, 0, 5, 1, 2, 6],\n",
    " [6, 2, 5, 0, 3, 1],\n",
    " [0, 1, 6, 4, 2, 5],\n",
    " [4, 6, 0, 3, 1, 5],\n",
    " [3, 1, 5, 4, 0, 2],\n",
    " [3, 5, 0, 2, 1, 6],\n",
    " [0, 6, 1, 2, 4, 3],\n",
    " [2, 4, 5, 1, 0, 6],\n",
    " [5, 3, 1, 2, 0, 4],\n",
    " [3, 6, 2, 4, 5, 0],\n",
    " [0, 5, 6, 3, 1, 2],\n",
    " [0, 3, 5, 6, 1, 4],\n",
    " [0, 4, 2, 5, 3, 1],\n",
    " [2, 3, 4, 0, 5, 1],\n",
    " [1, 2, 4, 0, 5, 6],\n",
    " [3, 5, 4, 1, 6, 0],\n",
    " [4, 6, 0, 1, 5, 2],\n",
    " [5, 0, 3, 1, 4, 6],\n",
    " [3, 6, 2, 5, 4, 0],\n",
    " [2, 3, 6, 5, 0, 4],\n",
    " [4, 0, 6, 3, 5, 1],\n",
    " [0, 2, 3, 5, 1, 6],\n",
    " [2, 1, 5, 0, 6, 3],\n",
    " [0, 6, 5, 1, 4, 2],\n",
    " [4, 5, 0, 2, 6, 1],\n",
    " [4, 2, 5, 6, 3, 0],\n",
    " [5, 3, 2, 4, 0, 1],\n",
    " [3, 5, 4, 2, 6, 0],\n",
    " [1, 2, 4, 3, 5, 0],\n",
    " [2, 3, 6, 1, 4, 0],\n",
    " [6, 3, 4, 0, 1, 2],\n",
    " [4, 0, 5, 1, 2, 3],\n",
    " [3, 4, 1, 0, 2, 6],\n",
    " [4, 0, 1, 3, 6, 2],\n",
    " [0, 5, 4, 6, 2, 3],\n",
    " [0, 1, 2, 6, 5, 3],\n",
    " [5, 3, 0, 4, 1, 6],\n",
    " [5, 4, 1, 0, 2, 6],\n",
    " [6, 4, 1, 5, 3, 0],\n",
    " [4, 0, 5, 3, 2, 6],\n",
    " [4, 0, 1, 5, 6, 2],\n",
    " [6, 0, 4, 5, 3, 2],\n",
    " [6, 0, 2, 1, 4, 3],\n",
    " [3, 2, 4, 1, 0, 5],\n",
    " [4, 2, 0, 5, 1, 3],\n",
    " [0, 2, 6, 3, 5, 4],\n",
    " [4, 1, 6, 0, 2, 5],\n",
    " [6, 4, 2, 1, 3, 0],\n",
    " [2, 5, 3, 4, 0, 1],\n",
    " [6, 4, 1, 0, 3, 5],\n",
    " [0, 2, 6, 1, 4, 5],\n",
    " [6, 5, 4, 3, 0, 2],\n",
    " [3, 2, 4, 6, 5, 0],\n",
    " [2, 1, 4, 3, 0, 5],\n",
    " [1, 6, 2, 3, 0, 5],\n",
    " [2, 5, 0, 6, 4, 3],\n",
    " [3, 2, 0, 4, 1, 6],\n",
    " [0, 3, 2, 4, 6, 1],\n",
    " [0, 5, 2, 3, 1, 4],\n",
    " [6, 4, 0, 2, 5, 1],\n",
    " [6, 0, 3, 5, 1, 4],\n",
    " [0, 5, 3, 4, 2, 6],\n",
    " [5, 3, 2, 0, 4, 1],\n",
    " [1, 4, 3, 5, 0, 6],\n",
    " [6, 2, 5, 3, 0, 4],\n",
    " [0, 5, 3, 6, 2, 1],\n",
    " [2, 0, 4, 6, 5, 1],\n",
    " [0, 4, 2, 1, 3, 5]]\n",
    "\n",
    "selected_index = [153, 154, 156, 159, 161, 164, 165, 167, 168, 172, 174 ,175, 176, 177, 180, 185, 186, 189, 191, 197, 206, 207, 208, 211, 216, 218, 221, 223, 225, 227, 228, 238, 239, 241, 243, 244, 248, 250, 259, 260, 262, 269, 270, 271 ,275, 287, 288, 291, 294, 299]\n",
    "map50= [[1, 0, 3, 4, 6, 5],\n",
    " [2, 1, 3, 0, 4, 6],\n",
    " [2, 3, 4, 1, 6, 0],\n",
    " [1, 0, 6, 5, 2, 3],\n",
    " [0, 1, 5, 3, 4, 6],\n",
    " [3, 2, 1, 5, 0, 4],\n",
    " [1, 0, 6, 5, 4, 2],\n",
    " [0, 4, 2, 6, 1, 3],\n",
    " [0, 3, 6, 1, 2, 5],\n",
    " [6, 2, 1, 4, 0, 3],\n",
    " [5, 0, 4, 2, 6, 3],\n",
    " [1, 0, 4, 3, 5, 2],\n",
    " [4, 0, 5, 6, 2, 3],\n",
    " [6, 1, 0, 4, 3, 5],\n",
    " [5, 6, 0, 3, 1, 2],\n",
    " [2, 5, 3, 4, 0, 6],\n",
    " [0, 2, 6, 5, 4, 1],\n",
    " [0, 3, 2, 1, 4, 6],\n",
    " [2, 5, 1, 6, 0, 4],\n",
    " [2, 5, 3, 1, 4, 0],\n",
    " [3, 4, 1, 6, 2, 0],\n",
    " [6, 3, 2, 1, 0, 5],\n",
    " [0, 6, 5, 1, 2, 4],\n",
    " [0, 6, 1, 3, 5, 4],\n",
    " [3, 2, 5, 1, 4, 0],\n",
    " [3, 4, 0, 5, 1, 6],\n",
    " [2, 5, 4, 0, 6, 1],\n",
    " [4, 2, 1, 6, 0, 3],\n",
    " [2, 4, 5, 6, 0, 3],\n",
    " [0, 5, 6, 2, 3, 1],\n",
    " [0, 5, 4, 3, 1, 2],\n",
    " [4, 1, 6, 5, 2, 0],\n",
    " [5, 3, 1, 0, 2, 6],\n",
    " [1, 5, 2, 4, 3, 0],\n",
    " [2, 1, 0, 3, 4, 5],\n",
    " [2, 0, 4, 5, 6, 3],\n",
    " [2, 5, 4, 1, 6, 0],\n",
    " [0, 5, 2, 1, 6, 4],\n",
    " [0, 5, 2, 4, 3, 1],\n",
    " [5, 2, 6, 1, 0, 3],\n",
    " [0, 2, 3, 6, 4, 5],\n",
    " [4, 5, 6, 2, 3, 0],\n",
    " [6, 2, 5, 4, 1, 0],\n",
    " [2, 6, 0, 1, 3, 5],\n",
    " [0, 1, 4, 5, 6, 2],\n",
    " [5, 1, 4, 6, 0, 3],\n",
    " [6, 0, 5, 4, 2, 1],\n",
    " [6, 4, 1, 0, 3, 2],\n",
    " [5, 0, 2, 3, 4, 6],\n",
    " [2, 3, 0, 1, 6, 5]]\n",
    "\n",
    "map_model = [['bart_xsum', 'bart_cnn', 't5', 'pegasus_xsum', 'original', 'gpt2'],\n",
    " ['bart_base', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'original', 't5'],\n",
    " ['bart_xsum', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 't5'],\n",
    " ['bart_cnn', 'bart_base', 'pegasus_xsum', 'gpt2', 't5', 'original'],\n",
    " ['original', 'bart_xsum', 'pegasus_xsum', 'gpt2', 'bart_cnn', 't5'],\n",
    " ['t5', 'bart_xsum', 'bart_base', 'bart_cnn', 'original', 'pegasus_xsum'],\n",
    " ['pegasus_xsum', 'bart_base', 'original', 't5', 'gpt2', 'bart_xsum'],\n",
    " ['pegasus_xsum', 'original', 'gpt2', 'bart_base', 'bart_xsum', 'bart_cnn'],\n",
    " ['original', 'bart_xsum', 't5', 'bart_base', 'bart_cnn', 'gpt2'],\n",
    " ['original', 'bart_xsum', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5'],\n",
    " ['t5', 'bart_xsum', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
    " ['original', 'bart_base', 't5', 'pegasus_xsum', 'gpt2', 'bart_cnn'],\n",
    " ['t5', 'original', 'gpt2', 'bart_base', 'bart_cnn', 'pegasus_xsum'],\n",
    " ['pegasus_xsum', 'bart_cnn', 'gpt2', 'original', 'bart_xsum', 'bart_base'],\n",
    " ['original', 'bart_base', 'pegasus_xsum', 't5', 'bart_cnn', 'gpt2'],\n",
    " ['t5', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'gpt2'],\n",
    " ['bart_xsum', 'bart_base', 'gpt2', 't5', 'original', 'bart_cnn'],\n",
    " ['bart_xsum', 'gpt2', 'original', 'bart_cnn', 'bart_base', 'pegasus_xsum'],\n",
    " ['original', 'pegasus_xsum', 'bart_base', 'bart_cnn', 't5', 'bart_xsum'],\n",
    " ['bart_cnn', 't5', 'gpt2', 'bart_base', 'original', 'pegasus_xsum'],\n",
    " ['gpt2', 'bart_xsum', 'bart_base', 'bart_cnn', 'original', 't5'],\n",
    " ['bart_xsum', 'pegasus_xsum', 'bart_cnn', 't5', 'gpt2', 'original'],\n",
    " ['original', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
    " ['original', 'bart_xsum', 'gpt2', 'pegasus_xsum', 'bart_base', 't5'],\n",
    " ['original', 't5', 'bart_cnn', 'gpt2', 'bart_xsum', 'bart_base'],\n",
    " ['bart_cnn', 'bart_xsum', 't5', 'original', 'gpt2', 'bart_base'],\n",
    " ['bart_base', 'bart_cnn', 't5', 'original', 'gpt2', 'pegasus_xsum'],\n",
    " ['bart_xsum', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
    " ['t5', 'pegasus_xsum', 'original', 'bart_base', 'gpt2', 'bart_cnn'],\n",
    " ['gpt2', 'original', 'bart_xsum', 'bart_base', 't5', 'pegasus_xsum'],\n",
    " ['bart_xsum', 'pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'original'],\n",
    " ['bart_cnn', 'bart_xsum', 'pegasus_xsum', 'gpt2', 'original', 't5'],\n",
    " ['t5', 'original', 'pegasus_xsum', 'bart_xsum', 'gpt2', 'bart_base'],\n",
    " ['original', 'bart_cnn', 'bart_xsum', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'bart_base', 'gpt2', 'original', 'pegasus_xsum', 'bart_xsum'],\n",
    " ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 't5', 'bart_cnn'],\n",
    " ['t5', 'gpt2', 'original', 'bart_cnn', 'pegasus_xsum', 'bart_base'],\n",
    " ['t5', 'bart_cnn', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'original'],\n",
    " ['gpt2', 'bart_xsum', 'bart_cnn', 't5', 'original', 'bart_base'],\n",
    " ['bart_xsum', 'gpt2', 't5', 'bart_cnn', 'pegasus_xsum', 'original'],\n",
    " ['bart_base', 'bart_cnn', 't5', 'bart_xsum', 'gpt2', 'original'],\n",
    " ['bart_cnn', 'bart_xsum', 'pegasus_xsum', 'bart_base', 't5', 'original'],\n",
    " ['pegasus_xsum', 'bart_xsum', 't5', 'original', 'bart_base', 'bart_cnn'],\n",
    " ['t5', 'original', 'gpt2', 'bart_base', 'bart_cnn', 'bart_xsum'],\n",
    " ['bart_xsum', 't5', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
    " ['t5', 'original', 'bart_base', 'bart_xsum', 'pegasus_xsum', 'bart_cnn'],\n",
    " ['original', 'gpt2', 't5', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n",
    " ['original', 'bart_base', 'bart_cnn', 'pegasus_xsum', 'gpt2', 'bart_xsum'],\n",
    " ['gpt2', 'bart_xsum', 'original', 't5', 'bart_base', 'pegasus_xsum'],\n",
    " ['gpt2', 't5', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
    " ['bart_base', 'original', 'bart_xsum', 't5', 'pegasus_xsum', 'gpt2'],\n",
    " ['bart_cnn', 'bart_base', 'bart_xsum', 'original', 't5', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
    " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'bart_xsum'],\n",
    " ['original', 'bart_base', 'gpt2', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
    " ['bart_xsum', 'bart_cnn', 'bart_base', 'gpt2', 'original', 't5'],\n",
    " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 't5', 'bart_cnn'],\n",
    " ['original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'bart_xsum'],\n",
    " ['original', 'bart_xsum', 'pegasus_xsum', 'bart_base', 'bart_cnn', 'gpt2'],\n",
    " ['pegasus_xsum', 'bart_cnn', 'bart_base', 't5', 'original', 'bart_xsum'],\n",
    " ['gpt2', 'original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_xsum'],\n",
    " ['bart_base', 'original', 't5', 'bart_xsum', 'gpt2', 'bart_cnn'],\n",
    " ['t5', 'original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n",
    " ['pegasus_xsum', 'bart_base', 'original', 't5', 'bart_xsum', 'gpt2'],\n",
    " ['gpt2', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
    " ['bart_cnn', 'gpt2', 'bart_xsum', 't5', 'original', 'pegasus_xsum'],\n",
    " ['original', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5', 'bart_base'],\n",
    " ['original', 'bart_xsum', 'bart_cnn', 'bart_base', 't5', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'gpt2', 'bart_base', 'pegasus_xsum', 'original', 't5'],\n",
    " ['bart_cnn', 'gpt2', 'bart_xsum', 'bart_base', 't5', 'original'],\n",
    " ['bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'bart_cnn', 'original'],\n",
    " ['pegasus_xsum', 'bart_xsum', 'bart_cnn', 'bart_base', 'original', 'gpt2'],\n",
    " ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 'bart_cnn', 't5'],\n",
    " ['original', 'pegasus_xsum', 'bart_base', 'bart_xsum', 'gpt2', 't5'],\n",
    " ['bart_xsum', 'bart_cnn', 'gpt2', 'bart_base', 't5', 'original'],\n",
    " ['bart_xsum', 't5', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'gpt2', 't5', 'original', 'pegasus_xsum', 'bart_base'],\n",
    " ['t5', 'bart_cnn', 'bart_base', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
    " ['bart_cnn', 't5', 'gpt2', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
    " ['original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'bart_base'],\n",
    " ['original', 'gpt2', 't5', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
    " ['t5', 'bart_base', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'original'],\n",
    " ['gpt2', 'bart_xsum', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
    " ['bart_base', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'original'],\n",
    " ['bart_cnn', 'bart_base', 'original', 'bart_xsum', 't5', 'gpt2'],\n",
    " ['bart_cnn', 'original', 't5', 'gpt2', 'pegasus_xsum', 'bart_xsum'],\n",
    " ['bart_cnn', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
    " ['original', 'gpt2', 'bart_cnn', 'bart_base', 'pegasus_xsum', 't5'],\n",
    " ['original', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'bart_base'],\n",
    " ['gpt2', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 'bart_xsum'],\n",
    " ['original', 'bart_cnn', 'bart_xsum', 'pegasus_xsum', 't5', 'gpt2'],\n",
    " ['t5', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'original'],\n",
    " ['pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'bart_base', 'original'],\n",
    " ['bart_cnn', 'pegasus_xsum', 'original', 'bart_base', 'bart_xsum', 'gpt2'],\n",
    " ['original', 'bart_base', 't5', 'gpt2', 'pegasus_xsum', 'bart_cnn'],\n",
    " ['gpt2', 'bart_base', 't5', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
    " ['pegasus_xsum', 'original', 'gpt2', 't5', 'bart_cnn', 'bart_base'],\n",
    " ['pegasus_xsum', 't5', 'bart_base', 'original', 'bart_xsum', 'bart_cnn'],\n",
    " ['gpt2', 'original', 'bart_cnn', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'bart_xsum', 'original', 'bart_base', 'pegasus_xsum', 'gpt2'],\n",
    " ['bart_base', 'original', 'bart_xsum', 't5', 'pegasus_xsum', 'gpt2'],\n",
    " ['bart_cnn', 'bart_base', 'bart_xsum', 'original', 't5', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
    " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'bart_xsum'],\n",
    " ['original', 'bart_base', 'gpt2', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
    " ['bart_xsum', 'bart_cnn', 'bart_base', 'gpt2', 'original', 't5'],\n",
    " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 't5', 'bart_cnn'],\n",
    " ['original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'bart_xsum'],\n",
    " ['original', 'bart_xsum', 'pegasus_xsum', 'bart_base', 'bart_cnn', 'gpt2'],\n",
    " ['pegasus_xsum', 'bart_cnn', 'bart_base', 't5', 'original', 'bart_xsum'],\n",
    " ['gpt2', 'original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_xsum'],\n",
    " ['bart_base', 'original', 't5', 'bart_xsum', 'gpt2', 'bart_cnn'],\n",
    " ['t5', 'original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n",
    " ['pegasus_xsum', 'bart_base', 'original', 't5', 'bart_xsum', 'gpt2'],\n",
    " ['gpt2', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
    " ['bart_cnn', 'gpt2', 'bart_xsum', 't5', 'original', 'pegasus_xsum'],\n",
    " ['original', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5', 'bart_base'],\n",
    " ['original', 'bart_xsum', 'bart_cnn', 'bart_base', 't5', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'gpt2', 'bart_base', 'pegasus_xsum', 'original', 't5'],\n",
    " ['bart_cnn', 'gpt2', 'bart_xsum', 'bart_base', 't5', 'original'],\n",
    " ['bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'bart_cnn', 'original'],\n",
    " ['pegasus_xsum', 'bart_xsum', 'bart_cnn', 'bart_base', 'original', 'gpt2'],\n",
    " ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 'bart_cnn', 't5'],\n",
    " ['original', 'pegasus_xsum', 'bart_base', 'bart_xsum', 'gpt2', 't5'],\n",
    " ['bart_xsum', 'bart_cnn', 'gpt2', 'bart_base', 't5', 'original'],\n",
    " ['bart_xsum', 't5', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'gpt2', 't5', 'original', 'pegasus_xsum', 'bart_base'],\n",
    " ['t5', 'bart_cnn', 'bart_base', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
    " ['bart_cnn', 't5', 'gpt2', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
    " ['original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'bart_base'],\n",
    " ['original', 'gpt2', 't5', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
    " ['t5', 'bart_base', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'original'],\n",
    " ['gpt2', 'bart_xsum', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
    " ['bart_base', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'original'],\n",
    " ['bart_cnn', 'bart_base', 'original', 'bart_xsum', 't5', 'gpt2'],\n",
    " ['bart_cnn', 'original', 't5', 'gpt2', 'pegasus_xsum', 'bart_xsum'],\n",
    " ['bart_cnn', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
    " ['original', 'gpt2', 'bart_cnn', 'bart_base', 'pegasus_xsum', 't5'],\n",
    " ['original', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'bart_base'],\n",
    " ['gpt2', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 'bart_xsum'],\n",
    " ['original', 'bart_cnn', 'bart_xsum', 'pegasus_xsum', 't5', 'gpt2'],\n",
    " ['t5', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'original'],\n",
    " ['pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'bart_base', 'original'],\n",
    " ['bart_cnn', 'pegasus_xsum', 'original', 'bart_base', 'bart_xsum', 'gpt2'],\n",
    " ['original', 'bart_base', 't5', 'gpt2', 'pegasus_xsum', 'bart_cnn'],\n",
    " ['gpt2', 'bart_base', 't5', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
    " ['pegasus_xsum', 'original', 'gpt2', 't5', 'bart_cnn', 'bart_base'],\n",
    " ['pegasus_xsum', 't5', 'bart_base', 'original', 'bart_xsum', 'bart_cnn'],\n",
    " ['gpt2', 'original', 'bart_cnn', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
    " ['bart_cnn', 'bart_xsum', 'original', 'bart_base', 'pegasus_xsum', 'gpt2']]\n",
    "\n",
    "\n",
    "def displaysmaples(samples):\n",
    "  i = 0\n",
    "  titles = samples.title.to_list()\n",
    "  abstract = samples.abstract.to_list()\n",
    "  bart_base = samples.bart_base.to_list()\n",
    "  bart_cnn = samples.bart_cnn.to_list()\n",
    "  bart_xsum = samples.bart_xsum.to_list()\n",
    "  t5_small = samples.t5_small.to_list()\n",
    "  gpt2 = samples.gpt2.to_list()\n",
    "  pegasus_xsum = samples.pegasus_xsum.to_list()\n",
    "\n",
    "  for t, a, t1, t2, t3, t4, t5, t6 in zip(titles, abstract, bart_base, bart_cnn, bart_xsum, t5_small, gpt2, pegasus_xsum):\n",
    "    print(\"original title: \", t)\n",
    "    print(\"abstract: \", a)\n",
    "    print(\"from bart_base: \")\n",
    "    gt1 = t1.split(\"<TITLE>\")[1:]\n",
    "    for gt in gt1:\n",
    "      print(gt)\n",
    "    print(\"\\n\")\n",
    "    print(\"from bart_cnn: \")\n",
    "    gt1 = t2.split(\"<TITLE>\")[1:]\n",
    "    for gt in gt1:\n",
    "      print(gt)\n",
    "    print(\"\\n\")\n",
    "    print(\"from bart_xsum: \")\n",
    "    gt1 = t3.split(\"<TITLE>\")[1:]\n",
    "    for gt in gt1:\n",
    "      print(gt)\n",
    "    print(\"\\n\")\n",
    "    print(\"from t5_small: \")\n",
    "    gt1 = t4.split(\"<TITLE>\")[1:]\n",
    "    for gt in gt1:\n",
    "      print(gt)\n",
    "    print(\"\\n\")\n",
    "    print(\"from gpt2: \")\n",
    "    gt1 = t5.split(\"<TITLE>\")[1:]\n",
    "    for gt in gt1:\n",
    "      print(gt)\n",
    "    print(\"\\n\")\n",
    "    print(\"from pegasus_xsum: \")\n",
    "    gt1 = t6.split(\"<TITLE>\")[1:]\n",
    "    for gt in gt1:\n",
    "      print(gt)\n",
    "    print(\"\\n\")\n",
    "    i = i + 1\n",
    "\n",
    "    if i >= 20:\n",
    "      break\n",
    "\n",
    "# index von Optionen der human evaluation, 20 - 99 samples und 100-150(von index 0 bis 49)\n",
    "# [4, 3, 5, 2, 0, 1] bedeutet \n",
    "#1.op aus t5_samall, \n",
    "#2.op aus bart_xsum,\n",
    "#3.op aus gpt2,\n",
    "#4.op aus bart_cnn,\n",
    "#5.op original,\n",
    "#6.op aus bart_base,\n",
    "\n",
    "def getindex():\n",
    "  result = []\n",
    "  for i in range(80):\n",
    "    col = np.arange(1,7)\n",
    "    np.random.shuffle(col)\n",
    "    col = col.tolist()[:-1]\n",
    "    col.append(0)\n",
    "    np.random.shuffle(col)\n",
    "    result.append(col)\n",
    "  return result\n",
    "\n",
    "\n",
    "def sampler(samples, map20, start, end):\n",
    "  i = start\n",
    "  titles = samples.title.to_list()[start: end]\n",
    "  abstract = samples.abstract.to_list()[start: end]\n",
    "  bart_base = samples.bart_base.to_list()[start: end]\n",
    "  bart_cnn = samples.bart_cnn.to_list()[start: end]\n",
    "  bart_xsum = samples.bart_xsum.to_list()[start: end]\n",
    "  t5_small = samples.t5_small.to_list()[start: end]\n",
    "  gpt2 = samples.gpt2.to_list()[start: end]\n",
    "  pegasus_xsum = samples.pegasus_xsum.to_list()[start: end]\n",
    "\n",
    "  tmap = []\n",
    "  for t, a, t1, t2, t3, t4, t5, t6 in zip(titles, abstract, bart_base, bart_cnn, bart_xsum, t5_small, gpt2, pegasus_xsum):\n",
    "    gt1 = t1.split(\"<TITLE>\")[1:][0]\n",
    "    gt2 = t2.split(\"<TITLE>\")[1:][0]\n",
    "    gt3 = t3.split(\"<TITLE>\")[1:][0]\n",
    "    gt4 = t4.split(\"<TITLE>\")[1:][0]\n",
    "    gt5 = t5.split(\"<TITLE>\")[1:][0]\n",
    "    gt6 = t6.split(\"<TITLE>\")[1:][0]\n",
    "    col = [t, gt1, gt2, gt3, gt4, gt5, gt6]\n",
    "    rcol = [col[map20[i][j]] for j in range(6)]\n",
    "    rcol.append(a)\n",
    "    tmap.append(rcol)\n",
    "    i = i + 1\n",
    "\n",
    "  return tmap\n",
    "def map2model(ls):\n",
    "  r = []\n",
    "  for i in ls:\n",
    "    if i == 0:\n",
    "      r.append(\"original\")\n",
    "    if i == 1:\n",
    "      r.append(\"bart_base\")\n",
    "    if i == 2:\n",
    "      r.append(\"bart_cnn\")\n",
    "    if i == 3:\n",
    "      r.append(\"bart_xsum\")\n",
    "    if i == 4:\n",
    "      r.append(\"t5\")\n",
    "    if i == 5:\n",
    "      r.append(\"gpt2\")\n",
    "    if i == 6:\n",
    "      r.append(\"pegasus_xsum\")\n",
    "  return r\n",
    "\n",
    "def map2index(ls):\n",
    "  r = []\n",
    "  for i in ls:\n",
    "    if i == \"original\":\n",
    "      r.append(0)\n",
    "    if i == \"bart_base\":\n",
    "      r.append(1)\n",
    "    if i == \"bart_cnn\":\n",
    "      r.append(2)\n",
    "    if i == \"bart_xsum\":\n",
    "      r.append(3)\n",
    "    if i == \"t5\":\n",
    "      r.append(4)\n",
    "    if i == \"gpt2\":\n",
    "      r.append(5)\n",
    "    if i == \"pegasus_xsum\":\n",
    "      r.append(6)\n",
    "  return r\n",
    "\n",
    "def map2modelAM(ls):\n",
    "  r = []\n",
    "  for i in ls:\n",
    "    if i == 0:\n",
    "      r.append(\"bart_base\")\n",
    "    if i == 1:\n",
    "      r.append(\"bart_cnn\")\n",
    "    if i == 2:\n",
    "      r.append(\"bart_xsum\")\n",
    "    if i == 3:\n",
    "      r.append(\"t5\")\n",
    "    if i == 4:\n",
    "      r.append(\"gpt2\")\n",
    "    if i == 5:\n",
    "      r.append(\"pegasus_xsum\")\n",
    "  return r\n",
    "\n",
    "# index von Optionen der human evaluation, 150 - 200 samples\n",
    "# [4, 3, 5, 2, 0, 1] bedeutet \n",
    "#1.op aus t5_samall, \n",
    "#2.op aus bart_xsum,\n",
    "#3.op aus gpt2,\n",
    "#4.op aus bart_cnn,\n",
    "#5.op original,\n",
    "#6.op aus bart_base,\n",
    "\n",
    "def getindex():\n",
    "  result = []\n",
    "  for i in range(50):\n",
    "    col = np.arange(1,7)\n",
    "    np.random.shuffle(col)\n",
    "    col = col.tolist()[:-1]\n",
    "    col.append(0)\n",
    "    np.random.shuffle(col)\n",
    "    result.append(col)\n",
    "  return result\n",
    "\n",
    "#map für Teinehmer\n",
    "\n",
    "def getTmap():\n",
    "  i = 0\n",
    "  b = False\n",
    "  map30 = []\n",
    "  counters = np.zeros(20).tolist()\n",
    "  while not b:\n",
    "    personMap = np.arange(20)\n",
    "    np.random.shuffle(personMap)\n",
    "    personMap = personMap[:10]\n",
    "    i0 = personMap[0]\n",
    "    i1 = personMap[1]\n",
    "\n",
    "    if (counters[i0] < 3) and (counters[i1] < 3):\n",
    "      counters[i0] = counters[i0] + 1\n",
    "      counters[i1] = counters[i1] + 1\n",
    "      map30.append(personMap)\n",
    "    tb = True\n",
    "    for t in counters:\n",
    "      tb = tb and (t == 3)\n",
    "    b = tb\n",
    "    if b == True:\n",
    "      print(\"terminiert at:\", counters)\n",
    "  return map30\n",
    "\n",
    "# Settings\n",
    "\n",
    "def setup_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.backends.cudnn.deteministic = True\n",
    "\n",
    "MODEL_OUT_DIR = '/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/'\n",
    "\n",
    "## Model Configurations\n",
    "p={\n",
    "    'max_len': 512,\n",
    "    'batch_size': 6,\n",
    "    'lr' : 4.0638e-05,\n",
    "    'epochs': 18,\n",
    "    'dropout': 0.5,\n",
    "    'num_threads' : 1,\n",
    "    'model_name' : 'allenai/scibert_scivocab_uncased',\n",
    "    #'model_name' : 'bert-base-uncased',\n",
    "    'do_train' : False,\n",
    "    'random_seed': 24\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Excerpt_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen, tokenizer): \n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = data.reset_index()\n",
    "        #Initialize the tokenizer for the desired transformer model\n",
    "        self.tokenizer = tokenizer\n",
    "        #Maximum length of the tokens list to keep all the sequences of fixed size\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        #Select the sentence and label at the specified index in the data frame\n",
    "        excerpt = self.df.loc[index, 'excerpt']\n",
    "        try:\n",
    "            target = float(self.df.loc[index, 'target'])\n",
    "        except:\n",
    "            target = 0.0\n",
    "        #identifier = self.df.loc[index, 'id']\n",
    "        #Preprocess the text to be suitable for the transformer\n",
    "        tokens = self.tokenizer.tokenize(excerpt) \n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] \n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n",
    "        #Obtain the indices of the tokens in the BERT Vocabulary\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \n",
    "        input_ids = torch.tensor(input_ids) \n",
    "        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        \n",
    "        target = torch.tensor([target], dtype=torch.float32)\n",
    "        \n",
    "        return input_ids, attention_mask, target\n",
    "\n",
    "# Init model\n",
    "\n",
    "class BertRegresser(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        #The output layer that takes the [CLS] representation and gives an output\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(p['dropout']),\n",
    "            nn.Linear(768, 1))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #Feed the input to Bert model to obtain contextualized representations\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #Obtain the representations of [CLS] heads\n",
    "        logits = outputs[1]\n",
    "        output = self.regressor(logits)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Train, eval function\n",
    "\n",
    "def evaluate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    mean_acc, mean_loss, count = 0, 0, 0\n",
    "    preds = []\n",
    "    lst_label = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, target in (dataloader):\n",
    "            \n",
    "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            preds += output\n",
    "            lst_label += target\n",
    "            mean_loss += criterion(output, target.type_as(output)).item()\n",
    "            # mean_err += get_rmse(output, target)\n",
    "            count += 1\n",
    "        predss = np.array([x.cpu().data.numpy().tolist() for x in preds]).squeeze()\n",
    "        lst_labels = np.array([x.cpu().data.numpy().tolist() for x in lst_label]).squeeze()\n",
    "        corr = stats.spearmanr(predss, lst_labels)\n",
    "    return corr[0] #mean_loss/count\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs, device):\n",
    "    best_acc = 0\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):\n",
    "            optimizer.zero_grad()  \n",
    "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "            \n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(output, target.type_as(output))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f\"Training loss is {train_loss/len(train_loader)}\")\n",
    "        val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n",
    "        print(\"Epoch {} complete! Correlations : {}\".format(epoch, val_loss))\n",
    "\n",
    "## Configuration loaded from AutoConfig \n",
    "aconfig = AutoConfig.from_pretrained(p['model_name'])\n",
    "## Tokenizer loaded from AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(p['model_name'])\n",
    "## Creating the model from the desired transformer model\n",
    "model = BertRegresser.from_pretrained(p['model_name'], config=aconfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze all layers except regression head\n",
    "\n",
    "\"\"\"unfreeze_layers = ['bert.pooler', 'regressor.1']\n",
    "for name, params in model.named_parameters():\n",
    "  params.requires_grad = False\n",
    "  for ele in unfreeze_layers:\n",
    "    if ele in name:\n",
    "      params.requires_grad = True\n",
    "      break\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "  if params.requires_grad:\n",
    "    print(name, params.size())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in model.named_parameters():\n",
    "  if params.requires_grad:\n",
    "    print(name, params.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## Putting model to device\n",
    "model = model.to(device)\n",
    "## Takes as the input the logits of the positive class and computes the binary cross-entropy \n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "## Optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=p['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map = pd.read_csv(f'{DATA_DIR}/annotated/Kopie von 140_annotations_pairs.csv', index_col=0)\n",
    "scores = pd.read_csv(f'{DATA_DIR}/annotated/Kopie von 140_humanannotation_withoutunannotated.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_df =text_map[['abstract']]\n",
    "title_df = text_map[['title', 'bart_base', 'bart_cnn', 'bart_xsum', 't5_small', 'gpt2', 'pegasus_xsum']]\n",
    "abstract_np = abstract_df.to_numpy()\n",
    "scores_np = scores.to_numpy()\n",
    "title_np = title_df.to_numpy()\n",
    "idx_map = [map2index(row) for row in map_model[:140]]\n",
    "title_np_picked = np.array([np.take(row1, np.sort(row2)) for row1, row2 in zip(title_np, idx_map)])\n",
    "score_np_picked = np.array([np.take(row1, np.sort(row2)) for row1, row2 in zip(scores_np, idx_map)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_np_picked = np.concatenate([abstract_np, title_np_picked,score_np_picked], axis=1)\n",
    "pairs_np_picked_shuffled = np.random.permutation(pairs_np_picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracs = pairs_np_picked_shuffled[:,:1]\n",
    "titles = pairs_np_picked_shuffled[:,1:7]\n",
    "scores = pairs_np_picked_shuffled[:,7:].astype(float)\n",
    "scores = np.around(scores, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for ab, row1, row2 in zip(abstracs, titles, scores):\n",
    "  assert len(row1) == len(row2)\n",
    "  assert len(row1) == 6\n",
    "  for t, s in zip(row1, row2):\n",
    "    \n",
    "    if np.isnan(s):\n",
    "      print('found nan score')\n",
    "    if t=='':\n",
    "      print('found empty title')\n",
    "    lst.append([ab[0] + '[SEP]' + t, s])\n",
    "df = pd.DataFrame(np.array(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['excerpt', 'target']\n",
    "#dataframe = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = df[:600].reset_index(drop=True)\n",
    "dfdev = df[600:660].reset_index(drop=True)\n",
    "dftest = df[660:].reset_index(drop=True)\n",
    "\n",
    "Path(f'{OUTPUT_DIR}/reward_model_robust_test/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dftrain.to_csv(f'{OUTPUT_DIR}/reward_model_robust_test/sciBert_shuffled_train.csv')\n",
    "dfdev.to_csv(f'{OUTPUT_DIR}/reward_model_robust_test/sciBert_shuffled_dev.csv')\n",
    "dftest.to_csv(f'{OUTPUT_DIR}/reward_model_robust_test/sciBert_shuffled_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Excerpt_Dataset(data=dftrain, maxlen=p['max_len'], tokenizer=tokenizer)\n",
    "dev_set = Excerpt_Dataset(data=dfdev, maxlen=p['max_len'], tokenizer=tokenizer)\n",
    "test_set = Excerpt_Dataset(data=dftest, maxlen=p['max_len'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)\n",
    "dev_loader = DataLoader(dataset=dev_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Train (do not use this for training of reward model, reward model trained using ray tune)\n",
    "\n",
    "if p['do_train']:\n",
    "  train(model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=dev_loader,\n",
    "    epochs = p['epochs'],\n",
    "    device = device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best read model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state, optimizer_state = torch.load(os.path.join(f'{PROJECT_ROOT}/reward model', \"checkpoint\"))\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device):\n",
    "    predicted_label = []\n",
    "    actual_label = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, target in (dataloader):\n",
    "            \n",
    "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "                        \n",
    "            predicted_label += output\n",
    "            actual_label += target\n",
    "            \n",
    "    return predicted_label, actual_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Display Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,GS_label = predict(model, train_loader, device)\n",
    "cpu_output = np.array([x.cpu().data.numpy() for x in output]).squeeze()\n",
    "cpu_target = np.array([x.cpu().data.numpy() for x in GS_label]).squeeze()\n",
    "stats.spearmanr(cpu_output, cpu_target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_output,dev_GS_label = predict(model, dev_loader, device)\n",
    "cpu_dev_output = np.array([x.cpu().data.numpy() for x in dev_output]).squeeze()\n",
    "cpu_dev_target = np.array([x.cpu().data.numpy() for x in dev_GS_label]).squeeze()\n",
    "stats.spearmanr(cpu_dev_output, cpu_dev_target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output,test_GS_label = predict(model, test_loader, device)\n",
    "cpu_test_output = np.array([x.cpu().data.numpy() for x in test_output]).squeeze()\n",
    "cpu_test_target = np.array([x.cpu().data.numpy() for x in test_GS_label]).squeeze()\n",
    "stats.spearmanr(cpu_test_output, cpu_test_target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_output,dev_GS_label = predict(model, dev_loader, device)\n",
    "cpu_dev_output = np.array([x.cpu().data.numpy() for x in dev_output]).squeeze()\n",
    "cpu_dev_target = np.array([x.cpu().data.numpy() for x in dev_GS_label]).squeeze()\n",
    "stats.spearmanr(cpu_dev_output, cpu_dev_target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output,test_GS_label = predict(model, test_loader, device)\n",
    "cpu_test_output = np.array([x.cpu().data.numpy() for x in test_output]).squeeze()\n",
    "cpu_test_target = np.array([x.cpu().data.numpy() for x in test_GS_label]).squeeze()\n",
    "stats.spearmanr(cpu_test_output, cpu_test_target)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('abstract-to-title')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "846c63257c8068d961f9bc7a1cc6d5c293f004d697fb3a71f59faad032bcda7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
