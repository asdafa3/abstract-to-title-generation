# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01-gpt2-with-value-head.ipynb (unless otherwise specified).

__all__ = ['CausalLMOutputWithCrossAttentions', 'ValueHead', 'BartHeadWithValueModel', 'respond_to_batch']

# Cell


from transformers import AutoModelForSeq2SeqLM, BartTokenizer, BartModel, BartPretrainedModel, BartConfig
from transformers import top_k_top_p_filtering, BeamSearchScorer
from torch import nn
from torch.nn import Identity
import torch.nn.functional as F
import torch
from transformers import BartForConditionalGeneration
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
from transformers.modeling_outputs import Seq2SeqLMOutput
from typing import List, Optional, Tuple, Union
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel
from transformers import top_k_top_p_filtering
from transformers.modeling_outputs import ModelOutput
from torch import nn
from torch.nn import Identity
import torch.nn.functional as F
import torch
from dataclasses import dataclass
from typing import Optional, Tuple
# Cell
# exports
@dataclass
class CausalLMOutputWithCrossAttentions(Seq2SeqLMOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None
    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None
    encoder_last_hidden_state: Optional[torch.FloatTensor] = None
    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None
    value: Optional[torch.FloatTensor] = None
# Cell

class ValueHead(nn.Module):
    """The ValueHead class implements a head for GPT2 that returns a scalar for each output token."""
    def __init__(self, config):
        super().__init__()
        self.detach_head = False
        self.pad_token_id = config.pad_token_id
        self.summary = Identity()
        num_classes = config.num_labels
        self.summary = nn.Linear(config.hidden_size, 1)


        self.activation = Identity()
        #self.activation = nn.Tanh()


        self.first_dropout = Identity()
        self.first_dropout = nn.Dropout(config.dropout)

        self.last_dropout = Identity()
        self.last_dropout = nn.Dropout(config.dropout)

        self.flatten = nn.Flatten()


    def forward(self, hidden_states, cls_index=None):
        if self.detach_head:
            output = hidden_states.detach()
        else:
            output = hidden_states
        output = self.first_dropout(output)
        output = self.summary(output)
        output = self.activation(output)
        output = self.last_dropout(output)

        return output

# Cell

class BartHeadWithValueModel(BartPretrainedModel):
    """The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head."""
    def __init__(self, config):
        super().__init__(config)
        
        self.model  = BartForConditionalGeneration(config)
        #self.lm_head = nn.Linear(config.max_position_embeddings, config.vocab_size, bias=False)
        self.v_head = ValueHead(config)
        output_hidden_states = True

    def get_encoder(self): 
        return self.model.get_encoder()

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.LongTensor] = None,
        decoder_attention_mask: Optional[torch.LongTensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        decoder_head_mask: Optional[torch.Tensor] = None,
        cross_attn_head_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        ):
        loss=None
        outputs = self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            head_mask=head_mask,
            decoder_head_mask=decoder_head_mask,
            cross_attn_head_mask=cross_attn_head_mask,
            encoder_outputs=encoder_outputs,
            inputs_embeds=inputs_embeds,
            decoder_inputs_embeds=decoder_inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=True,
            return_dict=return_dict,
        )

        logits = outputs[0]
        last_decoder_hidden_states = outputs[2][-1] 
        value = self.v_head(last_decoder_hidden_states)#.sequeeze(-1)


        return CausalLMOutputWithCrossAttentions(
            loss = loss,
            logits = logits,
            #decoder_hidden_states = outputs.decoder_hidden_states,
            decoder_attentions = outputs.decoder_attentions,
            cross_attentions = outputs.cross_attentions,
            #encoder_last_hidden_state = outputs.encoder_last_hidden_state,
            encoder_hidden_states = outputs.encoder_hidden_states,
            encoder_attentions = outputs.encoder_attentions,
            value = value,
        )
        #res = {'logits': logits, "past_key_values": outputs.past_key_values, "decoder_hidden_states": outputs.decoder_hidden_states, 
        #"encoder_last_hidden_state": outputs.encoder_last_hidden_state, "encoder_hidden_states": outputs.encoder_hidden_states, "value":value}
        #return res['logits'], res['encoder_hidden_states'], res['value'] 

# Cell

def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):
    """Sample text from language model."""
    input_ids = queries
    for i in range(txt_len):
        # Get Logits
        outputs = model(input_ids)
        next_token_logits = outputs[0][:, -1, :]
        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)
        # Sample
        probs = F.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)
        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)
    return input_ids[:, -txt_len:]