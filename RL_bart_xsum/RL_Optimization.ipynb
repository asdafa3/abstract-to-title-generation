{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r \"{PROJECT_ROOT}/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f'{PROJECT_ROOT}')\n",
    "sys.path.append(f'{PROJECT_ROOT}/deps/BARTScore')\n",
    "sys.path.append(f'{PROJECT_ROOT}/deps/bert_score')\n",
    "sys.path.append(f'{PROJECT_ROOT}/RL_bart_xsum/trl/trl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, BartTokenizer, BartModel, BartForConditionalGeneration, BartConfig, GPT2Config,GPT2LMHeadModel\n",
    "from transformers import top_k_top_p_filtering, GPT2Model\n",
    "from transformers import GPT2Tokenizer, AutoModel, BartTokenizer, AutoTokenizer\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from bart_xsum import ValueHead\n",
    "import ppo\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from threading import active_count\n",
    "import sys\n",
    "from bart_score import BARTScorer\n",
    "from bert_score import score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull data only pulls changed data\n",
    "!dvc pull -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.backends.cudnn.deteministic = True\n",
    "setup_seed(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegresser(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        #The output layer that takes the [CLS] representation and gives an output\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(768, 1))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #Feed the input to Bert model to obtain contextualized representations\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #Obtain the representations of [CLS] heads\n",
    "        logits = outputs[1]\n",
    "        output = self.regressor(logits)\n",
    "        return output\n",
    "\n",
    "\n",
    "config = BartConfig('facebook/bart-large-xsum', output_hidden_states=True)\n",
    "model_name = f\"{MODEL_DIR}/BART-XSum/\"\n",
    "\n",
    "print(DEVICE_ID)\n",
    "\n",
    "#load preptrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_hidden_states=True).to(DEVICE_ID)\n",
    "#load reference model\n",
    "ref_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_hidden_states=True).to(DEVICE_ID)\n",
    "#load reward model\n",
    "reward_model = BertRegresser.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "model_state, optimizer_state = torch.load(f\"{PROJECT_ROOT}/reward_model/checkpoint\") # <---------------------- !!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "reward_model.load_state_dict(model_state)\n",
    "reward_model.to(DEVICE_ID)\n",
    "#load Valuehead\n",
    "hmodel = ValueHead(config).to(DEVICE_ID)\n",
    "\n",
    "#load reward tokenizer\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "#load model tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated datad\n",
    "anno_sample = pd.read_json(DATASET_140_ANNOTATED_JSON)\n",
    "def creat_tupel(sample_np):\n",
    "  ab = [sample_np[0]]*6\n",
    "  ht = [sample_np[1]]*6\n",
    "  indices = np.array([2, 5, 8, 11, 14, 17])\n",
    "  sysms = list(sample_np[2].keys())\n",
    "  sysms = ['original'] + sysms\n",
    "  gents = list(sample_np[2].values())\n",
    "  gents = [sample_np[1]] + gents\n",
    "  gents_scores = list(sample_np[3].values())\n",
    "  gents_scores = [gents_scores[i] for i in indices]\n",
    "  max_idx = gents_scores.index(max(gents_scores))\n",
    "  res = np.transpose(np.array([ab, ht, gents, gents_scores, sysms]))\n",
    "  return res[max_idx,:].reshape(1,5), res\n",
    "anno_sample_np =anno_sample.to_numpy()\n",
    "res = []\n",
    "res1 = []\n",
    "for row in anno_sample_np:\n",
    "  r1, r2 = creat_tupel(row)\n",
    "  res += r1.tolist()\n",
    "  res1 += r2.tolist()\n",
    "gen_title_score_pairs = np.array(res1)\n",
    "gen_title_score_pairs_bestone = np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **3 Datasettings (only for analysis in default RL, final model using cross learning with all sample with best title)**\"\"\"\n",
    "\n",
    "#@title pick bad annotated data or from bart-xsum\n",
    "gen_title_score_pairs_bad = []\n",
    "scores = gen_title_score_pairs_bestone[:,-2]\n",
    "scores = np.array([float(s) for s in scores])\n",
    "for row in gen_title_score_pairs_bestone:\n",
    "  if float(row[-2]) <= np.mean(scores):\n",
    "    gen_title_score_pairs_bad.append(row)\n",
    "\n",
    "gen_title_score_pairs_bad = np.array(gen_title_score_pairs_bad)\n",
    "\n",
    "#@title pick annotated data from bart-xsum\n",
    "gen_title_score_pairs_xsum = []\n",
    "for row in gen_title_score_pairs:\n",
    "  if row[-1] == 'bart_xsum':\n",
    "    gen_title_score_pairs_xsum.append(row)\n",
    "\n",
    "gen_title_score_pairs_xsum = np.array(gen_title_score_pairs_xsum)\n",
    "\n",
    "#@title pick good annotated data or from bart-xsum\n",
    "gen_title_score_pairs_good = []\n",
    "scores = gen_title_score_pairs_bestone[:,-2]\n",
    "scores = np.array([float(s) for s in scores])\n",
    "for row in gen_title_score_pairs_bestone:\n",
    "  if float(row[-2]) >= np.mean(scores) or row[-1] == 'bart_xsum':\n",
    "    gen_title_score_pairs_good.append(row)\n",
    "\n",
    "gen_title_score_pairs_good = np.array(gen_title_score_pairs_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **2 Train Mode (Cross learning/ Default RL)**\"\"\"\n",
    "gen_kwargs = {\n",
    "    \"min_length\":-1,\n",
    "    \"top_k\": 2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    #\"length_penalty\" : -20.0,\n",
    "    #\"num_return_sequences\" : 5,\n",
    "    #\"repetition_penalty\" : 1.5,\n",
    "}\n",
    "def ACT_step(title_score_pairs, start_ids, stop_ids, tokenizer, model, ppo_trainer):\n",
    "  res = []\n",
    "  for i in range(start_ids, stop_ids):\n",
    "    row = title_score_pairs[i]\n",
    "    ref_response_txt = row[2]\n",
    "    #model_name = row[4]\n",
    "    original_title = row[1]\n",
    "    query_txt = row[0]\n",
    "    query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')\n",
    "    #act step\n",
    "    ref_response_tensor = tokenizer(ref_response_txt, return_tensors='pt').to('cuda')\n",
    "     # define a reward for response\n",
    "    t = '[CLS]' + query_txt + '[SEP]' + ref_response_txt\n",
    "    ref_reward_encode = reward_tokenizer(t, return_tensors='pt').to('cuda')\n",
    "    #reward_input = reward_encode.input_ids\n",
    "    #reward_att = reward_encode.attention_mask\n",
    "    ref_reward = reward_model(ref_reward_encode['input_ids'], ref_reward_encode['attention_mask']).squeeze(-1)\n",
    "    ref_reward = torch.tensor([ref_reward.item()]).to('cuda')\n",
    "    #reward = torch.tensor(float(row[3])).to('cuda')\n",
    "    # train model with ppo\n",
    "    train_stats = ppo_trainer.step(query_tensor[\"input_ids\"], ref_response_tensor[\"input_ids\"], ref_reward)\n",
    "\n",
    "    # get model response\n",
    "    '''response_tensor = model.generate(\n",
    "                input_ids = query_tensor[\"input_ids\"],\n",
    "                attention_mask = query_tensor[\"attention_mask\"],\n",
    "                max_length = 30,\n",
    "                num_beams = 5,\n",
    "                num_return_sequences = 1,\n",
    "                repetition_penalty=2.0, \n",
    "                length_penalty=10.0,\n",
    "                early_stopping = True,\n",
    "                ).to('cuda')'''\n",
    "    response_tensor = model.generate(input_ids = query_tensor[\"input_ids\"],\n",
    "                attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n",
    "    response_txt = tokenizer.batch_decode(response_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    #test ref model response(generated title from fine-tunde bart-xsum)\n",
    "\n",
    "    # define a reward for response\n",
    "    t = '[CLS]' + query_txt + '[SEP]' + response_txt\n",
    "    reward_encode = reward_tokenizer(t, return_tensors='pt').to('cuda')\n",
    "    #reward_input = reward_encode.input_ids\n",
    "    #reward_att = reward_encode.attention_mask\n",
    "    reward = reward_model(reward_encode['input_ids'], reward_encode['attention_mask']).squeeze(-1)\n",
    "    reward = torch.tensor([reward.item()]).to('cuda')\n",
    "\n",
    "    # train model with ppo\n",
    "    train_stats = ppo_trainer.step(query_tensor[\"input_ids\"], response_tensor, reward)\n",
    "\n",
    "    #generate title with rewarded model\n",
    "    '''new_model_response_ids = model.generate(\n",
    "                input_ids = query_tensor[\"input_ids\"],\n",
    "                attention_mask = query_tensor[\"attention_mask\"],\n",
    "                max_length = 30,\n",
    "                num_beams = 5,\n",
    "                num_return_sequences = 1,\n",
    "                repetition_penalty=2.0, \n",
    "                length_penalty=10.0,\n",
    "                early_stopping = True,\n",
    "                )'''\n",
    "    new_model_response_ids = model.generate(input_ids = query_tensor[\"input_ids\"],\n",
    "                attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n",
    "    new_model_response_ids = new_model_response_ids.cpu().data.numpy()\n",
    "    new_model_response_txt = tokenizer.batch_decode(new_model_response_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    res.append([response_txt, new_model_response_txt])\n",
    "    if i % 10 == 0:\n",
    "      print('Original title: ', original_title)\n",
    "      print('Generated reference title: ', ref_response_txt)\n",
    "      print('RL-rewarded-after-step bart_xsum generated title: ', new_model_response_txt)\n",
    "      print('- - - -'*20 + '>')\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"min_length\":-1,\n",
    "    \"top_k\": 2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    #\"length_penalty\" : 4.0,\n",
    "    #\"repetition_penalty\" : 1.5,\n",
    "}\n",
    "def RL_steps(title_score_pairs, start_ids, stop_ids, tokenizer, model, ppo_trainer):\n",
    "  res = []\n",
    "  for i in range(start_ids, stop_ids):\n",
    "    row = title_score_pairs[i]\n",
    "    #model_name = row[4]\n",
    "    original_title = row[1]\n",
    "    query_txt = row[0]\n",
    "    query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')\n",
    "    # get model response\n",
    "    '''response_tensor = model.generate(\n",
    "                input_ids = query_tensor[\"input_ids\"],\n",
    "                attention_mask = query_tensor[\"attention_mask\"],\n",
    "                max_length = 30,\n",
    "                num_beams = 5,\n",
    "                num_return_sequences = 1,\n",
    "                repetition_penalty=2.0,\n",
    "                length_penalty=10.0,\n",
    "                early_stopping = True,\n",
    "                ).to('cuda')'''\n",
    "\n",
    "    response_tensor = model.generate(input_ids = query_tensor[\"input_ids\"],\n",
    "                attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n",
    "    response_txt = tokenizer.batch_decode(response_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    #test ref model response(generated title from fine-tunde bart-xsum)\n",
    "    #ref_response_txt = row[2]\n",
    "\n",
    "    # define a reward for response\n",
    "    t = '[CLS]' + query_txt + '[SEP]' + response_txt\n",
    "    reward_encode = reward_tokenizer(t, return_tensors='pt').to('cuda')\n",
    "    #reward_input = reward_encode.input_ids\n",
    "    #reward_att = reward_encode.attention_mask\n",
    "    reward = reward_model(reward_encode['input_ids'], reward_encode['attention_mask']).squeeze(-1)\n",
    "    #reward = row[2].item()\n",
    "    reward = torch.tensor([reward.item()]).to('cuda')\n",
    "\n",
    "    # train model with ppo\n",
    "    train_stats = ppo_trainer.step(query_tensor[\"input_ids\"], response_tensor, reward)\n",
    "\n",
    "    #generate title with rewarded model\n",
    "    new_model_response_ids = model.generate(\n",
    "                input_ids = query_tensor[\"input_ids\"],\n",
    "                attention_mask = query_tensor[\"attention_mask\"],\n",
    "                max_length = 30,\n",
    "                num_beams = 5,\n",
    "                num_return_sequences = 1,\n",
    "                repetition_penalty=2.0,\n",
    "                length_penalty=10.0,\n",
    "                early_stopping = True,\n",
    "                )\n",
    "    new_model_response_ids = new_model_response_ids.cpu().data.numpy()\n",
    "    new_model_response_txt = tokenizer.batch_decode(new_model_response_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    res.append([response_txt, new_model_response_txt])\n",
    "    if i % 6 == 0:\n",
    "      print('Original title: ', original_title)\n",
    "      #print('Generated reference title: ', ref_response_txt)\n",
    "      print('RL-rewarded-after-step bart_xsum generated title: ', new_model_response_txt)\n",
    "      print('- - - -'*20 + '>')\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "df1 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_lowLR.csv', index_col=0)\n",
    "df2 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_midLrEp.csv', index_col=0)\n",
    "df3 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_verylowLR.csv', index_col=0)\n",
    "df4 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_midLrEp_good_scores.csv', index_col=0)\n",
    "ab = df1[['abstract']].to_numpy().squeeze()\n",
    "ot = df1[['original title']].to_numpy().squeeze()\n",
    "gt = df1[['generated title before RL']].to_numpy().squeeze()\n",
    "t1 = df1[['generated title after RL']].to_numpy().squeeze()\n",
    "t2 = df2[['title-xsum-reward']].to_numpy().squeeze()\n",
    "t3 = df3[['generated title after RL']].to_numpy().squeeze()\n",
    "t4 = df1[['generated title after RL']].to_numpy().squeeze()\n",
    "ttt = np.array([ab,ot,gt,t1,t2,t3,t4]).transpose()\n",
    "df = pd.DataFrame(ttt)\n",
    "df.columns =['abstracz','human title', 'before RL', 'low LR', 'mid LR', 'very low LR', 'mid LR goog only']\n",
    "df.to_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_dif_setup.csv')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **Train Settings and Do Train**\"\"\"\n",
    "\n",
    "#@title initialize PPOtrainer\n",
    "# initialize trainer\n",
    "#ppo_config = {'batch_size': 1, 'forward_batch_size': 1}\n",
    "\n",
    "ppo_config = {\n",
    "        \"lr\": 2e-6,#1.41e-5,#6e-7,#\"lr\": 3e-6,#,\n",
    "        \"adap_kl_ctrl\": True,\n",
    "        \"init_kl_coef\":0.2,\n",
    "        \"target\": 6,\n",
    "        \"horizon\":10000,\n",
    "        \"gamma\":1,\n",
    "        \"lam\":0.95,\n",
    "        \"cliprange\": .2,\n",
    "        \"cliprange_value\":.2,\n",
    "        \"vf_coef\":.1,\n",
    "        \"batch_size\": 1,\n",
    "        \"forward_batch_size\": 1,\n",
    "        \"ppo_epochs\": 3,\n",
    "    }\n",
    "ppo_trainer = PPOTrainer(model, ref_model, hmodel, **ppo_config)\n",
    "res = ACT_step(gen_title_score_pairs_bestone, 0, len(gen_title_score_pairs_bestone), tokenizer, model, ppo_trainer)\n",
    "#res = RL_steps(gen_title_score_pairs_bad, 0, len(gen_title_score_pairs_bad), tokenizer, model, ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt = \"This bachelor thesis explores the generation of title based on a given abstract using neural language model. Recently, neural language models have been used in many scenarios with practical applications. For example, in scientific writing, automatic summary generation from long texts is used to assist in the reading and selection of relevant scientific articles. Title is an important part of scientific article, but the title generation using neural language and optimization for neural language model based on human preferences are less studied. This thesis addresses this gap and presents an optimized model based on state-of-the-art pre-trained neural language model which generate human-preferred titles from a given abstract. The model is fine-tuned on datasets of scientific article and optimized from human preferences using the novel learning perspective in reinforcement learning environment. The result shows that, the neural language model have powerful capabilities on the abstract-to-title task and the reinforcement learning approach is effective in scalable learning of neural language model.\"\n",
    "\n",
    "query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tensor=model.generate(input_ids = query_tensor[\"input_ids\"], \n",
    "                          attention_mask = query_tensor[\"attention_mask\"], \n",
    "                          max_new_tokens=30, \n",
    "                          **gen_kwargs)[-30:].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tensor = model.generate(\n",
    "              input_ids = query_tensor[\"input_ids\"],\n",
    "              attention_mask = query_tensor[\"attention_mask\"],\n",
    "              max_length = 30,\n",
    "              num_beams = 5,\n",
    "              num_return_sequences = 1,\n",
    "              repetition_penalty=2.0,\n",
    "              length_penalty=10.0,\n",
    "              early_stopping = True,\n",
    "              ).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_txt = tokenizer.batch_decode(ref_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Generation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outres = []\n",
    "for row in gen_title_score_pairs_bestone:\n",
    "  ab = row[0]\n",
    "  ht = row[1]\n",
    "  ogt = row[2]\n",
    "  query_txt = ab\n",
    "  query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')\n",
    "  #get ref model response\n",
    "  '''ref_tensor = ref_model.generate(\n",
    "              input_ids = query_tensor[\"input_ids\"],\n",
    "              attention_mask = query_tensor[\"attention_mask\"],\n",
    "              max_length = 30,\n",
    "              num_beams = 5,\n",
    "              num_return_sequences = 1,\n",
    "              repetition_penalty=2.0, \n",
    "              length_penalty=10.0,\n",
    "              early_stopping = True,\n",
    "              ).to('cuda')'''\n",
    "  ref_tensor=ref_model.generate(input_ids = query_tensor[\"input_ids\"], \n",
    "                          attention_mask = query_tensor[\"attention_mask\"], \n",
    "                          max_new_tokens=30, \n",
    "                          **gen_kwargs)[-30:].to('cuda')\n",
    "  ref_txt = tokenizer.batch_decode(ref_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "  # get model response\n",
    "  '''response_tensor = model.generate(\n",
    "              input_ids = query_tensor[\"input_ids\"],\n",
    "              attention_mask = query_tensor[\"attention_mask\"],\n",
    "              max_length = 30,\n",
    "              num_beams = 5,\n",
    "              num_return_sequences = 1,\n",
    "              repetition_penalty=2.0, \n",
    "              length_penalty=10.0,\n",
    "              early_stopping = True,\n",
    "              ).to('cuda')'''\n",
    "  response_tensor = model.generate(input_ids = query_tensor[\"input_ids\"], attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n",
    "\n",
    "  response_txt = tokenizer.batch_decode(response_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "  outres.append([ab, ht,ogt, ref_txt, response_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(outres)\n",
    "df.columns = ['abstract', 'original title','best title' , 'generated title before RL', 'generated title after RL']\n",
    "path=f'{OUTPUT_DIR}\\act_'+ \"{:.9f}\".format(ppo_config['lr'])+'_'+str(ppo_config['ppo_epochs'])+'.csv'\n",
    "df = df.drop(['original title'], axis=1)\n",
    "df.columns = ['abstract', 'best title', 'title-xsum', 'title-xsum-reward']\n",
    "df_np = df.to_numpy()\n",
    "\n",
    "n_np = []\n",
    "for row in df_np:\n",
    "  ab = row[0]\n",
    "  ts = row[1:]\n",
    "  #ts[0] = ts[0]\n",
    "  #ts[0] = ts[0]\n",
    "  #ts = np.random.permutation(ts)\n",
    "  n_np.append([ab, ts[1], ts[2]])\n",
    "n_np = np.array(n_np)\n",
    "se = pd.DataFrame(n_np[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/Thesis/BARTScore\n",
    "\n",
    "bart_scorer = BARTScorer(device='cuda:0',checkpoint='D:\\\\Thesis\\\\BARTScore-main\\\\bart_xsum')\n",
    "\n",
    "# %cd /content/drive/MyDrive/Thesis/emnlp19-moverscore-master\n",
    "'''sys.path.append('D:\\\\Thesis\\\\emnlp19-moverscore-master')\n",
    "from moverscore_v2 import get_idf_dict, word_mover_score\n",
    "from typing import List, Union, Iterable\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "def sentence_score(hypothesis: str, references: List[str], trace=0):\n",
    "    \n",
    "    idf_dict_hyp = defaultdict(lambda: 1.)\n",
    "    idf_dict_ref = defaultdict(lambda: 1.)\n",
    "    \n",
    "    hypothesis = [hypothesis] * len(references)\n",
    "    \n",
    "    sentence_score = 0 \n",
    "\n",
    "    scores = word_mover_score(references, hypothesis, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=False)\n",
    "    \n",
    "    sentence_score = np.mean(scores)\n",
    "    \n",
    "    if trace > 0:\n",
    "        print(hypothesis, references, sentence_score)\n",
    "            \n",
    "    return sentence_score\n",
    "'''\n",
    "# Commented out IPython magic to ensure Python compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Thesis/bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{OUTPUT_DIR}\\output_111_annotated_from_bart_xsum_act_bad_final0.000002000_3.csv', index_col=0)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs = df['abstract'].to_list()\n",
    "ots = df['original title'].to_list()\n",
    "bts = df['best title'].to_list()\n",
    "ogts = df['generated title before RL'].to_list()\n",
    "rlts = df['generated title after RL'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores11 = bart_scorer.score(abs, ots, batch_size=1)\n",
    "scores21 = bart_scorer.score(abs, ogts, batch_size=1)\n",
    "scores31 = bart_scorer.score(abs, rlts, batch_size=1)\n",
    "'''\n",
    "scores12 = [sentence_score(ot, [ab]) for ot,ab in zip(ots, abs)]\n",
    "scores22 = [sentence_score(ogt, [ab]) for ogt,ab in zip(ogts, abs)]\n",
    "scores32 = [sentence_score(rlt, [ab]) for rlt,ab in zip(rlts, abs)]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,r,score13 = score(ots, abs, lang=\"en\")\n",
    "p,r,score23 = score(ogts, abs, lang=\"en\")\n",
    "p,r,score33 = score(rlts, abs , lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BartScore\n",
    "print('abs_ots: ', sum(scores11)/len(scores11))\n",
    "print('abs_ogts: ', sum(scores21)/len(scores21))\n",
    "print('abs_rlts: ', sum(scores31)/len(scores31))\n",
    "#BertScore\n",
    "print('abs_ots: ', np.array(score13.tolist()).mean())\n",
    "print('abs_ogts: ', np.array(score23.tolist()).mean())\n",
    "print('abs_rlts: ', np.array(score33.tolist()).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MoverScore\n",
    "\n",
    "print('abs_ots: ', sum(scores12)/len(scores12))\n",
    "print('abs_ogts: ', sum(scores22)/len(scores22))\n",
    "int('abs_rlts: ', sum(scores32)/len(scores32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = [np.exp(-0.654991332689921), 0.8518536269664765, 0.5175741747308048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = [np.exp(-0.6468217780192693), 0.8508123060067495, 0.5174440166876286]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r3 = [np.exp(-0.6468217780192693), 0.5174440166876286, 0.8508123060067495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_metrics = normalize(np.array([r1, r2]), axis=0, norm='l1')\n",
    "normalized_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_metrics = normalize(np.array([r1, r2]), axis=0, norm='l1')\n",
    "normalized_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('abstract-to-title')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "846c63257c8068d961f9bc7a1cc6d5c293f004d697fb3a71f59faad032bcda7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
