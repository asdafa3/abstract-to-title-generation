{"cells":[{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading builder script: 5.60kB [00:00, 1.76MB/s]                   \n","100%|██████████| 12/12 [00:04<00:00,  2.57ba/s]\n","100%|██████████| 1/1 [00:00<00:00,  3.25ba/s]\n","[nltk_data] Downloading package punkt to /home/linusb/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Using amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, title, abstract. If __index_level_0__, title, abstract are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/home/linusb/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 11864\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 3\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 8\n","  Total optimization steps = 1482\n","  0%|          | 3/1482 [00:11<1:33:08,  3.78s/it]"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.94 GiB total capacity; 5.10 GiB already allocated; 15.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/RL_Optimization.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/RL_Optimization.ipynb#ch0000037?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/RL_Optimization.ipynb#ch0000037?line=9'>10</a>\u001b[0m \u001b[39m#from gpt2 import *\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/RL_Optimization.ipynb#ch0000037?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbart_xsum\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/RL_Optimization.ipynb#ch0000037?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mppo\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/RL_Optimization.ipynb#ch0000037?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n","File \u001b[0;32m~/git/abstract-to-title-generation/RL_scbert_bart_xsum/bart_xsum.py:114\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='file:///home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/bart_xsum.py?line=109'>110</a>\u001b[0m old_collator \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mdata_collator\n\u001b[1;32m    <a href='file:///home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/bart_xsum.py?line=111'>112</a>\u001b[0m trainer\u001b[39m.\u001b[39mdata_collator \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m data: \u001b[39mdict\u001b[39m(old_collator(data))\n\u001b[0;32m--> <a href='file:///home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/bart_xsum.py?line=113'>114</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    <a href='file:///home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/bart_xsum.py?line=115'>116</a>\u001b[0m \u001b[39m#model.save_pretrained(\"./output/bart_large_xsum\")\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/linusb/git/abstract-to-title-generation/RL_scbert_bart_xsum/bart_xsum.py?line=117'>118</a>\u001b[0m \u001b[39m\"\"\"# **Generation**\"\"\"\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1311'>1312</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1313'>1314</a>\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1314'>1315</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1315'>1316</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1316'>1317</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1317'>1318</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1318'>1319</a>\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1319'>1320</a>\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1320'>1321</a>\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1321'>1322</a>\u001b[0m )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1614\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1611'>1612</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1612'>1613</a>\u001b[0m     scale_before \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n\u001b[0;32m-> <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1613'>1614</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer)\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1614'>1615</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m   <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/trainer.py?line=1615'>1616</a>\u001b[0m     scale_after \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=333'>334</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=335'>336</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=337'>338</a>\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=339'>340</a>\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=341'>342</a>\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:285\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=282'>283</a>\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=283'>284</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=284'>285</a>\u001b[0m     retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mstep(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py?line=285'>286</a>\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py?line=62'>63</a>\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py?line=63'>64</a>\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py?line=64'>65</a>\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///home/linusb/.local/lib/python3.10/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/optimization.py:361\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/optimization.py?line=358'>359</a>\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/optimization.py?line=359'>360</a>\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m--> <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/optimization.py?line=360'>361</a>\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39;49msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/optimization.py?line=362'>363</a>\u001b[0m step_size \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='file:///home/linusb/.local/lib/python3.10/site-packages/transformers/optimization.py?line=363'>364</a>\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mcorrect_bias\u001b[39m\u001b[39m\"\u001b[39m]:  \u001b[39m# No bias correction for Bert\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.94 GiB total capacity; 5.10 GiB already allocated; 15.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from transformers import AutoModelForSeq2SeqLM, BartTokenizer, BartModel, BartForConditionalGeneration, BartConfig, GPT2Config,GPT2LMHeadModel\n","from transformers import top_k_top_p_filtering, GPT2Model\n","from transformers import GPT2Tokenizer, AutoModel, BartTokenizer, AutoTokenizer\n","from transformers import BertModel, BertPreTrainedModel\n","import torch\n","from torch import nn\n","from torch.nn import Identity\n","import torch.nn.functional as F\n","import sys\n","#from gpt2 import *\n","from bart_xsum import *\n","from ppo import *\n","import numpy as np\n","import os\n","import pandas as pd\n","from threading import active_count\n","import sys\n","#sys.path.append('D:\\\\Thesis\\\\BARTScore-main')\n","#sys.path.append('D:\\\\Thesis\\\\bert_score-master')\n","from bart_score import BARTScorer\n","from bert_score import score\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import normalize\n","from config import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# install requirements\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  0% Transferring|                                   |0/1 [00:00<?,     ?file/s]\n","!\u001b[A\n","  0%|          |3f3b128845e658a9522da88cffc0f2.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","  0%|          |3f3b128845e658a9522da88cffc0f2.di0.00/254 [00:00<?,        ?B/s]\u001b[A\n","100%|██████████|3f3b128845e658a9522da88cffc0f2254/254 [00:01<00:00,      208B/s]\u001b[A\n","  0% Transferring|                                   |0/1 [00:00<?,     ?file/s]\u001b[A\n","!\u001b[A\n","  0%|          |c2650ad50b003a1975eb10374e11dc     0.00/? [00:00<?,        ?B/s]\u001b[A\n","  0%|          |c2650ad50b003a1975eb10374e11dc 0.00/10.3M [00:00<?,        ?B/s]\u001b[A\n","100%|██████████|c2650ad50b003a1975eb10374e10.3M/10.3M [00:15<00:00,     713kB/s]\u001b[A\n","  0% Checkout|                                      |0/22 [00:00<?,     ?file/s]\u001b[A\n","!\u001b[A\n","  0%|          |.25z8NeC3az9SDitye3B3BM.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n","  0%|          |.25z8NeC3az9SDitye3B3BM.tmp     0.00/4.00 [00:00<?,        ?B/s]\u001b[A\n","                                                                                \u001b[A\n","!\u001b[A\n","  0%|          |c2650ad50b003a1975eb10374e11dc     0.00/? [00:00<?,        ?B/s]\u001b[A\n","  0%|          |c2650ad50b003a1975eb10374e11dc 0.00/10.3M [00:00<?,        ?B/s]\u001b[A\n","\u001b[33mM\u001b[0m       data/filtered/                                                 \u001b[A\n","1 file modified and 1 file fetched\n","\u001b[0m"]}],"source":["# pull data only pulls changed data\n","!dvc pull"]},{"cell_type":"markdown","metadata":{},"source":["## Code Section"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def setup_seed(seed):\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  np.random.seed(seed)\n","  torch.backends.cudnn.deteministic = True\n","setup_seed(57)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class BertRegresser(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.bert = BertModel(config)\n","        #The output layer that takes the [CLS] representation and gives an output\n","        self.regressor = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(768, 1))\n","\n","    def forward(self, input_ids, attention_mask):\n","        #Feed the input to Bert model to obtain contextualized representations\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        #Obtain the representations of [CLS] heads\n","        logits = outputs[1]\n","        output = self.regressor(logits)\n","        return output\n","\n","\n","config = BartConfig('facebook/bart-large-xsum', output_hidden_states=True)\n","model_name = f\"{MODEL_DIR}/bart_large_xsum\"\n","\n","#load preptrained model\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_hidden_states=True).to('cuda')\n","#load reference model\n","ref_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_hidden_states=True).to('cuda')\n","#load reward model\n","reward_model = BertRegresser.from_pretrained('allenai/scibert_scivocab_uncased')\n","\n","model_state, optimizer_state = torch.load(f\"{MODEL_DIR}/reward_model\")\n","reward_model.load_state_dict(model_state)\n","reward_model.to('cuda')\n","#load Valuehead\n","hmodel = ValueHead(config).to('cuda')\n","\n","#load reward tokenizer\n","reward_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n","#load model tokenizer\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load annotated datad\n","anno_sample = pd.read_json(DATASET_140_ANNOTATED_JSON)\n","def creat_tupel(sample_np):\n","  ab = [sample_np[0]]*6\n","  ht = [sample_np[1]]*6\n","  indices = np.array([2, 5, 8, 11, 14, 17])\n","  sysms = list(sample_np[2].keys())\n","  sysms = ['original'] + sysms\n","  gents = list(sample_np[2].values())\n","  gents = [sample_np[1]] + gents\n","  gents_scores = list(sample_np[3].values())\n","  gents_scores = [gents_scores[i] for i in indices]\n","  max_idx = gents_scores.index(max(gents_scores))\n","  res = np.transpose(np.array([ab, ht, gents, gents_scores, sysms]))\n","  return res[max_idx,:].reshape(1,5), res\n","anno_sample_np =anno_sample.to_numpy()\n","res = []\n","res1 = []\n","for row in anno_sample_np:\n","  r1, r2 = creat_tupel(row)\n","  res += r1.tolist()\n","  res1 += r2.tolist()\n","gen_title_score_pairs = np.array(res1)\n","gen_title_score_pairs_bestone = np.array(res)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"# **3 Datasettings (only for analysis in default RL, final model using cross learning with all sample witgh best title)**\"\"\"\n","\n","#@title pick bad annotated data or from bart-xsum\n","gen_title_score_pairs_bad = []\n","scores = gen_title_score_pairs_bestone[:,-2]\n","scores = np.array([float(s) for s in scores])\n","for row in gen_title_score_pairs_bestone:\n","  if float(row[-2]) <= np.mean(scores):\n","    gen_title_score_pairs_bad.append(row)\n","\n","gen_title_score_pairs_bad = np.array(gen_title_score_pairs_bad)\n","\n","#@title pick annotated data from bart-xsum\n","gen_title_score_pairs_xsum = []\n","for row in gen_title_score_pairs:\n","  if row[-1] == 'bart_xsum':\n","    gen_title_score_pairs_xsum.append(row)\n","\n","gen_title_score_pairs_xsum = np.array(gen_title_score_pairs_xsum)\n","\n","#@title pick good annotated data or from bart-xsum\n","gen_title_score_pairs_good = []\n","scores = gen_title_score_pairs_bestone[:,-2]\n","scores = np.array([float(s) for s in scores])\n","for row in gen_title_score_pairs_bestone:\n","  if float(row[-2]) >= np.mean(scores) or row[-1] == 'bart_xsum':\n","    gen_title_score_pairs_good.append(row)\n","\n","gen_title_score_pairs_good = np.array(gen_title_score_pairs_good)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"# **2 Train Mode (Cross learning/ Default RL)**\"\"\"\n","gen_kwargs = {\n","    \"min_length\":-1,\n","    \"top_k\": 2,\n","    \"top_p\": 0.8,\n","    \"do_sample\": True,\n","    \"pad_token_id\": tokenizer.eos_token_id,\n","    #\"length_penalty\" : -20.0,\n","    #\"num_return_sequences\" : 5,\n","    #\"repetition_penalty\" : 1.5,\n","}\n","def ACT_step(title_score_pairs, start_ids, stop_ids, tokenizer, model, ppo_trainer):\n","  res = []\n","  for i in range(start_ids, stop_ids):\n","    row = title_score_pairs[i]\n","    ref_response_txt = row[2]\n","    #model_name = row[4]\n","    original_title = row[1]\n","    query_txt = row[0]\n","    query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')\n","    #act step\n","    ref_response_tensor = tokenizer(ref_response_txt, return_tensors='pt').to('cuda')\n","     # define a reward for response\n","    t = '[CLS]' + query_txt + '[SEP]' + ref_response_txt\n","    ref_reward_encode = reward_tokenizer(t, return_tensors='pt').to('cuda')\n","    #reward_input = reward_encode.input_ids\n","    #reward_att = reward_encode.attention_mask\n","    ref_reward = reward_model(ref_reward_encode['input_ids'], ref_reward_encode['attention_mask']).squeeze(-1)\n","    ref_reward = torch.tensor([ref_reward.item()]).to('cuda')\n","    #reward = torch.tensor(float(row[3])).to('cuda')\n","    # train model with ppo\n","    train_stats = ppo_trainer.step(query_tensor[\"input_ids\"], ref_response_tensor[\"input_ids\"], ref_reward)\n","\n","    # get model response\n","    '''response_tensor = model.generate(\n","                input_ids = query_tensor[\"input_ids\"],\n","                attention_mask = query_tensor[\"attention_mask\"],\n","                max_length = 30,\n","                num_beams = 5,\n","                num_return_sequences = 1,\n","                repetition_penalty=2.0, \n","                length_penalty=10.0,\n","                early_stopping = True,\n","                ).to('cuda')'''\n","    response_tensor = model.generate(input_ids = query_tensor[\"input_ids\"],\n","                attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n","    response_txt = tokenizer.batch_decode(response_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","    #test ref model response(generated title from fine-tunde bart-xsum)\n","\n","    # define a reward for response\n","    t = '[CLS]' + query_txt + '[SEP]' + response_txt\n","    reward_encode = reward_tokenizer(t, return_tensors='pt').to('cuda')\n","    #reward_input = reward_encode.input_ids\n","    #reward_att = reward_encode.attention_mask\n","    reward = reward_model(reward_encode['input_ids'], reward_encode['attention_mask']).squeeze(-1)\n","    reward = torch.tensor([reward.item()]).to('cuda')\n","\n","    # train model with ppo\n","    train_stats = ppo_trainer.step(query_tensor[\"input_ids\"], response_tensor, reward)\n","\n","    #generate title with rewarded model\n","    '''new_model_response_ids = model.generate(\n","                input_ids = query_tensor[\"input_ids\"],\n","                attention_mask = query_tensor[\"attention_mask\"],\n","                max_length = 30,\n","                num_beams = 5,\n","                num_return_sequences = 1,\n","                repetition_penalty=2.0, \n","                length_penalty=10.0,\n","                early_stopping = True,\n","                )'''\n","    new_model_response_ids = model.generate(input_ids = query_tensor[\"input_ids\"],\n","                attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n","    new_model_response_ids = new_model_response_ids.cpu().data.numpy()\n","    new_model_response_txt = tokenizer.batch_decode(new_model_response_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","    res.append([response_txt, new_model_response_txt])\n","    if i % 10 == 0:\n","      print('Original title: ', original_title)\n","      print('Generated reference title: ', ref_response_txt)\n","      print('RL-rewarded-after-step bart_xsum generated title: ', new_model_response_txt)\n","      print('- - - -'*20 + '>')\n","  return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen_kwargs = {\n","    \"min_length\":-1,\n","    \"top_k\": 2,\n","    \"top_p\": 0.8,\n","    \"do_sample\": True,\n","    \"pad_token_id\": tokenizer.eos_token_id,\n","    #\"length_penalty\" : 4.0,\n","    #\"repetition_penalty\" : 1.5,\n","}\n","def RL_steps(title_score_pairs, start_ids, stop_ids, tokenizer, model, ppo_trainer):\n","  res = []\n","  for i in range(start_ids, stop_ids):\n","    row = title_score_pairs[i]\n","    #model_name = row[4]\n","    original_title = row[1]\n","    query_txt = row[0]\n","    query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')\n","    # get model response\n","    '''response_tensor = model.generate(\n","                input_ids = query_tensor[\"input_ids\"],\n","                attention_mask = query_tensor[\"attention_mask\"],\n","                max_length = 30,\n","                num_beams = 5,\n","                num_return_sequences = 1,\n","                repetition_penalty=2.0,\n","                length_penalty=10.0,\n","                early_stopping = True,\n","                ).to('cuda')'''\n","\n","    response_tensor = model.generate(input_ids = query_tensor[\"input_ids\"],\n","                attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n","    response_txt = tokenizer.batch_decode(response_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","    #test ref model response(generated title from fine-tunde bart-xsum)\n","    #ref_response_txt = row[2]\n","\n","    # define a reward for response\n","    t = '[CLS]' + query_txt + '[SEP]' + response_txt\n","    reward_encode = reward_tokenizer(t, return_tensors='pt').to('cuda')\n","    #reward_input = reward_encode.input_ids\n","    #reward_att = reward_encode.attention_mask\n","    reward = reward_model(reward_encode['input_ids'], reward_encode['attention_mask']).squeeze(-1)\n","    #reward = row[2].item()\n","    reward = torch.tensor([reward.item()]).to('cuda')\n","\n","    # train model with ppo\n","    train_stats = ppo_trainer.step(query_tensor[\"input_ids\"], response_tensor, reward)\n","\n","    #generate title with rewarded model\n","    new_model_response_ids = model.generate(\n","                input_ids = query_tensor[\"input_ids\"],\n","                attention_mask = query_tensor[\"attention_mask\"],\n","                max_length = 30,\n","                num_beams = 5,\n","                num_return_sequences = 1,\n","                repetition_penalty=2.0,\n","                length_penalty=10.0,\n","                early_stopping = True,\n","                )\n","    new_model_response_ids = new_model_response_ids.cpu().data.numpy()\n","    new_model_response_txt = tokenizer.batch_decode(new_model_response_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","    res.append([response_txt, new_model_response_txt])\n","    if i % 6 == 0:\n","      print('Original title: ', original_title)\n","      #print('Generated reference title: ', ref_response_txt)\n","      print('RL-rewarded-after-step bart_xsum generated title: ', new_model_response_txt)\n","      print('- - - -'*20 + '>')\n","  return res"]},{"cell_type":"markdown","metadata":{},"source":["# test setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","'''\n","df1 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_lowLR.csv', index_col=0)\n","df2 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_midLrEp.csv', index_col=0)\n","df3 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_verylowLR.csv', index_col=0)\n","df4 = pd.read_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_annotated_from_bart_xsum_midLrEp_good_scores.csv', index_col=0)\n","ab = df1[['abstract']].to_numpy().squeeze()\n","ot = df1[['original title']].to_numpy().squeeze()\n","gt = df1[['generated title before RL']].to_numpy().squeeze()\n","t1 = df1[['generated title after RL']].to_numpy().squeeze()\n","t2 = df2[['title-xsum-reward']].to_numpy().squeeze()\n","t3 = df3[['generated title after RL']].to_numpy().squeeze()\n","t4 = df1[['generated title after RL']].to_numpy().squeeze()\n","ttt = np.array([ab,ot,gt,t1,t2,t3,t4]).transpose()\n","df = pd.DataFrame(ttt)\n","df.columns =['abstracz','human title', 'before RL', 'low LR', 'mid LR', 'very low LR', 'mid LR goog only']\n","df.to_csv('/content/drive/MyDrive/Thesis/RL_scbert_bart_xsum/output/output_111_dif_setup.csv')\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"# **Train Settings and Do Train**\"\"\"\n","\n","#@title initialize PPOtrainer\n","# initialize trainer\n","#ppo_config = {'batch_size': 1, 'forward_batch_size': 1}\n","\n","ppo_config = {\n","        \"lr\": 2e-6,#1.41e-5,#6e-7,#\"lr\": 3e-6,#,\n","        \"adap_kl_ctrl\": True,\n","        \"init_kl_coef\":0.2,\n","        \"target\": 6,\n","        \"horizon\":10000,\n","        \"gamma\":1,\n","        \"lam\":0.95,\n","        \"cliprange\": .2,\n","        \"cliprange_value\":.2,\n","        \"vf_coef\":.1,\n","        \"batch_size\": 1,\n","        \"forward_batch_size\": 1,\n","        \"ppo_epochs\": 3,\n","    }\n","ppo_trainer = PPOTrainer(model, ref_model, hmodel, **ppo_config)\n","res = ACT_step(gen_title_score_pairs_bestone, 0, len(gen_title_score_pairs_bestone), tokenizer, model, ppo_trainer)\n","#res = RL_steps(gen_title_score_pairs_bad, 0, len(gen_title_score_pairs_bad), tokenizer, model, ppo_trainer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query_txt = \"This bachelor thesis explores the generation of title based on a given abstract using neural language model. Recently, neural language models have been used in many scenarios with practical applications. For example, in scientific writing, automatic summary generation from long texts is used to assist in the reading and selection of relevant scientific articles. Title is an important part of scientific article, but the title generation using neural language and optimization for neural language model based on human preferences are less studied. This thesis addresses this gap and presents an optimized model based on state-of-the-art pre-trained neural language model which generate human-preferred titles from a given abstract. The model is fine-tuned on datasets of scientific article and optimized from human preferences using the novel learning perspective in reinforcement learning environment. The result shows that, the neural language model have powerful capabilities on the abstract-to-title task and the reinforcement learning approach is effective in scalable learning of neural language model.\"\n","\n","query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ref_tensor=model.generate(input_ids = query_tensor[\"input_ids\"], \n","                          attention_mask = query_tensor[\"attention_mask\"], \n","                          max_new_tokens=30, \n","                          **gen_kwargs)[-30:].to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ref_tensor = model.generate(\n","              input_ids = query_tensor[\"input_ids\"],\n","              attention_mask = query_tensor[\"attention_mask\"],\n","              max_length = 30,\n","              num_beams = 5,\n","              num_return_sequences = 1,\n","              repetition_penalty=2.0,\n","              length_penalty=10.0,\n","              early_stopping = True,\n","              ).to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ref_txt = tokenizer.batch_decode(ref_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"]},{"cell_type":"markdown","metadata":{},"source":["\n","# **Generation**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outres = []\n","for row in gen_title_score_pairs_bestone:\n","  ab = row[0]\n","  ht = row[1]\n","  ogt = row[2]\n","  query_txt = ab\n","  query_tensor = tokenizer(query_txt, return_tensors=\"pt\").to('cuda')\n","  #get ref model response\n","  '''ref_tensor = ref_model.generate(\n","              input_ids = query_tensor[\"input_ids\"],\n","              attention_mask = query_tensor[\"attention_mask\"],\n","              max_length = 30,\n","              num_beams = 5,\n","              num_return_sequences = 1,\n","              repetition_penalty=2.0, \n","              length_penalty=10.0,\n","              early_stopping = True,\n","              ).to('cuda')'''\n","  ref_tensor=ref_model.generate(input_ids = query_tensor[\"input_ids\"], \n","                          attention_mask = query_tensor[\"attention_mask\"], \n","                          max_new_tokens=30, \n","                          **gen_kwargs)[-30:].to('cuda')\n","  ref_txt = tokenizer.batch_decode(ref_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","  # get model response\n","  '''response_tensor = model.generate(\n","              input_ids = query_tensor[\"input_ids\"],\n","              attention_mask = query_tensor[\"attention_mask\"],\n","              max_length = 30,\n","              num_beams = 5,\n","              num_return_sequences = 1,\n","              repetition_penalty=2.0, \n","              length_penalty=10.0,\n","              early_stopping = True,\n","              ).to('cuda')'''\n","  response_tensor = model.generate(input_ids = query_tensor[\"input_ids\"], attention_mask = query_tensor[\"attention_mask\"], max_new_tokens=30, **gen_kwargs)[-30:].to('cuda')\n","\n","  response_txt = tokenizer.batch_decode(response_tensor, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","  outres.append([ab, ht,ogt, ref_txt, response_txt])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(outres)\n","df.columns = ['abstract', 'original title','best title' , 'generated title before RL', 'generated title after RL']\n","path=f'{OUTPUT_DIR}\\act_'+ \"{:.9f}\".format(ppo_config['lr'])+'_'+str(ppo_config['ppo_epochs'])+'.csv'\n","df = df.drop(['original title'], axis=1)\n","df.columns = ['abstract', 'best title', 'title-xsum', 'title-xsum-reward']\n","df_np = df.to_numpy()\n","\n","n_np = []\n","for row in df_np:\n","  ab = row[0]\n","  ts = row[1:]\n","  #ts[0] = ts[0]\n","  #ts[0] = ts[0]\n","  #ts = np.random.permutation(ts)\n","  n_np.append([ab, ts[1], ts[2]])\n","n_np = np.array(n_np)\n","se = pd.DataFrame(n_np[:30])"]},{"cell_type":"markdown","metadata":{},"source":["# **Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %cd /content/drive/MyDrive/Thesis/BARTScore\n","\n","bart_scorer = BARTScorer(device='cuda:0',checkpoint='D:\\\\Thesis\\\\BARTScore-main\\\\bart_xsum')\n","\n","# %cd /content/drive/MyDrive/Thesis/emnlp19-moverscore-master\n","'''sys.path.append('D:\\\\Thesis\\\\emnlp19-moverscore-master')\n","from moverscore_v2 import get_idf_dict, word_mover_score\n","from typing import List, Union, Iterable\n","from collections import defaultdict\n","import numpy as np\n","def sentence_score(hypothesis: str, references: List[str], trace=0):\n","    \n","    idf_dict_hyp = defaultdict(lambda: 1.)\n","    idf_dict_ref = defaultdict(lambda: 1.)\n","    \n","    hypothesis = [hypothesis] * len(references)\n","    \n","    sentence_score = 0 \n","\n","    scores = word_mover_score(references, hypothesis, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=False)\n","    \n","    sentence_score = np.mean(scores)\n","    \n","    if trace > 0:\n","        print(hypothesis, references, sentence_score)\n","            \n","    return sentence_score\n","'''\n","# Commented out IPython magic to ensure Python compatibility."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /content/drive/MyDrive/Thesis/bert_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(f'{OUTPUT_DIR}\\output_111_annotated_from_bart_xsum_act_bad_final0.000002000_3.csv', index_col=0)[:30]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["abs = df['abstract'].to_list()\n","ots = df['original title'].to_list()\n","bts = df['best title'].to_list()\n","ogts = df['generated title before RL'].to_list()\n","rlts = df['generated title after RL'].to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scores11 = bart_scorer.score(abs, ots, batch_size=1)\n","scores21 = bart_scorer.score(abs, ogts, batch_size=1)\n","scores31 = bart_scorer.score(abs, rlts, batch_size=1)\n","'''\n","scores12 = [sentence_score(ot, [ab]) for ot,ab in zip(ots, abs)]\n","scores22 = [sentence_score(ogt, [ab]) for ogt,ab in zip(ogts, abs)]\n","scores32 = [sentence_score(rlt, [ab]) for rlt,ab in zip(rlts, abs)]'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["p,r,score13 = score(ots, abs, lang=\"en\")\n","p,r,score23 = score(ogts, abs, lang=\"en\")\n","p,r,score33 = score(rlts, abs , lang=\"en\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#BartScore\n","print('abs_ots: ', sum(scores11)/len(scores11))\n","print('abs_ogts: ', sum(scores21)/len(scores21))\n","print('abs_rlts: ', sum(scores31)/len(scores31))\n","#BertScore\n","print('abs_ots: ', np.array(score13.tolist()).mean())\n","print('abs_ogts: ', np.array(score23.tolist()).mean())\n","print('abs_rlts: ', np.array(score33.tolist()).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#MoverScore\n","\n","print('abs_ots: ', sum(scores12)/len(scores12))\n","print('abs_ogts: ', sum(scores22)/len(scores22))\n","int('abs_rlts: ', sum(scores32)/len(scores32))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["r1 = [np.exp(-0.654991332689921), 0.8518536269664765, 0.5175741747308048]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["r2 = [np.exp(-0.6468217780192693), 0.8508123060067495, 0.5174440166876286]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#r3 = [np.exp(-0.6468217780192693), 0.5174440166876286, 0.8508123060067495]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["normalized_metrics = normalize(np.array([r1, r2]), axis=0, norm='l1')\n","normalized_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["normalized_metrics = normalize(np.array([r1, r2]), axis=0, norm='l1')\n","normalized_metrics"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":2}
