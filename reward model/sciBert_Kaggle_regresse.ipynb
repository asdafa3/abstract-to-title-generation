{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xALrBKkVYuYd"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5bEY6Dydubt",
        "outputId": "1825b67a-7e8a-4a53-8454-a5fb990de100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: git_root in /home/linusb/.local/lib/python3.10/site-packages (0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Running on local machine\n",
            "/home/linusb/git/abstract-to-title-generation\n"
          ]
        }
      ],
      "source": [
        "%pip install git_root\n",
        "\n",
        "PROJECT_ROOT = None\n",
        "in_colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if in_colab:\n",
        "  print('Running on CoLab')\n",
        "  PROJECT_ROOT = \"/content/drive/MyDrive/DL4NLP/abstract-to-title-generation\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "else:\n",
        "  print('Running on local machine')\n",
        "  from git_root import git_root\n",
        "  PROJECT_ROOT = git_root()\n",
        "\n",
        "%cd {PROJECT_ROOT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WguLovM5YuYk",
        "outputId": "3eeead18-71f9-44ff-e818-aa068b3aa8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: transformers in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.19.2)\n",
            "Requirement already satisfied: pandas in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.4.3)\n",
            "Requirement already satisfied: nltk in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.22.3)\n",
            "Requirement already satisfied: tqdm in /usr/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.1.96)\n",
            "Requirement already satisfied: torch in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.0)\n",
            "Requirement already satisfied: scipy in /usr/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.8.0)\n",
            "Requirement already satisfied: git_root in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.1)\n",
            "Requirement already satisfied: dvc[gdrive] in /usr/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.10.2)\n",
            "Requirement already satisfied: rouge_score in /home/linusb/.local/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.0.4)\n",
            "Requirement already satisfied: xxhash in /home/linusb/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /home/linusb/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /home/linusb/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (8.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/linusb/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2022.3.0)\n",
            "Requirement already satisfied: dill in /home/linusb/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /home/linusb/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/linusb/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (2022.4.24)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/linusb/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: filelock in /home/linusb/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: joblib in /home/linusb/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 4)) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/lib/python3.10/site-packages (from torch->-r requirements.txt (line 8)) (4.2.0)\n",
            "Requirement already satisfied: shtab<2,>=1.3.4 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.5.4)\n",
            "Requirement already satisfied: configobj>=5.0.6 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (5.0.6)\n",
            "Requirement already satisfied: dvclive>=0.7.3 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.8.0)\n",
            "Requirement already satisfied: grandalf==0.6 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.6)\n",
            "Requirement already satisfied: flatten-dict<1,>=0.4.1 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.4.2)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (2.8)\n",
            "Requirement already satisfied: diskcache>=5.2.1 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (5.4.0)\n",
            "Requirement already satisfied: pydot>=1.2.4 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.10.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (5.9.0)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.11 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.17.21)\n",
            "Requirement already satisfied: scmrepo==0.0.19 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.0.19)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: dpath<3,>=2.0.2 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (2.0.6)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: aiohttp-retry>=2.4.5 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (2.4.6)\n",
            "Requirement already satisfied: voluptuous>=0.11.7 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.13.1)\n",
            "Requirement already satisfied: dvc-render==0.0.5 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.0.5)\n",
            "Requirement already satisfied: distro>=1.3.0 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.7.0)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.4.4)\n",
            "Requirement already satisfied: nanotime>=0.5.2 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.5.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.0.8)\n",
            "Requirement already satisfied: flufl.lock>=5 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.8.9)\n",
            "Requirement already satisfied: rich>=10.13.0 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (12.4.1)\n",
            "Requirement already satisfied: pathspec<0.10.0,>=0.9.0 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: python-benedict>=0.24.2 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.25.1)\n",
            "Requirement already satisfied: funcy>=1.14 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.17)\n",
            "Requirement already satisfied: dictdiffer>=0.8.1 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: pygtrie>=2.3.2 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (2.4.2)\n",
            "Requirement already satisfied: zc.lockfile>=1.2.1 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (2.0)\n",
            "Requirement already satisfied: pydrive2[fsspec]>=1.10.1 in /usr/lib/python3.10/site-packages (from dvc[gdrive]->-r requirements.txt (line 11)) (1.10.1)\n",
            "Requirement already satisfied: future in /usr/lib/python3.10/site-packages (from grandalf==0.6->dvc[gdrive]->-r requirements.txt (line 11)) (0.18.2)\n",
            "Requirement already satisfied: gitpython>3 in /usr/lib/python3.10/site-packages (from scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (3.1.27)\n",
            "Requirement already satisfied: pygit2>=1.7.2 in /usr/lib/python3.10/site-packages (from scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (1.9.0)\n",
            "Requirement already satisfied: asyncssh<3,>=2.7.1 in /usr/lib/python3.10/site-packages (from scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (2.10.1)\n",
            "Requirement already satisfied: dulwich>=0.20.34 in /usr/lib/python3.10/site-packages (from scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (0.20.36)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /home/linusb/.local/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: atpublic>=2.3 in /usr/lib/python3.10/site-packages (from flufl.lock>=5->dvc[gdrive]->-r requirements.txt (line 11)) (3.0.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.12.5 in /usr/lib/python3.10/site-packages (from pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (2.48.0)\n",
            "Requirement already satisfied: pyOpenSSL>=19.1.0 in /usr/lib/python3.10/site-packages (from pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (22.0.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/lib/python3.10/site-packages (from pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (4.1.3)\n",
            "Requirement already satisfied: phonenumbers<9.0.0,>=8.12.0 in /usr/lib/python3.10/site-packages (from python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (8.12.48)\n",
            "Requirement already satisfied: python-fsutil<1.0.0,>=0.6.0 in /usr/lib/python3.10/site-packages (from python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (0.6.0)\n",
            "Requirement already satisfied: mailchecker<5.0.0,>=4.1.0 in /usr/lib/python3.10/site-packages (from python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (4.1.16)\n",
            "Requirement already satisfied: ftfy<7.0.0,>=6.0.0 in /usr/lib/python3.10/site-packages (from python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (6.1.1)\n",
            "Requirement already satisfied: python-slugify<7.0.0,>=6.0.1 in /usr/lib/python3.10/site-packages (from python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (6.1.2)\n",
            "Requirement already satisfied: xmltodict<1.0.0,>=0.12.0 in /usr/lib/python3.10/site-packages (from python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (0.13.0)\n",
            "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.9)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/lib/python3.10/site-packages (from rich>=10.13.0->dvc[gdrive]->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/lib/python3.10/site-packages (from rich>=10.13.0->dvc[gdrive]->-r requirements.txt (line 11)) (0.9.1)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/lib/python3.10/site-packages (from ruamel.yaml>=0.17.11->dvc[gdrive]->-r requirements.txt (line 11)) (0.2.6)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.10/site-packages (from zc.lockfile>=1.2.1->dvc[gdrive]->-r requirements.txt (line 11)) (60.6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: async_timeout<5.0,>=4.0.0a3 in /usr/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/lib/python3.10/site-packages (from asyncssh<3,>=2.7.1->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (37.0.0)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3.10/site-packages (from dulwich>=0.20.34->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (2022.6.15)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/lib/python3.10/site-packages (from ftfy<7.0.0,>=6.0.0->python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (0.2.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/lib/python3.10/site-packages (from gitpython>3->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (4.0.9)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/lib/python3.10/site-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /usr/lib/python3.10/site-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (2.6.6)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/lib/python3.10/site-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (4.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/lib/python3.10/site-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (2.8.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/lib/python3.10/site-packages (from google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.20.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/lib/python3.10/site-packages (from oauth2client>=4.0.0->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (4.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/lib/python3.10/site-packages (from oauth2client>=4.0.0->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/lib/python3.10/site-packages (from oauth2client>=4.0.0->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (0.2.7)\n",
            "Requirement already satisfied: cffi>=1.9.1 in /usr/lib/python3.10/site-packages (from pygit2>=1.7.2->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/lib/python3.10/site-packages (from python-slugify<7.0.0,>=6.0.1->python-benedict>=0.24.2->dvc[gdrive]->-r requirements.txt (line 11)) (1.3)\n",
            "Requirement already satisfied: pycparser in /usr/lib/python3.10/site-packages (from cffi>=1.9.1->pygit2>=1.7.2->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (2.21)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython>3->scmrepo==0.0.19->dvc[gdrive]->-r requirements.txt (line 11)) (5.0.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (3.20.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (1.56.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.12.5->pydrive2[fsspec]>=1.10.1->dvc[gdrive]->-r requirements.txt (line 11)) (5.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Everything is up to date.                                                       \n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install requirements\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "# pull data only pulls changed data\n",
        "!dvc pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "9WuBmWnZqysV",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import trange\n",
        "from scipy import stats\n",
        "from transformers import BertModel, BertPreTrainedModel, AutoConfig, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prsgC8BwYuYr"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "nit7GMLqFRQ4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# index map, which used to indicate annotated abstract-title pairs.\n",
        "map80 = [[3, 2, 4, 6, 0, 5],\n",
        " [1, 5, 6, 3, 0, 4],\n",
        " [3, 2, 6, 1, 0, 4],\n",
        " [2, 1, 6, 5, 4, 0],\n",
        " [0, 3, 6, 5, 2, 4],\n",
        " [4, 3, 1, 2, 0, 6],\n",
        " [6, 1, 0, 4, 5, 3],\n",
        " [6, 0, 5, 1, 3, 2],\n",
        " [0, 3, 4, 1, 2, 5],\n",
        " [0, 3, 2, 6, 5, 4],\n",
        " [4, 3, 0, 5, 1, 6],\n",
        " [0, 1, 4, 6, 5, 2],\n",
        " [4, 0, 5, 1, 2, 6],\n",
        " [6, 2, 5, 0, 3, 1],\n",
        " [0, 1, 6, 4, 2, 5],\n",
        " [4, 6, 0, 3, 1, 5],\n",
        " [3, 1, 5, 4, 0, 2],\n",
        " [3, 5, 0, 2, 1, 6],\n",
        " [0, 6, 1, 2, 4, 3],\n",
        " [2, 4, 5, 1, 0, 6],\n",
        " [5, 3, 1, 2, 0, 4],\n",
        " [3, 6, 2, 4, 5, 0],\n",
        " [0, 5, 6, 3, 1, 2],\n",
        " [0, 3, 5, 6, 1, 4],\n",
        " [0, 4, 2, 5, 3, 1],\n",
        " [2, 3, 4, 0, 5, 1],\n",
        " [1, 2, 4, 0, 5, 6],\n",
        " [3, 5, 4, 1, 6, 0],\n",
        " [4, 6, 0, 1, 5, 2],\n",
        " [5, 0, 3, 1, 4, 6],\n",
        " [3, 6, 2, 5, 4, 0],\n",
        " [2, 3, 6, 5, 0, 4],\n",
        " [4, 0, 6, 3, 5, 1],\n",
        " [0, 2, 3, 5, 1, 6],\n",
        " [2, 1, 5, 0, 6, 3],\n",
        " [0, 6, 5, 1, 4, 2],\n",
        " [4, 5, 0, 2, 6, 1],\n",
        " [4, 2, 5, 6, 3, 0],\n",
        " [5, 3, 2, 4, 0, 1],\n",
        " [3, 5, 4, 2, 6, 0],\n",
        " [1, 2, 4, 3, 5, 0],\n",
        " [2, 3, 6, 1, 4, 0],\n",
        " [6, 3, 4, 0, 1, 2],\n",
        " [4, 0, 5, 1, 2, 3],\n",
        " [3, 4, 1, 0, 2, 6],\n",
        " [4, 0, 1, 3, 6, 2],\n",
        " [0, 5, 4, 6, 2, 3],\n",
        " [0, 1, 2, 6, 5, 3],\n",
        " [5, 3, 0, 4, 1, 6],\n",
        " [5, 4, 1, 0, 2, 6],\n",
        " [6, 4, 1, 5, 3, 0],\n",
        " [4, 0, 5, 3, 2, 6],\n",
        " [4, 0, 1, 5, 6, 2],\n",
        " [6, 0, 4, 5, 3, 2],\n",
        " [6, 0, 2, 1, 4, 3],\n",
        " [3, 2, 4, 1, 0, 5],\n",
        " [4, 2, 0, 5, 1, 3],\n",
        " [0, 2, 6, 3, 5, 4],\n",
        " [4, 1, 6, 0, 2, 5],\n",
        " [6, 4, 2, 1, 3, 0],\n",
        " [2, 5, 3, 4, 0, 1],\n",
        " [6, 4, 1, 0, 3, 5],\n",
        " [0, 2, 6, 1, 4, 5],\n",
        " [6, 5, 4, 3, 0, 2],\n",
        " [3, 2, 4, 6, 5, 0],\n",
        " [2, 1, 4, 3, 0, 5],\n",
        " [1, 6, 2, 3, 0, 5],\n",
        " [2, 5, 0, 6, 4, 3],\n",
        " [3, 2, 0, 4, 1, 6],\n",
        " [0, 3, 2, 4, 6, 1],\n",
        " [0, 5, 2, 3, 1, 4],\n",
        " [6, 4, 0, 2, 5, 1],\n",
        " [6, 0, 3, 5, 1, 4],\n",
        " [0, 5, 3, 4, 2, 6],\n",
        " [5, 3, 2, 0, 4, 1],\n",
        " [1, 4, 3, 5, 0, 6],\n",
        " [6, 2, 5, 3, 0, 4],\n",
        " [0, 5, 3, 6, 2, 1],\n",
        " [2, 0, 4, 6, 5, 1],\n",
        " [0, 4, 2, 1, 3, 5]]\n",
        "\n",
        "selected_index = [153, 154, 156, 159, 161, 164, 165, 167, 168, 172, 174 ,175, 176, 177, 180, 185, 186, 189, 191, 197, 206, 207, 208, 211, 216, 218, 221, 223, 225, 227, 228, 238, 239, 241, 243, 244, 248, 250, 259, 260, 262, 269, 270, 271 ,275, 287, 288, 291, 294, 299]\n",
        "\n",
        "map50= [[1, 0, 3, 4, 6, 5],\n",
        " [2, 1, 3, 0, 4, 6],\n",
        " [2, 3, 4, 1, 6, 0],\n",
        " [1, 0, 6, 5, 2, 3],\n",
        " [0, 1, 5, 3, 4, 6],\n",
        " [3, 2, 1, 5, 0, 4],\n",
        " [1, 0, 6, 5, 4, 2],\n",
        " [0, 4, 2, 6, 1, 3],\n",
        " [0, 3, 6, 1, 2, 5],\n",
        " [6, 2, 1, 4, 0, 3],\n",
        " [5, 0, 4, 2, 6, 3],\n",
        " [1, 0, 4, 3, 5, 2],\n",
        " [4, 0, 5, 6, 2, 3],\n",
        " [6, 1, 0, 4, 3, 5],\n",
        " [5, 6, 0, 3, 1, 2],\n",
        " [2, 5, 3, 4, 0, 6],\n",
        " [0, 2, 6, 5, 4, 1],\n",
        " [0, 3, 2, 1, 4, 6],\n",
        " [2, 5, 1, 6, 0, 4],\n",
        " [2, 5, 3, 1, 4, 0],\n",
        " [3, 4, 1, 6, 2, 0],\n",
        " [6, 3, 2, 1, 0, 5],\n",
        " [0, 6, 5, 1, 2, 4],\n",
        " [0, 6, 1, 3, 5, 4],\n",
        " [3, 2, 5, 1, 4, 0],\n",
        " [3, 4, 0, 5, 1, 6],\n",
        " [2, 5, 4, 0, 6, 1],\n",
        " [4, 2, 1, 6, 0, 3],\n",
        " [2, 4, 5, 6, 0, 3],\n",
        " [0, 5, 6, 2, 3, 1],\n",
        " [0, 5, 4, 3, 1, 2],\n",
        " [4, 1, 6, 5, 2, 0],\n",
        " [5, 3, 1, 0, 2, 6],\n",
        " [1, 5, 2, 4, 3, 0],\n",
        " [2, 1, 0, 3, 4, 5],\n",
        " [2, 0, 4, 5, 6, 3],\n",
        " [2, 5, 4, 1, 6, 0],\n",
        " [0, 5, 2, 1, 6, 4],\n",
        " [0, 5, 2, 4, 3, 1],\n",
        " [5, 2, 6, 1, 0, 3],\n",
        " [0, 2, 3, 6, 4, 5],\n",
        " [4, 5, 6, 2, 3, 0],\n",
        " [6, 2, 5, 4, 1, 0],\n",
        " [2, 6, 0, 1, 3, 5],\n",
        " [0, 1, 4, 5, 6, 2],\n",
        " [5, 1, 4, 6, 0, 3],\n",
        " [6, 0, 5, 4, 2, 1],\n",
        " [6, 4, 1, 0, 3, 2],\n",
        " [5, 0, 2, 3, 4, 6],\n",
        " [2, 3, 0, 1, 6, 5]]\n",
        "\n",
        "map_model = [['bart_xsum', 'bart_cnn', 't5', 'pegasus_xsum', 'original', 'gpt2'],\n",
        " ['bart_base', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'original', 't5'],\n",
        " ['bart_xsum', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 't5'],\n",
        " ['bart_cnn', 'bart_base', 'pegasus_xsum', 'gpt2', 't5', 'original'],\n",
        " ['original', 'bart_xsum', 'pegasus_xsum', 'gpt2', 'bart_cnn', 't5'],\n",
        " ['t5', 'bart_xsum', 'bart_base', 'bart_cnn', 'original', 'pegasus_xsum'],\n",
        " ['pegasus_xsum', 'bart_base', 'original', 't5', 'gpt2', 'bart_xsum'],\n",
        " ['pegasus_xsum', 'original', 'gpt2', 'bart_base', 'bart_xsum', 'bart_cnn'],\n",
        " ['original', 'bart_xsum', 't5', 'bart_base', 'bart_cnn', 'gpt2'],\n",
        " ['original', 'bart_xsum', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5'],\n",
        " ['t5', 'bart_xsum', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
        " ['original', 'bart_base', 't5', 'pegasus_xsum', 'gpt2', 'bart_cnn'],\n",
        " ['t5', 'original', 'gpt2', 'bart_base', 'bart_cnn', 'pegasus_xsum'],\n",
        " ['pegasus_xsum', 'bart_cnn', 'gpt2', 'original', 'bart_xsum', 'bart_base'],\n",
        " ['original', 'bart_base', 'pegasus_xsum', 't5', 'bart_cnn', 'gpt2'],\n",
        " ['t5', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'gpt2'],\n",
        " ['bart_xsum', 'bart_base', 'gpt2', 't5', 'original', 'bart_cnn'],\n",
        " ['bart_xsum', 'gpt2', 'original', 'bart_cnn', 'bart_base', 'pegasus_xsum'],\n",
        " ['original', 'pegasus_xsum', 'bart_base', 'bart_cnn', 't5', 'bart_xsum'],\n",
        " ['bart_cnn', 't5', 'gpt2', 'bart_base', 'original', 'pegasus_xsum'],\n",
        " ['gpt2', 'bart_xsum', 'bart_base', 'bart_cnn', 'original', 't5'],\n",
        " ['bart_xsum', 'pegasus_xsum', 'bart_cnn', 't5', 'gpt2', 'original'],\n",
        " ['original', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
        " ['original', 'bart_xsum', 'gpt2', 'pegasus_xsum', 'bart_base', 't5'],\n",
        " ['original', 't5', 'bart_cnn', 'gpt2', 'bart_xsum', 'bart_base'],\n",
        " ['bart_cnn', 'bart_xsum', 't5', 'original', 'gpt2', 'bart_base'],\n",
        " ['bart_base', 'bart_cnn', 't5', 'original', 'gpt2', 'pegasus_xsum'],\n",
        " ['bart_xsum', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
        " ['t5', 'pegasus_xsum', 'original', 'bart_base', 'gpt2', 'bart_cnn'],\n",
        " ['gpt2', 'original', 'bart_xsum', 'bart_base', 't5', 'pegasus_xsum'],\n",
        " ['bart_xsum', 'pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'original'],\n",
        " ['bart_cnn', 'bart_xsum', 'pegasus_xsum', 'gpt2', 'original', 't5'],\n",
        " ['t5', 'original', 'pegasus_xsum', 'bart_xsum', 'gpt2', 'bart_base'],\n",
        " ['original', 'bart_cnn', 'bart_xsum', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'bart_base', 'gpt2', 'original', 'pegasus_xsum', 'bart_xsum'],\n",
        " ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 't5', 'bart_cnn'],\n",
        " ['t5', 'gpt2', 'original', 'bart_cnn', 'pegasus_xsum', 'bart_base'],\n",
        " ['t5', 'bart_cnn', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'original'],\n",
        " ['gpt2', 'bart_xsum', 'bart_cnn', 't5', 'original', 'bart_base'],\n",
        " ['bart_xsum', 'gpt2', 't5', 'bart_cnn', 'pegasus_xsum', 'original'],\n",
        " ['bart_base', 'bart_cnn', 't5', 'bart_xsum', 'gpt2', 'original'],\n",
        " ['bart_cnn', 'bart_xsum', 'pegasus_xsum', 'bart_base', 't5', 'original'],\n",
        " ['pegasus_xsum', 'bart_xsum', 't5', 'original', 'bart_base', 'bart_cnn'],\n",
        " ['t5', 'original', 'gpt2', 'bart_base', 'bart_cnn', 'bart_xsum'],\n",
        " ['bart_xsum', 't5', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
        " ['t5', 'original', 'bart_base', 'bart_xsum', 'pegasus_xsum', 'bart_cnn'],\n",
        " ['original', 'gpt2', 't5', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n",
        " ['original', 'bart_base', 'bart_cnn', 'pegasus_xsum', 'gpt2', 'bart_xsum'],\n",
        " ['gpt2', 'bart_xsum', 'original', 't5', 'bart_base', 'pegasus_xsum'],\n",
        " ['gpt2', 't5', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
        " ['bart_base', 'original', 'bart_xsum', 't5', 'pegasus_xsum', 'gpt2'],\n",
        " ['bart_cnn', 'bart_base', 'bart_xsum', 'original', 't5', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
        " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'bart_xsum'],\n",
        " ['original', 'bart_base', 'gpt2', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
        " ['bart_xsum', 'bart_cnn', 'bart_base', 'gpt2', 'original', 't5'],\n",
        " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 't5', 'bart_cnn'],\n",
        " ['original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'bart_xsum'],\n",
        " ['original', 'bart_xsum', 'pegasus_xsum', 'bart_base', 'bart_cnn', 'gpt2'],\n",
        " ['pegasus_xsum', 'bart_cnn', 'bart_base', 't5', 'original', 'bart_xsum'],\n",
        " ['gpt2', 'original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_xsum'],\n",
        " ['bart_base', 'original', 't5', 'bart_xsum', 'gpt2', 'bart_cnn'],\n",
        " ['t5', 'original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n",
        " ['pegasus_xsum', 'bart_base', 'original', 't5', 'bart_xsum', 'gpt2'],\n",
        " ['gpt2', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
        " ['bart_cnn', 'gpt2', 'bart_xsum', 't5', 'original', 'pegasus_xsum'],\n",
        " ['original', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5', 'bart_base'],\n",
        " ['original', 'bart_xsum', 'bart_cnn', 'bart_base', 't5', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'gpt2', 'bart_base', 'pegasus_xsum', 'original', 't5'],\n",
        " ['bart_cnn', 'gpt2', 'bart_xsum', 'bart_base', 't5', 'original'],\n",
        " ['bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'bart_cnn', 'original'],\n",
        " ['pegasus_xsum', 'bart_xsum', 'bart_cnn', 'bart_base', 'original', 'gpt2'],\n",
        " ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 'bart_cnn', 't5'],\n",
        " ['original', 'pegasus_xsum', 'bart_base', 'bart_xsum', 'gpt2', 't5'],\n",
        " ['bart_xsum', 'bart_cnn', 'gpt2', 'bart_base', 't5', 'original'],\n",
        " ['bart_xsum', 't5', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'gpt2', 't5', 'original', 'pegasus_xsum', 'bart_base'],\n",
        " ['t5', 'bart_cnn', 'bart_base', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
        " ['bart_cnn', 't5', 'gpt2', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
        " ['original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'bart_base'],\n",
        " ['original', 'gpt2', 't5', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
        " ['t5', 'bart_base', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'original'],\n",
        " ['gpt2', 'bart_xsum', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
        " ['bart_base', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'original'],\n",
        " ['bart_cnn', 'bart_base', 'original', 'bart_xsum', 't5', 'gpt2'],\n",
        " ['bart_cnn', 'original', 't5', 'gpt2', 'pegasus_xsum', 'bart_xsum'],\n",
        " ['bart_cnn', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
        " ['original', 'gpt2', 'bart_cnn', 'bart_base', 'pegasus_xsum', 't5'],\n",
        " ['original', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'bart_base'],\n",
        " ['gpt2', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 'bart_xsum'],\n",
        " ['original', 'bart_cnn', 'bart_xsum', 'pegasus_xsum', 't5', 'gpt2'],\n",
        " ['t5', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'original'],\n",
        " ['pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'bart_base', 'original'],\n",
        " ['bart_cnn', 'pegasus_xsum', 'original', 'bart_base', 'bart_xsum', 'gpt2'],\n",
        " ['original', 'bart_base', 't5', 'gpt2', 'pegasus_xsum', 'bart_cnn'],\n",
        " ['gpt2', 'bart_base', 't5', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
        " ['pegasus_xsum', 'original', 'gpt2', 't5', 'bart_cnn', 'bart_base'],\n",
        " ['pegasus_xsum', 't5', 'bart_base', 'original', 'bart_xsum', 'bart_cnn'],\n",
        " ['gpt2', 'original', 'bart_cnn', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'bart_xsum', 'original', 'bart_base', 'pegasus_xsum', 'gpt2'],\n",
        " ['bart_base', 'original', 'bart_xsum', 't5', 'pegasus_xsum', 'gpt2'],\n",
        " ['bart_cnn', 'bart_base', 'bart_xsum', 'original', 't5', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
        " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'bart_xsum'],\n",
        " ['original', 'bart_base', 'gpt2', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
        " ['bart_xsum', 'bart_cnn', 'bart_base', 'gpt2', 'original', 't5'],\n",
        " ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 't5', 'bart_cnn'],\n",
        " ['original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'bart_xsum'],\n",
        " ['original', 'bart_xsum', 'pegasus_xsum', 'bart_base', 'bart_cnn', 'gpt2'],\n",
        " ['pegasus_xsum', 'bart_cnn', 'bart_base', 't5', 'original', 'bart_xsum'],\n",
        " ['gpt2', 'original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_xsum'],\n",
        " ['bart_base', 'original', 't5', 'bart_xsum', 'gpt2', 'bart_cnn'],\n",
        " ['t5', 'original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n",
        " ['pegasus_xsum', 'bart_base', 'original', 't5', 'bart_xsum', 'gpt2'],\n",
        " ['gpt2', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
        " ['bart_cnn', 'gpt2', 'bart_xsum', 't5', 'original', 'pegasus_xsum'],\n",
        " ['original', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5', 'bart_base'],\n",
        " ['original', 'bart_xsum', 'bart_cnn', 'bart_base', 't5', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'gpt2', 'bart_base', 'pegasus_xsum', 'original', 't5'],\n",
        " ['bart_cnn', 'gpt2', 'bart_xsum', 'bart_base', 't5', 'original'],\n",
        " ['bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'bart_cnn', 'original'],\n",
        " ['pegasus_xsum', 'bart_xsum', 'bart_cnn', 'bart_base', 'original', 'gpt2'],\n",
        " ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 'bart_cnn', 't5'],\n",
        " ['original', 'pegasus_xsum', 'bart_base', 'bart_xsum', 'gpt2', 't5'],\n",
        " ['bart_xsum', 'bart_cnn', 'gpt2', 'bart_base', 't5', 'original'],\n",
        " ['bart_xsum', 't5', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'gpt2', 't5', 'original', 'pegasus_xsum', 'bart_base'],\n",
        " ['t5', 'bart_cnn', 'bart_base', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
        " ['bart_cnn', 't5', 'gpt2', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
        " ['original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'bart_base'],\n",
        " ['original', 'gpt2', 't5', 'bart_xsum', 'bart_base', 'bart_cnn'],\n",
        " ['t5', 'bart_base', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'original'],\n",
        " ['gpt2', 'bart_xsum', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n",
        " ['bart_base', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'original'],\n",
        " ['bart_cnn', 'bart_base', 'original', 'bart_xsum', 't5', 'gpt2'],\n",
        " ['bart_cnn', 'original', 't5', 'gpt2', 'pegasus_xsum', 'bart_xsum'],\n",
        " ['bart_cnn', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n",
        " ['original', 'gpt2', 'bart_cnn', 'bart_base', 'pegasus_xsum', 't5'],\n",
        " ['original', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'bart_base'],\n",
        " ['gpt2', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 'bart_xsum'],\n",
        " ['original', 'bart_cnn', 'bart_xsum', 'pegasus_xsum', 't5', 'gpt2'],\n",
        " ['t5', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'original'],\n",
        " ['pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'bart_base', 'original'],\n",
        " ['bart_cnn', 'pegasus_xsum', 'original', 'bart_base', 'bart_xsum', 'gpt2'],\n",
        " ['original', 'bart_base', 't5', 'gpt2', 'pegasus_xsum', 'bart_cnn'],\n",
        " ['gpt2', 'bart_base', 't5', 'pegasus_xsum', 'original', 'bart_xsum'],\n",
        " ['pegasus_xsum', 'original', 'gpt2', 't5', 'bart_cnn', 'bart_base'],\n",
        " ['pegasus_xsum', 't5', 'bart_base', 'original', 'bart_xsum', 'bart_cnn'],\n",
        " ['gpt2', 'original', 'bart_cnn', 'bart_xsum', 't5', 'pegasus_xsum'],\n",
        " ['bart_cnn', 'bart_xsum', 'original', 'bart_base', 'pegasus_xsum', 'gpt2']]\n",
        "\n",
        "\n",
        "def displaysmaples(samples):\n",
        "  i = 0\n",
        "  titles = samples.title.to_list()\n",
        "  abstract = samples.abstract.to_list()\n",
        "  bart_base = samples.bart_base.to_list()\n",
        "  bart_cnn = samples.bart_cnn.to_list()\n",
        "  bart_xsum = samples.bart_xsum.to_list()\n",
        "  t5_small = samples.t5_small.to_list()\n",
        "  gpt2 = samples.gpt2.to_list()\n",
        "  pegasus_xsum = samples.pegasus_xsum.to_list()\n",
        "\n",
        "  for t, a, t1, t2, t3, t4, t5, t6 in zip(titles, abstract, bart_base, bart_cnn, bart_xsum, t5_small, gpt2, pegasus_xsum):\n",
        "    print(\"original title: \", t)\n",
        "    print(\"abstract: \", a)\n",
        "    print(\"from bart_base: \")\n",
        "    gt1 = t1.split(\"<TITLE>\")[1:]\n",
        "    for gt in gt1:\n",
        "      print(gt)\n",
        "    print(\"\\n\")\n",
        "    print(\"from bart_cnn: \")\n",
        "    gt1 = t2.split(\"<TITLE>\")[1:]\n",
        "    for gt in gt1:\n",
        "      print(gt)\n",
        "    print(\"\\n\")\n",
        "    print(\"from bart_xsum: \")\n",
        "    gt1 = t3.split(\"<TITLE>\")[1:]\n",
        "    for gt in gt1:\n",
        "      print(gt)\n",
        "    print(\"\\n\")\n",
        "    print(\"from t5_small: \")\n",
        "    gt1 = t4.split(\"<TITLE>\")[1:]\n",
        "    for gt in gt1:\n",
        "      print(gt)\n",
        "    print(\"\\n\")\n",
        "    print(\"from gpt2: \")\n",
        "    gt1 = t5.split(\"<TITLE>\")[1:]\n",
        "    for gt in gt1:\n",
        "      print(gt)\n",
        "    print(\"\\n\")\n",
        "    print(\"from pegasus_xsum: \")\n",
        "    gt1 = t6.split(\"<TITLE>\")[1:]\n",
        "    for gt in gt1:\n",
        "      print(gt)\n",
        "    print(\"\\n\")\n",
        "    i = i + 1\n",
        "\n",
        "    if i >= 20:\n",
        "      break\n",
        "\n",
        "# index von Optionen der human evaluation, 20 - 99 samples und 100-150(von index 0 bis 49)\n",
        "# [4, 3, 5, 2, 0, 1] bedeutet\n",
        "#1.op aus t5_samall,\n",
        "#2.op aus bart_xsum,\n",
        "#3.op aus gpt2,\n",
        "#4.op aus bart_cnn,\n",
        "#5.op original,\n",
        "#6.op aus bart_base,\n",
        "\n",
        "def getindex():\n",
        "  result = []\n",
        "  for i in range(80):\n",
        "    col = np.arange(1,7)\n",
        "    np.random.shuffle(col)\n",
        "    col = col.tolist()[:-1]\n",
        "    col.append(0)\n",
        "    np.random.shuffle(col)\n",
        "    result.append(col)\n",
        "  return result\n",
        "\n",
        "\n",
        "def sampler(samples, map20, start, end):\n",
        "  i = start\n",
        "  titles = samples.title.to_list()[start: end]\n",
        "  abstract = samples.abstract.to_list()[start: end]\n",
        "  bart_base = samples.bart_base.to_list()[start: end]\n",
        "  bart_cnn = samples.bart_cnn.to_list()[start: end]\n",
        "  bart_xsum = samples.bart_xsum.to_list()[start: end]\n",
        "  t5_small = samples.t5_small.to_list()[start: end]\n",
        "  gpt2 = samples.gpt2.to_list()[start: end]\n",
        "  pegasus_xsum = samples.pegasus_xsum.to_list()[start: end]\n",
        "\n",
        "  tmap = []\n",
        "  for t, a, t1, t2, t3, t4, t5, t6 in zip(titles, abstract, bart_base, bart_cnn, bart_xsum, t5_small, gpt2, pegasus_xsum):\n",
        "    gt1 = t1.split(\"<TITLE>\")[1:][0]\n",
        "    gt2 = t2.split(\"<TITLE>\")[1:][0]\n",
        "    gt3 = t3.split(\"<TITLE>\")[1:][0]\n",
        "    gt4 = t4.split(\"<TITLE>\")[1:][0]\n",
        "    gt5 = t5.split(\"<TITLE>\")[1:][0]\n",
        "    gt6 = t6.split(\"<TITLE>\")[1:][0]\n",
        "    col = [t, gt1, gt2, gt3, gt4, gt5, gt6]\n",
        "    rcol = [col[map20[i][j]] for j in range(6)]\n",
        "    rcol.append(a)\n",
        "    tmap.append(rcol)\n",
        "    i = i + 1\n",
        "\n",
        "  return tmap\n",
        "def map2model(ls):\n",
        "  r = []\n",
        "  for i in ls:\n",
        "    if i == 0:\n",
        "      r.append(\"original\")\n",
        "    if i == 1:\n",
        "      r.append(\"bart_base\")\n",
        "    if i == 2:\n",
        "      r.append(\"bart_cnn\")\n",
        "    if i == 3:\n",
        "      r.append(\"bart_xsum\")\n",
        "    if i == 4:\n",
        "      r.append(\"t5\")\n",
        "    if i == 5:\n",
        "      r.append(\"gpt2\")\n",
        "    if i == 6:\n",
        "      r.append(\"pegasus_xsum\")\n",
        "  return r\n",
        "\n",
        "def map2index(ls):\n",
        "  r = []\n",
        "  for i in ls:\n",
        "    if i == \"original\":\n",
        "      r.append(0)\n",
        "    if i == \"bart_base\":\n",
        "      r.append(1)\n",
        "    if i == \"bart_cnn\":\n",
        "      r.append(2)\n",
        "    if i == \"bart_xsum\":\n",
        "      r.append(3)\n",
        "    if i == \"t5\":\n",
        "      r.append(4)\n",
        "    if i == \"gpt2\":\n",
        "      r.append(5)\n",
        "    if i == \"pegasus_xsum\":\n",
        "      r.append(6)\n",
        "  return r\n",
        "\n",
        "def map2modelAM(ls):\n",
        "  r = []\n",
        "  for i in ls:\n",
        "    if i == 0:\n",
        "      r.append(\"bart_base\")\n",
        "    if i == 1:\n",
        "      r.append(\"bart_cnn\")\n",
        "    if i == 2:\n",
        "      r.append(\"bart_xsum\")\n",
        "    if i == 3:\n",
        "      r.append(\"t5\")\n",
        "    if i == 4:\n",
        "      r.append(\"gpt2\")\n",
        "    if i == 5:\n",
        "      r.append(\"pegasus_xsum\")\n",
        "  return r\n",
        "\n",
        "\n",
        "# index von Optionen der human evaluation, 150 - 200 samples\n",
        "# [4, 3, 5, 2, 0, 1] bedeutet\n",
        "#1.op aus t5_samall,\n",
        "#2.op aus bart_xsum,\n",
        "#3.op aus gpt2,\n",
        "#4.op aus bart_cnn,\n",
        "#5.op original,\n",
        "#6.op aus bart_base,\n",
        "\n",
        "def getindex():\n",
        "  result = []\n",
        "  for i in range(50):\n",
        "    col = np.arange(1,7)\n",
        "    np.random.shuffle(col)\n",
        "    col = col.tolist()[:-1]\n",
        "    col.append(0)\n",
        "    np.random.shuffle(col)\n",
        "    result.append(col)\n",
        "  return result\n",
        "\n",
        "#map für Teinehmer\n",
        "\n",
        "def getTmap():\n",
        "  i = 0\n",
        "  b = False\n",
        "  map30 = []\n",
        "  counters = np.zeros(20).tolist()\n",
        "  while not b:\n",
        "    personMap = np.arange(20)\n",
        "    np.random.shuffle(personMap)\n",
        "    personMap = personMap[:10]\n",
        "    i0 = personMap[0]\n",
        "    i1 = personMap[1]\n",
        "\n",
        "    if (counters[i0] < 3) and (counters[i1] < 3):\n",
        "      counters[i0] = counters[i0] + 1\n",
        "      counters[i1] = counters[i1] + 1\n",
        "      map30.append(personMap)\n",
        "    tb = True\n",
        "    for t in counters:\n",
        "      tb = tb and (t == 3)\n",
        "    b = tb\n",
        "    if b == True:\n",
        "      print(\"terminiert at:\", counters)\n",
        "  return map30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0woEoArFGmq8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# **Fine Tuning**\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "GQnEmWjNsJJK",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Excerpt_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, tokenizer):\n",
        "        #Store the contents of the file in a pandas dataframe\n",
        "        self.df = data.reset_index()\n",
        "        #Initialize the tokenizer for the desired transformer model\n",
        "        self.tokenizer = tokenizer\n",
        "        #Maximum length of the tokens list to keep all the sequences of fixed size\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Select the sentence and label at the specified index in the data frame\n",
        "        excerpt = self.df.loc[index, 'excerpt']\n",
        "        try:\n",
        "            target = self.df.loc[index, 'target']\n",
        "        except:\n",
        "            target = 0.0\n",
        "        #identifier = self.df.loc[index, 'id']\n",
        "        #Preprocess the text to be suitable for the transformer\n",
        "        tokens = self.tokenizer.tokenize(excerpt)\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        if len(tokens) < self.maxlen:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]']\n",
        "        #Obtain the indices of the tokens in the BERT Vocabulary\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attention_mask = (input_ids != 0).long()\n",
        "\n",
        "        target = torch.tensor([target], dtype=torch.float32)\n",
        "\n",
        "        return input_ids, attention_mask, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SmOlriORubv4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class BertRegresser(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        #The output layer that takes the [CLS] representation and gives an output\n",
        "        self.cls_layer1 = nn.Linear(config.hidden_size,128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.ff1 = nn.Linear(128,128)\n",
        "        self.tanh1 = nn.Tanh()\n",
        "        self.ff2 = nn.Linear(128,1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        #Feed the input to Bert model to obtain contextualized representations\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        #Obtain the representations of [CLS] heads\n",
        "        logits = outputs.last_hidden_state[:,0,:]\n",
        "        output = self.cls_layer1(logits)\n",
        "        output = self.relu1(output)\n",
        "        output = self.ff1(output)\n",
        "        output = self.tanh1(output)\n",
        "        output = self.ff2(output)\n",
        "        return output\n",
        "'''\n",
        "class BertRegresser(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        #The output layer that takes the [CLS] representation and gives an output\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(p['dropout']),\n",
        "            nn.Linear(768, 1))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        #Feed the input to Bert model to obtain contextualized representations\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        #Obtain the representations of [CLS] heads\n",
        "        logits = outputs[1]\n",
        "        output = self.regressor(logits)\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v936RtNkYuZk"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J7sZxb6Frmpj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def setup_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.backends.cudnn.deteministic = True\n",
        "\n",
        "## Model Configurations\n",
        "p = {\n",
        "    'max_len': 512,\n",
        "    'batch_size': 6,\n",
        "    'lr' : 4.063769241800226e-05,\n",
        "    'epochs': 12,\n",
        "    'dropout': 0.5,\n",
        "    'num_threads' : 1,\n",
        "    #'model_name' : 'allenai/scibert_scivocab_uncased',\n",
        "    'model_name' : 'bert-base-uncased',\n",
        "    'do_train' : True,\n",
        "    'random_seed': 24\n",
        "}\n",
        "\n",
        "setup_seed(p['random_seed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dGvKcMnwJ4G",
        "outputId": "f5da679e-1fce-4beb-a034-0059514f0dbd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertRegresser: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertRegresser from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertRegresser from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertRegresser were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['regressor.1.weight', 'regressor.1.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert.pooler.dense.weight torch.Size([768, 768])\n",
            "bert.pooler.dense.bias torch.Size([768])\n",
            "regressor.1.weight torch.Size([1, 768])\n",
            "regressor.1.bias torch.Size([1])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "    mean_acc, mean_loss, count = 0, 0, 0\n",
        "    preds = []\n",
        "    lst_label = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, target in (dataloader):\n",
        "\n",
        "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
        "            output = model(input_ids, attention_mask)\n",
        "            preds += output\n",
        "            lst_label += target\n",
        "            mean_loss += criterion(output, target.type_as(output)).item()\n",
        "#             mean_err += get_rmse(output, target)\n",
        "            count += 1\n",
        "        predss = np.array([x.cpu().data.numpy().tolist() for x in preds]).squeeze()\n",
        "        lst_labels = np.array([x.cpu().data.numpy().tolist() for x in lst_label]).squeeze()\n",
        "        corr = stats.spearmanr(predss, lst_labels)\n",
        "    return corr[0] #mean_loss/count\n",
        "\n",
        "def train(model, criterion, optimizer, train_loader, val_loader, epochs, device):\n",
        "    best_acc = 0\n",
        "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
        "\n",
        "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(output, target.type_as(output))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f\"Training loss is {train_loss/len(train_loader)}\")\n",
        "        val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n",
        "        print(\"Epoch {} complete! Correlations : {}\".format(epoch, val_loss))\n",
        "\n",
        "\n",
        "## Configuration loaded from AutoConfig\n",
        "aconfig = AutoConfig.from_pretrained(p['model_name'])\n",
        "## Tokenizer loaded from AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(p['model_name'])\n",
        "## Creating the model from the desired transformer model\n",
        "model = BertRegresser.from_pretrained(p['model_name'], config=aconfig)\n",
        "\n",
        "\n",
        "#frezze all layers except regression head\n",
        "'''\n",
        "unfreeze_layers  = ['bert.pooler', 'cls_layer1.', 'ff1.', 'ff2']\n",
        "\n",
        "for name, params in model.named_parameters():\n",
        "  params.requires_grad = False\n",
        "  for ele in unfreeze_layers:\n",
        "    if ele in name:\n",
        "      params.requires_grad = True\n",
        "      break\n",
        "\n",
        "for name, params in model.named_parameters():\n",
        "  if params.requires_grad:\n",
        "    print(name, params.size())\n",
        "'''\n",
        "\n",
        "unfreeze_layers = ['bert.pooler', 'regressor.1']\n",
        "for name, params in model.named_parameters():\n",
        "  params.requires_grad = False\n",
        "  for ele in unfreeze_layers:\n",
        "    if ele in name:\n",
        "      params.requires_grad = True\n",
        "      break\n",
        "\n",
        "for name, params in model.named_parameters():\n",
        "  if params.requires_grad:\n",
        "    print(name, params.size())\n",
        "\n",
        "arch = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(arch)\n",
        "device = torch.device(arch)\n",
        "\n",
        "## Putting model to device\n",
        "model = model.to(device)\n",
        "## Takes as the input the logits of the positive class and computes the binary cross-entropy\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = nn.MSELoss()\n",
        "## Optimizer\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=p['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3PdNb9OpDUq",
        "outputId": "bc94d1c5-1276-4cdd-ad37-ced32f497ddc"
      },
      "outputs": [],
      "source": [
        "MODEL_OUT_DIR = f'{PROJECT_ROOT}/output/sciBert_Kaggle'\n",
        "\n",
        "#for s in p.values():\n",
        "#  MODEL_OUT_DIR += '_' + str(s).replace('/', '_', 1)\n",
        "\n",
        "#print(MODEL_OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9lrD5eHbFIKk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "DATA_DIR = f'{PROJECT_ROOT}/data/annotated'\n",
        "text_map = pd.read_csv(f'{DATA_DIR}/140_annotations_pairs.csv', index_col=0)\n",
        "scores = pd.read_csv(f'{DATA_DIR}/140_humanannotation_withoutunannotated.csv', index_col=0)\n",
        "\n",
        "abstract_df =text_map[['abstract']]\n",
        "title_df = text_map[['title', 'bart_base', 'bart_cnn', 'bart_xsum', 't5_small', 'gpt2', 'pegasus_xsum']]\n",
        "abstract_np = abstract_df.to_numpy()\n",
        "scores_np = scores.to_numpy()\n",
        "title_np = title_df.to_numpy()\n",
        "idx_map = [map2index(row) for row in map_model[:140]]\n",
        "title_np_picked = np.array([np.take(row1, np.sort(row2)) for row1, row2 in zip(title_np, idx_map)])\n",
        "score_np_picked = np.array([np.take(row1, np.sort(row2)) for row1, row2 in zip(scores_np, idx_map)])\n",
        "\n",
        "pairs_np_picked = np.concatenate([abstract_np, title_np_picked,score_np_picked], axis=1)\n",
        "pairs_np_picked_shuffled = np.random.permutation(pairs_np_picked)\n",
        "\n",
        "abstracs = pairs_np_picked_shuffled[:,:1]\n",
        "titles = pairs_np_picked_shuffled[:,1:7]\n",
        "scores = pairs_np_picked_shuffled[:,7:].astype(float)\n",
        "scores = np.around(scores, 4)\n",
        "\n",
        "lst = []\n",
        "\n",
        "for ab, row1, row2 in zip(abstracs, titles, scores):\n",
        "  assert len(row1) == len(row2)\n",
        "  assert len(row1) == 6\n",
        "  for t, s in zip(row1, row2):\n",
        "\n",
        "    if np.isnan(s):\n",
        "      print('found nan score')\n",
        "    if t=='':\n",
        "      print('found empty title')\n",
        "    lst.append([ab[0] + '[SEP]' + t, s])\n",
        "df = pd.DataFrame(np.array(lst))\n",
        "\n",
        "df.columns = ['excerpt', 'target']\n",
        "#dataframe = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "dftrain = df[:600].reset_index(drop=True)\n",
        "dfdev = df[600:660].reset_index(drop=True)\n",
        "dftest = df[660:].reset_index(drop=True)\n",
        "\n",
        "OUT_DIR = f'{MODEL_OUT_DIR}/robust_test'\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "dftrain.to_csv(f'{OUT_DIR}/sciBert_shuffled_train.csv')\n",
        "dfdev.to_csv(f'{OUT_DIR}/sciBert_shuffled_dev.csv')\n",
        "dftest.to_csv(f'{OUT_DIR}/sciBert_shuffled_test.csv')\n",
        "\n",
        "dftrain = pd.read_csv(f'{OUT_DIR}/sciBert_shuffled_train.csv', index_col=0)\n",
        "dfdev = pd.read_csv(f'{OUT_DIR}/sciBert_shuffled_dev.csv', index_col=0)\n",
        "dftest = pd.read_csv(f'{OUT_DIR}/sciBert_shuffled_test.csv', index_col=0)\n",
        "\n",
        "\n",
        "## Training Dataset\n",
        "train_set = Excerpt_Dataset(data=dftrain, maxlen=p['max_len'], tokenizer=tokenizer)\n",
        "dev_set = Excerpt_Dataset(data=dfdev, maxlen=p['max_len'], tokenizer=tokenizer)\n",
        "test_set = Excerpt_Dataset(data=dftest, maxlen=p['max_len'], tokenizer=tokenizer)\n",
        "\n",
        "## Data Loaders\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)\n",
        "dev_loader = DataLoader(dataset=dev_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qMee8sQz2NV5",
        "outputId": "a7d3e9d1-2485-44c4-f344-39f45ae55397",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss is 0.4737148632109165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   8%|▊         | 1/12 [02:02<22:25, 122.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 complete! Correlations : 0.04497406305619948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   8%|▊         | 1/12 [03:29<38:24, 209.46s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/linusb/git/abstract-to-title-generation/reward model/sciBert_Kaggle_regresse.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m p[\u001b[39m'\u001b[39m\u001b[39mdo_train\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=1'>2</a>\u001b[0m   train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=2'>3</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=3'>4</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=4'>5</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=5'>6</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=6'>7</a>\u001b[0m     val_loader\u001b[39m=\u001b[39;49mdev_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=7'>8</a>\u001b[0m     epochs \u001b[39m=\u001b[39;49m p[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=8'>9</a>\u001b[0m     device \u001b[39m=\u001b[39;49m device\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000014?line=9'>10</a>\u001b[0m   )\n",
            "\u001b[1;32m/home/linusb/git/abstract-to-title-generation/reward model/sciBert_Kaggle_regresse.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000011?line=31'>32</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000011?line=32'>33</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000011?line=34'>35</a>\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000011?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining loss is \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_loader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linusb/git/abstract-to-title-generation/reward%20model/sciBert_Kaggle_regresse.ipynb#ch0000011?line=37'>38</a>\u001b[0m val_loss \u001b[39m=\u001b[39m evaluate(model\u001b[39m=\u001b[39mmodel, criterion\u001b[39m=\u001b[39mcriterion, dataloader\u001b[39m=\u001b[39mval_loader, device\u001b[39m=\u001b[39mdevice)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if p['do_train']:\n",
        "  train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=dev_loader,\n",
        "    epochs = p['epochs'],\n",
        "    device = device\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg9AtLg-axHn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "#torch.save(model.state_dict(), MODEL_OUT_DIR + '.model')\n",
        "\n",
        "#load model\n",
        "model_state, optimizer_state = torch.load(os.path.join('../output/sciBert_Kaggle/ray_results/runner_2022-04-29_16-44-10/runner_980f0_00002_2_lr=4.0638e-05_2022-04-29_17-04-24/checkpoint_000017', \"checkpoint\"))\n",
        "model.load_state_dict(model_state)\n",
        "\n",
        "#model.load_state_dict(torch.load(\"../output/sciBert_Kaggle/sciBert.model\"))\n",
        "#model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATEEkP7Narl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#@title prediction function\n",
        "def predict(model, dataloader, device):\n",
        "    predicted_label = []\n",
        "    actual_label = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, target in (dataloader):\n",
        "            \n",
        "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
        "            output = model(input_ids, attention_mask)\n",
        "                        \n",
        "            predicted_label += output\n",
        "            actual_label += target\n",
        "            \n",
        "    return predicted_label, actual_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_iKX2VOG7ti",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# **Display Correlation**\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN5JZ9ElGABX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "output,GS_label = predict(model, train_loader, device)\n",
        "cpu_output = np.array([x.cpu().data.numpy() for x in output]).squeeze()\n",
        "cpu_target = np.array([x.cpu().data.numpy() for x in GS_label]).squeeze()\n",
        "stats.spearmanr(cpu_output, cpu_target)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEV-es5KpREu",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "dev_output,dev_GS_label = predict(model, dev_loader, device)\n",
        "cpu_dev_output = np.array([x.cpu().data.numpy() for x in dev_output]).squeeze()\n",
        "cpu_dev_target = np.array([x.cpu().data.numpy() for x in dev_GS_label]).squeeze()\n",
        "stats.spearmanr(cpu_dev_output, cpu_dev_target)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcFR76sa53F7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_output,test_GS_label = predict(model, test_loader, device)\n",
        "cpu_test_output = np.array([x.cpu().data.numpy() for x in test_output]).squeeze()\n",
        "cpu_test_target = np.array([x.cpu().data.numpy() for x in test_GS_label]).squeeze()\n",
        "stats.spearmanr(cpu_test_output, cpu_test_target)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfX7ty-34BEo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "dev_output,dev_GS_label = predict(model, dev_loader, device)\n",
        "cpu_dev_output = np.array([x.cpu().data.numpy() for x in dev_output]).squeeze()\n",
        "cpu_dev_target = np.array([x.cpu().data.numpy() for x in dev_GS_label]).squeeze()\n",
        "stats.spearmanr(cpu_dev_output, cpu_dev_target)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l70-oVBm8oP7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_output,test_GS_label = predict(model, test_loader, device)\n",
        "cpu_test_output = np.array([x.cpu().data.numpy() for x in test_output]).squeeze()\n",
        "cpu_test_target = np.array([x.cpu().data.numpy() for x in test_GS_label]).squeeze()\n",
        "stats.spearmanr(cpu_test_output, cpu_test_target)[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Kopie von sciBert_Kaggle_regresse.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
