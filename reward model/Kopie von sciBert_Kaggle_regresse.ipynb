{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopie von sciBert_Kaggle_regresse.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[],"mount_file_id":"1y4K1d4WObidfwPOxn9kTWzyzvtBAvIn5","authorship_tag":"ABX9TyM5vlKH62PMx52UtHKmwoDq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9WuBmWnZqysV","cellView":"form"},"outputs":[],"source":["#@title install  { form-width: \"10%\" }\n","%cd /content/drive/MyDrive/Thesis\n","!pip install transformers\n","!pip install sentencepiece\n","import pandas as pd\n","import numpy as np\n","!pip install datasets\n","import torch\n","import datasets\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","source":["#@title index map, which used to indicate annotated abstract-title pairs.\n","map80 = [[3, 2, 4, 6, 0, 5],\n"," [1, 5, 6, 3, 0, 4],\n"," [3, 2, 6, 1, 0, 4],\n"," [2, 1, 6, 5, 4, 0],\n"," [0, 3, 6, 5, 2, 4],\n"," [4, 3, 1, 2, 0, 6],\n"," [6, 1, 0, 4, 5, 3],\n"," [6, 0, 5, 1, 3, 2],\n"," [0, 3, 4, 1, 2, 5],\n"," [0, 3, 2, 6, 5, 4],\n"," [4, 3, 0, 5, 1, 6],\n"," [0, 1, 4, 6, 5, 2],\n"," [4, 0, 5, 1, 2, 6],\n"," [6, 2, 5, 0, 3, 1],\n"," [0, 1, 6, 4, 2, 5],\n"," [4, 6, 0, 3, 1, 5],\n"," [3, 1, 5, 4, 0, 2],\n"," [3, 5, 0, 2, 1, 6],\n"," [0, 6, 1, 2, 4, 3],\n"," [2, 4, 5, 1, 0, 6],\n"," [5, 3, 1, 2, 0, 4],\n"," [3, 6, 2, 4, 5, 0],\n"," [0, 5, 6, 3, 1, 2],\n"," [0, 3, 5, 6, 1, 4],\n"," [0, 4, 2, 5, 3, 1],\n"," [2, 3, 4, 0, 5, 1],\n"," [1, 2, 4, 0, 5, 6],\n"," [3, 5, 4, 1, 6, 0],\n"," [4, 6, 0, 1, 5, 2],\n"," [5, 0, 3, 1, 4, 6],\n"," [3, 6, 2, 5, 4, 0],\n"," [2, 3, 6, 5, 0, 4],\n"," [4, 0, 6, 3, 5, 1],\n"," [0, 2, 3, 5, 1, 6],\n"," [2, 1, 5, 0, 6, 3],\n"," [0, 6, 5, 1, 4, 2],\n"," [4, 5, 0, 2, 6, 1],\n"," [4, 2, 5, 6, 3, 0],\n"," [5, 3, 2, 4, 0, 1],\n"," [3, 5, 4, 2, 6, 0],\n"," [1, 2, 4, 3, 5, 0],\n"," [2, 3, 6, 1, 4, 0],\n"," [6, 3, 4, 0, 1, 2],\n"," [4, 0, 5, 1, 2, 3],\n"," [3, 4, 1, 0, 2, 6],\n"," [4, 0, 1, 3, 6, 2],\n"," [0, 5, 4, 6, 2, 3],\n"," [0, 1, 2, 6, 5, 3],\n"," [5, 3, 0, 4, 1, 6],\n"," [5, 4, 1, 0, 2, 6],\n"," [6, 4, 1, 5, 3, 0],\n"," [4, 0, 5, 3, 2, 6],\n"," [4, 0, 1, 5, 6, 2],\n"," [6, 0, 4, 5, 3, 2],\n"," [6, 0, 2, 1, 4, 3],\n"," [3, 2, 4, 1, 0, 5],\n"," [4, 2, 0, 5, 1, 3],\n"," [0, 2, 6, 3, 5, 4],\n"," [4, 1, 6, 0, 2, 5],\n"," [6, 4, 2, 1, 3, 0],\n"," [2, 5, 3, 4, 0, 1],\n"," [6, 4, 1, 0, 3, 5],\n"," [0, 2, 6, 1, 4, 5],\n"," [6, 5, 4, 3, 0, 2],\n"," [3, 2, 4, 6, 5, 0],\n"," [2, 1, 4, 3, 0, 5],\n"," [1, 6, 2, 3, 0, 5],\n"," [2, 5, 0, 6, 4, 3],\n"," [3, 2, 0, 4, 1, 6],\n"," [0, 3, 2, 4, 6, 1],\n"," [0, 5, 2, 3, 1, 4],\n"," [6, 4, 0, 2, 5, 1],\n"," [6, 0, 3, 5, 1, 4],\n"," [0, 5, 3, 4, 2, 6],\n"," [5, 3, 2, 0, 4, 1],\n"," [1, 4, 3, 5, 0, 6],\n"," [6, 2, 5, 3, 0, 4],\n"," [0, 5, 3, 6, 2, 1],\n"," [2, 0, 4, 6, 5, 1],\n"," [0, 4, 2, 1, 3, 5]]\n","\n","selected_index = [153, 154, 156, 159, 161, 164, 165, 167, 168, 172, 174 ,175, 176, 177, 180, 185, 186, 189, 191, 197, 206, 207, 208, 211, 216, 218, 221, 223, 225, 227, 228, 238, 239, 241, 243, 244, 248, 250, 259, 260, 262, 269, 270, 271 ,275, 287, 288, 291, 294, 299]\n","\n","map50= [[1, 0, 3, 4, 6, 5],\n"," [2, 1, 3, 0, 4, 6],\n"," [2, 3, 4, 1, 6, 0],\n"," [1, 0, 6, 5, 2, 3],\n"," [0, 1, 5, 3, 4, 6],\n"," [3, 2, 1, 5, 0, 4],\n"," [1, 0, 6, 5, 4, 2],\n"," [0, 4, 2, 6, 1, 3],\n"," [0, 3, 6, 1, 2, 5],\n"," [6, 2, 1, 4, 0, 3],\n"," [5, 0, 4, 2, 6, 3],\n"," [1, 0, 4, 3, 5, 2],\n"," [4, 0, 5, 6, 2, 3],\n"," [6, 1, 0, 4, 3, 5],\n"," [5, 6, 0, 3, 1, 2],\n"," [2, 5, 3, 4, 0, 6],\n"," [0, 2, 6, 5, 4, 1],\n"," [0, 3, 2, 1, 4, 6],\n"," [2, 5, 1, 6, 0, 4],\n"," [2, 5, 3, 1, 4, 0],\n"," [3, 4, 1, 6, 2, 0],\n"," [6, 3, 2, 1, 0, 5],\n"," [0, 6, 5, 1, 2, 4],\n"," [0, 6, 1, 3, 5, 4],\n"," [3, 2, 5, 1, 4, 0],\n"," [3, 4, 0, 5, 1, 6],\n"," [2, 5, 4, 0, 6, 1],\n"," [4, 2, 1, 6, 0, 3],\n"," [2, 4, 5, 6, 0, 3],\n"," [0, 5, 6, 2, 3, 1],\n"," [0, 5, 4, 3, 1, 2],\n"," [4, 1, 6, 5, 2, 0],\n"," [5, 3, 1, 0, 2, 6],\n"," [1, 5, 2, 4, 3, 0],\n"," [2, 1, 0, 3, 4, 5],\n"," [2, 0, 4, 5, 6, 3],\n"," [2, 5, 4, 1, 6, 0],\n"," [0, 5, 2, 1, 6, 4],\n"," [0, 5, 2, 4, 3, 1],\n"," [5, 2, 6, 1, 0, 3],\n"," [0, 2, 3, 6, 4, 5],\n"," [4, 5, 6, 2, 3, 0],\n"," [6, 2, 5, 4, 1, 0],\n"," [2, 6, 0, 1, 3, 5],\n"," [0, 1, 4, 5, 6, 2],\n"," [5, 1, 4, 6, 0, 3],\n"," [6, 0, 5, 4, 2, 1],\n"," [6, 4, 1, 0, 3, 2],\n"," [5, 0, 2, 3, 4, 6],\n"," [2, 3, 0, 1, 6, 5]]\n","\n","map_model = [['bart_xsum', 'bart_cnn', 't5', 'pegasus_xsum', 'original', 'gpt2'],\n"," ['bart_base', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'original', 't5'],\n"," ['bart_xsum', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 't5'],\n"," ['bart_cnn', 'bart_base', 'pegasus_xsum', 'gpt2', 't5', 'original'],\n"," ['original', 'bart_xsum', 'pegasus_xsum', 'gpt2', 'bart_cnn', 't5'],\n"," ['t5', 'bart_xsum', 'bart_base', 'bart_cnn', 'original', 'pegasus_xsum'],\n"," ['pegasus_xsum', 'bart_base', 'original', 't5', 'gpt2', 'bart_xsum'],\n"," ['pegasus_xsum', 'original', 'gpt2', 'bart_base', 'bart_xsum', 'bart_cnn'],\n"," ['original', 'bart_xsum', 't5', 'bart_base', 'bart_cnn', 'gpt2'],\n"," ['original', 'bart_xsum', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5'],\n"," ['t5', 'bart_xsum', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n"," ['original', 'bart_base', 't5', 'pegasus_xsum', 'gpt2', 'bart_cnn'],\n"," ['t5', 'original', 'gpt2', 'bart_base', 'bart_cnn', 'pegasus_xsum'],\n"," ['pegasus_xsum', 'bart_cnn', 'gpt2', 'original', 'bart_xsum', 'bart_base'],\n"," ['original', 'bart_base', 'pegasus_xsum', 't5', 'bart_cnn', 'gpt2'],\n"," ['t5', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'gpt2'],\n"," ['bart_xsum', 'bart_base', 'gpt2', 't5', 'original', 'bart_cnn'],\n"," ['bart_xsum', 'gpt2', 'original', 'bart_cnn', 'bart_base', 'pegasus_xsum'],\n"," ['original', 'pegasus_xsum', 'bart_base', 'bart_cnn', 't5', 'bart_xsum'],\n"," ['bart_cnn', 't5', 'gpt2', 'bart_base', 'original', 'pegasus_xsum'],\n"," ['gpt2', 'bart_xsum', 'bart_base', 'bart_cnn', 'original', 't5'],\n"," ['bart_xsum', 'pegasus_xsum', 'bart_cnn', 't5', 'gpt2', 'original'],\n"," ['original', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'bart_base', 'bart_cnn'],\n"," ['original', 'bart_xsum', 'gpt2', 'pegasus_xsum', 'bart_base', 't5'],\n"," ['original', 't5', 'bart_cnn', 'gpt2', 'bart_xsum', 'bart_base'],\n"," ['bart_cnn', 'bart_xsum', 't5', 'original', 'gpt2', 'bart_base'],\n"," ['bart_base', 'bart_cnn', 't5', 'original', 'gpt2', 'pegasus_xsum'],\n"," ['bart_xsum', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n"," ['t5', 'pegasus_xsum', 'original', 'bart_base', 'gpt2', 'bart_cnn'],\n"," ['gpt2', 'original', 'bart_xsum', 'bart_base', 't5', 'pegasus_xsum'],\n"," ['bart_xsum', 'pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'original'],\n"," ['bart_cnn', 'bart_xsum', 'pegasus_xsum', 'gpt2', 'original', 't5'],\n"," ['t5', 'original', 'pegasus_xsum', 'bart_xsum', 'gpt2', 'bart_base'],\n"," ['original', 'bart_cnn', 'bart_xsum', 'gpt2', 'bart_base', 'pegasus_xsum'],\n"," ['bart_cnn', 'bart_base', 'gpt2', 'original', 'pegasus_xsum', 'bart_xsum'],\n"," ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 't5', 'bart_cnn'],\n"," ['t5', 'gpt2', 'original', 'bart_cnn', 'pegasus_xsum', 'bart_base'],\n"," ['t5', 'bart_cnn', 'gpt2', 'pegasus_xsum', 'bart_xsum', 'original'],\n"," ['gpt2', 'bart_xsum', 'bart_cnn', 't5', 'original', 'bart_base'],\n"," ['bart_xsum', 'gpt2', 't5', 'bart_cnn', 'pegasus_xsum', 'original'],\n"," ['bart_base', 'bart_cnn', 't5', 'bart_xsum', 'gpt2', 'original'],\n"," ['bart_cnn', 'bart_xsum', 'pegasus_xsum', 'bart_base', 't5', 'original'],\n"," ['pegasus_xsum', 'bart_xsum', 't5', 'original', 'bart_base', 'bart_cnn'],\n"," ['t5', 'original', 'gpt2', 'bart_base', 'bart_cnn', 'bart_xsum'],\n"," ['bart_xsum', 't5', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n"," ['t5', 'original', 'bart_base', 'bart_xsum', 'pegasus_xsum', 'bart_cnn'],\n"," ['original', 'gpt2', 't5', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n"," ['original', 'bart_base', 'bart_cnn', 'pegasus_xsum', 'gpt2', 'bart_xsum'],\n"," ['gpt2', 'bart_xsum', 'original', 't5', 'bart_base', 'pegasus_xsum'],\n"," ['gpt2', 't5', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n"," ['bart_base', 'original', 'bart_xsum', 't5', 'pegasus_xsum', 'gpt2'],\n"," ['bart_cnn', 'bart_base', 'bart_xsum', 'original', 't5', 'pegasus_xsum'],\n"," ['bart_cnn', 'bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n"," ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'bart_xsum'],\n"," ['original', 'bart_base', 'gpt2', 'bart_xsum', 't5', 'pegasus_xsum'],\n"," ['bart_xsum', 'bart_cnn', 'bart_base', 'gpt2', 'original', 't5'],\n"," ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 't5', 'bart_cnn'],\n"," ['original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'bart_xsum'],\n"," ['original', 'bart_xsum', 'pegasus_xsum', 'bart_base', 'bart_cnn', 'gpt2'],\n"," ['pegasus_xsum', 'bart_cnn', 'bart_base', 't5', 'original', 'bart_xsum'],\n"," ['gpt2', 'original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_xsum'],\n"," ['bart_base', 'original', 't5', 'bart_xsum', 'gpt2', 'bart_cnn'],\n"," ['t5', 'original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n"," ['pegasus_xsum', 'bart_base', 'original', 't5', 'bart_xsum', 'gpt2'],\n"," ['gpt2', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'bart_cnn'],\n"," ['bart_cnn', 'gpt2', 'bart_xsum', 't5', 'original', 'pegasus_xsum'],\n"," ['original', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5', 'bart_base'],\n"," ['original', 'bart_xsum', 'bart_cnn', 'bart_base', 't5', 'pegasus_xsum'],\n"," ['bart_cnn', 'gpt2', 'bart_base', 'pegasus_xsum', 'original', 't5'],\n"," ['bart_cnn', 'gpt2', 'bart_xsum', 'bart_base', 't5', 'original'],\n"," ['bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'bart_cnn', 'original'],\n"," ['pegasus_xsum', 'bart_xsum', 'bart_cnn', 'bart_base', 'original', 'gpt2'],\n"," ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 'bart_cnn', 't5'],\n"," ['original', 'pegasus_xsum', 'bart_base', 'bart_xsum', 'gpt2', 't5'],\n"," ['bart_xsum', 'bart_cnn', 'gpt2', 'bart_base', 't5', 'original'],\n"," ['bart_xsum', 't5', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n"," ['bart_cnn', 'gpt2', 't5', 'original', 'pegasus_xsum', 'bart_base'],\n"," ['t5', 'bart_cnn', 'bart_base', 'pegasus_xsum', 'original', 'bart_xsum'],\n"," ['bart_cnn', 't5', 'gpt2', 'pegasus_xsum', 'original', 'bart_xsum'],\n"," ['original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'bart_base'],\n"," ['original', 'gpt2', 't5', 'bart_xsum', 'bart_base', 'bart_cnn'],\n"," ['t5', 'bart_base', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'original'],\n"," ['gpt2', 'bart_xsum', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n"," ['bart_base', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'original'],\n"," ['bart_cnn', 'bart_base', 'original', 'bart_xsum', 't5', 'gpt2'],\n"," ['bart_cnn', 'original', 't5', 'gpt2', 'pegasus_xsum', 'bart_xsum'],\n"," ['bart_cnn', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n"," ['original', 'gpt2', 'bart_cnn', 'bart_base', 'pegasus_xsum', 't5'],\n"," ['original', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'bart_base'],\n"," ['gpt2', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 'bart_xsum'],\n"," ['original', 'bart_cnn', 'bart_xsum', 'pegasus_xsum', 't5', 'gpt2'],\n"," ['t5', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'original'],\n"," ['pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'bart_base', 'original'],\n"," ['bart_cnn', 'pegasus_xsum', 'original', 'bart_base', 'bart_xsum', 'gpt2'],\n"," ['original', 'bart_base', 't5', 'gpt2', 'pegasus_xsum', 'bart_cnn'],\n"," ['gpt2', 'bart_base', 't5', 'pegasus_xsum', 'original', 'bart_xsum'],\n"," ['pegasus_xsum', 'original', 'gpt2', 't5', 'bart_cnn', 'bart_base'],\n"," ['pegasus_xsum', 't5', 'bart_base', 'original', 'bart_xsum', 'bart_cnn'],\n"," ['gpt2', 'original', 'bart_cnn', 'bart_xsum', 't5', 'pegasus_xsum'],\n"," ['bart_cnn', 'bart_xsum', 'original', 'bart_base', 'pegasus_xsum', 'gpt2'],\n"," ['bart_base', 'original', 'bart_xsum', 't5', 'pegasus_xsum', 'gpt2'],\n"," ['bart_cnn', 'bart_base', 'bart_xsum', 'original', 't5', 'pegasus_xsum'],\n"," ['bart_cnn', 'bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n"," ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'bart_xsum'],\n"," ['original', 'bart_base', 'gpt2', 'bart_xsum', 't5', 'pegasus_xsum'],\n"," ['bart_xsum', 'bart_cnn', 'bart_base', 'gpt2', 'original', 't5'],\n"," ['bart_base', 'original', 'pegasus_xsum', 'gpt2', 't5', 'bart_cnn'],\n"," ['original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'bart_xsum'],\n"," ['original', 'bart_xsum', 'pegasus_xsum', 'bart_base', 'bart_cnn', 'gpt2'],\n"," ['pegasus_xsum', 'bart_cnn', 'bart_base', 't5', 'original', 'bart_xsum'],\n"," ['gpt2', 'original', 't5', 'bart_cnn', 'pegasus_xsum', 'bart_xsum'],\n"," ['bart_base', 'original', 't5', 'bart_xsum', 'gpt2', 'bart_cnn'],\n"," ['t5', 'original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum'],\n"," ['pegasus_xsum', 'bart_base', 'original', 't5', 'bart_xsum', 'gpt2'],\n"," ['gpt2', 'pegasus_xsum', 'original', 'bart_xsum', 'bart_base', 'bart_cnn'],\n"," ['bart_cnn', 'gpt2', 'bart_xsum', 't5', 'original', 'pegasus_xsum'],\n"," ['original', 'bart_cnn', 'pegasus_xsum', 'gpt2', 't5', 'bart_base'],\n"," ['original', 'bart_xsum', 'bart_cnn', 'bart_base', 't5', 'pegasus_xsum'],\n"," ['bart_cnn', 'gpt2', 'bart_base', 'pegasus_xsum', 'original', 't5'],\n"," ['bart_cnn', 'gpt2', 'bart_xsum', 'bart_base', 't5', 'original'],\n"," ['bart_xsum', 't5', 'bart_base', 'pegasus_xsum', 'bart_cnn', 'original'],\n"," ['pegasus_xsum', 'bart_xsum', 'bart_cnn', 'bart_base', 'original', 'gpt2'],\n"," ['original', 'pegasus_xsum', 'gpt2', 'bart_base', 'bart_cnn', 't5'],\n"," ['original', 'pegasus_xsum', 'bart_base', 'bart_xsum', 'gpt2', 't5'],\n"," ['bart_xsum', 'bart_cnn', 'gpt2', 'bart_base', 't5', 'original'],\n"," ['bart_xsum', 't5', 'original', 'gpt2', 'bart_base', 'pegasus_xsum'],\n"," ['bart_cnn', 'gpt2', 't5', 'original', 'pegasus_xsum', 'bart_base'],\n"," ['t5', 'bart_cnn', 'bart_base', 'pegasus_xsum', 'original', 'bart_xsum'],\n"," ['bart_cnn', 't5', 'gpt2', 'pegasus_xsum', 'original', 'bart_xsum'],\n"," ['original', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'bart_base'],\n"," ['original', 'gpt2', 't5', 'bart_xsum', 'bart_base', 'bart_cnn'],\n"," ['t5', 'bart_base', 'pegasus_xsum', 'gpt2', 'bart_cnn', 'original'],\n"," ['gpt2', 'bart_xsum', 'bart_base', 'original', 'bart_cnn', 'pegasus_xsum'],\n"," ['bart_base', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'original'],\n"," ['bart_cnn', 'bart_base', 'original', 'bart_xsum', 't5', 'gpt2'],\n"," ['bart_cnn', 'original', 't5', 'gpt2', 'pegasus_xsum', 'bart_xsum'],\n"," ['bart_cnn', 'gpt2', 't5', 'bart_base', 'pegasus_xsum', 'original'],\n"," ['original', 'gpt2', 'bart_cnn', 'bart_base', 'pegasus_xsum', 't5'],\n"," ['original', 'gpt2', 'bart_cnn', 't5', 'bart_xsum', 'bart_base'],\n"," ['gpt2', 'bart_cnn', 'pegasus_xsum', 'bart_base', 'original', 'bart_xsum'],\n"," ['original', 'bart_cnn', 'bart_xsum', 'pegasus_xsum', 't5', 'gpt2'],\n"," ['t5', 'gpt2', 'pegasus_xsum', 'bart_cnn', 'bart_xsum', 'original'],\n"," ['pegasus_xsum', 'bart_cnn', 'gpt2', 't5', 'bart_base', 'original'],\n"," ['bart_cnn', 'pegasus_xsum', 'original', 'bart_base', 'bart_xsum', 'gpt2'],\n"," ['original', 'bart_base', 't5', 'gpt2', 'pegasus_xsum', 'bart_cnn'],\n"," ['gpt2', 'bart_base', 't5', 'pegasus_xsum', 'original', 'bart_xsum'],\n"," ['pegasus_xsum', 'original', 'gpt2', 't5', 'bart_cnn', 'bart_base'],\n"," ['pegasus_xsum', 't5', 'bart_base', 'original', 'bart_xsum', 'bart_cnn'],\n"," ['gpt2', 'original', 'bart_cnn', 'bart_xsum', 't5', 'pegasus_xsum'],\n"," ['bart_cnn', 'bart_xsum', 'original', 'bart_base', 'pegasus_xsum', 'gpt2']]\n","\n","\n","def displaysmaples(samples):\n","  i = 0\n","  titles = samples.title.to_list()\n","  abstract = samples.abstract.to_list()\n","  bart_base = samples.bart_base.to_list()\n","  bart_cnn = samples.bart_cnn.to_list()\n","  bart_xsum = samples.bart_xsum.to_list()\n","  t5_small = samples.t5_small.to_list()\n","  gpt2 = samples.gpt2.to_list()\n","  pegasus_xsum = samples.pegasus_xsum.to_list()\n","\n","  for t, a, t1, t2, t3, t4, t5, t6 in zip(titles, abstract, bart_base, bart_cnn, bart_xsum, t5_small, gpt2, pegasus_xsum):\n","    print(\"original title: \", t)\n","    print(\"abstract: \", a)\n","    print(\"from bart_base: \")\n","    gt1 = t1.split(\"<TITLE>\")[1:]\n","    for gt in gt1:\n","      print(gt)\n","    print(\"\\n\")\n","    print(\"from bart_cnn: \")\n","    gt1 = t2.split(\"<TITLE>\")[1:]\n","    for gt in gt1:\n","      print(gt)\n","    print(\"\\n\")\n","    print(\"from bart_xsum: \")\n","    gt1 = t3.split(\"<TITLE>\")[1:]\n","    for gt in gt1:\n","      print(gt)\n","    print(\"\\n\")\n","    print(\"from t5_small: \")\n","    gt1 = t4.split(\"<TITLE>\")[1:]\n","    for gt in gt1:\n","      print(gt)\n","    print(\"\\n\")\n","    print(\"from gpt2: \")\n","    gt1 = t5.split(\"<TITLE>\")[1:]\n","    for gt in gt1:\n","      print(gt)\n","    print(\"\\n\")\n","    print(\"from pegasus_xsum: \")\n","    gt1 = t6.split(\"<TITLE>\")[1:]\n","    for gt in gt1:\n","      print(gt)\n","    print(\"\\n\")\n","    i = i + 1\n","\n","    if i >= 20:\n","      break\n","\n","import numpy as np\n","# index von Optionen der human evaluation, 20 - 99 samples und 100-150(von index 0 bis 49)\n","# [4, 3, 5, 2, 0, 1] bedeutet \n","#1.op aus t5_samall, \n","#2.op aus bart_xsum,\n","#3.op aus gpt2,\n","#4.op aus bart_cnn,\n","#5.op original,\n","#6.op aus bart_base,\n","\n","def getindex():\n","  result = []\n","  for i in range(80):\n","    col = np.arange(1,7)\n","    np.random.shuffle(col)\n","    col = col.tolist()[:-1]\n","    col.append(0)\n","    np.random.shuffle(col)\n","    result.append(col)\n","  return result\n","\n","\n","def sampler(samples, map20, start, end):\n","  i = start\n","  titles = samples.title.to_list()[start: end]\n","  abstract = samples.abstract.to_list()[start: end]\n","  bart_base = samples.bart_base.to_list()[start: end]\n","  bart_cnn = samples.bart_cnn.to_list()[start: end]\n","  bart_xsum = samples.bart_xsum.to_list()[start: end]\n","  t5_small = samples.t5_small.to_list()[start: end]\n","  gpt2 = samples.gpt2.to_list()[start: end]\n","  pegasus_xsum = samples.pegasus_xsum.to_list()[start: end]\n","\n","  tmap = []\n","  for t, a, t1, t2, t3, t4, t5, t6 in zip(titles, abstract, bart_base, bart_cnn, bart_xsum, t5_small, gpt2, pegasus_xsum):\n","    gt1 = t1.split(\"<TITLE>\")[1:][0]\n","    gt2 = t2.split(\"<TITLE>\")[1:][0]\n","    gt3 = t3.split(\"<TITLE>\")[1:][0]\n","    gt4 = t4.split(\"<TITLE>\")[1:][0]\n","    gt5 = t5.split(\"<TITLE>\")[1:][0]\n","    gt6 = t6.split(\"<TITLE>\")[1:][0]\n","    col = [t, gt1, gt2, gt3, gt4, gt5, gt6]\n","    rcol = [col[map20[i][j]] for j in range(6)]\n","    rcol.append(a)\n","    tmap.append(rcol)\n","    i = i + 1\n","\n","  return tmap\n","def map2model(ls):\n","  r = []\n","  for i in ls:\n","    if i == 0:\n","      r.append(\"original\")\n","    if i == 1:\n","      r.append(\"bart_base\")\n","    if i == 2:\n","      r.append(\"bart_cnn\")\n","    if i == 3:\n","      r.append(\"bart_xsum\")\n","    if i == 4:\n","      r.append(\"t5\")\n","    if i == 5:\n","      r.append(\"gpt2\")\n","    if i == 6:\n","      r.append(\"pegasus_xsum\")\n","  return r\n","\n","def map2index(ls):\n","  r = []\n","  for i in ls:\n","    if i == \"original\":\n","      r.append(0)\n","    if i == \"bart_base\":\n","      r.append(1)\n","    if i == \"bart_cnn\":\n","      r.append(2)\n","    if i == \"bart_xsum\":\n","      r.append(3)\n","    if i == \"t5\":\n","      r.append(4)\n","    if i == \"gpt2\":\n","      r.append(5)\n","    if i == \"pegasus_xsum\":\n","      r.append(6)\n","  return r\n","\n","def map2modelAM(ls):\n","  r = []\n","  for i in ls:\n","    if i == 0:\n","      r.append(\"bart_base\")\n","    if i == 1:\n","      r.append(\"bart_cnn\")\n","    if i == 2:\n","      r.append(\"bart_xsum\")\n","    if i == 3:\n","      r.append(\"t5\")\n","    if i == 4:\n","      r.append(\"gpt2\")\n","    if i == 5:\n","      r.append(\"pegasus_xsum\")\n","  return r\n","\n","\n","import numpy as np\n","# index von Optionen der human evaluation, 150 - 200 samples\n","# [4, 3, 5, 2, 0, 1] bedeutet \n","#1.op aus t5_samall, \n","#2.op aus bart_xsum,\n","#3.op aus gpt2,\n","#4.op aus bart_cnn,\n","#5.op original,\n","#6.op aus bart_base,\n","\n","def getindex():\n","  result = []\n","  for i in range(50):\n","    col = np.arange(1,7)\n","    np.random.shuffle(col)\n","    col = col.tolist()[:-1]\n","    col.append(0)\n","    np.random.shuffle(col)\n","    result.append(col)\n","  return result\n","\n","#map für Teinehmer\n","import numpy as np\n","\n","def getTmap():\n","  i = 0\n","  b = False\n","  map30 = []\n","  counters = np.zeros(20).tolist()\n","  while not b:\n","    personMap = np.arange(20)\n","    np.random.shuffle(personMap)\n","    personMap = personMap[:10]\n","    i0 = personMap[0]\n","    i1 = personMap[1]\n","\n","    if (counters[i0] < 3) and (counters[i1] < 3):\n","      counters[i0] = counters[i0] + 1\n","      counters[i1] = counters[i1] + 1\n","      map30.append(personMap)\n","    tb = True\n","    for t in counters:\n","      tb = tb and (t == 3)\n","    b = tb\n","    if b == True:\n","      print(\"terminiert at:\", counters)\n","  return map30\n","\n"],"metadata":{"cellView":"form","id":"nit7GMLqFRQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","def setup_seed(seed):\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  np.random.seed(seed)\n","  torch.backends.cudnn.deteministic = True\n","\n","MODEL_OUT_DIR = '/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/'\n","\n","## Model Configurations\n","p={\n","    'max_len': 512,\n","    'batch_size': 6,\n","    'lr' : 4.063769241800226e-05,\n","    'epochs': 12,\n","    'dropout': 0.5,\n","    'num_threads' : 1,\n","    #'model_name' : 'allenai/scibert_scivocab_uncased',\n","    'model_name' : 'bert-base-uncased',\n","    'do_train' : True,\n","    'random_seed': 24\n","}\n","\n","for s in p.values():\n","  MODEL_OUT_DIR += '_' + str(s).replace('/', '_', 1) \n","\n","setup_seed(p['random_seed'])\n","\n","\n","\n"],"metadata":{"id":"J7sZxb6Frmpj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","# **Fine Tuning**\n","\n","\n","---\n"],"metadata":{"id":"0woEoArFGmq8"}},{"cell_type":"code","source":["#@title dataset { form-width: \"10%\" }\n","from datasets import Dataset\n","class Excerpt_Dataset(Dataset):\n","\n","    def __init__(self, data, maxlen, tokenizer): \n","        #Store the contents of the file in a pandas dataframe\n","        self.df = data.reset_index()\n","        #Initialize the tokenizer for the desired transformer model\n","        self.tokenizer = tokenizer\n","        #Maximum length of the tokens list to keep all the sequences of fixed size\n","        self.maxlen = maxlen\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):    \n","        #Select the sentence and label at the specified index in the data frame\n","        excerpt = self.df.loc[index, 'excerpt']\n","        try:\n","            target = self.df.loc[index, 'target']\n","        except:\n","            target = 0.0\n","        #identifier = self.df.loc[index, 'id']\n","        #Preprocess the text to be suitable for the transformer\n","        tokens = self.tokenizer.tokenize(excerpt) \n","        tokens = ['[CLS]'] + tokens + ['[SEP]'] \n","        if len(tokens) < self.maxlen:\n","            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \n","        else:\n","            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n","        #Obtain the indices of the tokens in the BERT Vocabulary\n","        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \n","        input_ids = torch.tensor(input_ids) \n","        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n","        attention_mask = (input_ids != 0).long()\n","        \n","        target = torch.tensor([target], dtype=torch.float32)\n","        \n","        return input_ids, attention_mask, target"],"metadata":{"id":"GQnEmWjNsJJK","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title init model { form-width: \"10%\" }\n","from transformers import BertModel,BertPreTrainedModel\n","import torch.nn as nn\n","'''\n","class BertRegresser(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.bert = BertModel(config)\n","        #The output layer that takes the [CLS] representation and gives an output\n","        self.cls_layer1 = nn.Linear(config.hidden_size,128)\n","        self.relu1 = nn.ReLU()\n","        self.ff1 = nn.Linear(128,128)\n","        self.tanh1 = nn.Tanh()\n","        self.ff2 = nn.Linear(128,1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        #Feed the input to Bert model to obtain contextualized representations\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        #Obtain the representations of [CLS] heads\n","        logits = outputs.last_hidden_state[:,0,:]\n","        output = self.cls_layer1(logits)\n","        output = self.relu1(output)\n","        output = self.ff1(output)\n","        output = self.tanh1(output)\n","        output = self.ff2(output)\n","        return output\n","'''\n","class BertRegresser(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.bert = BertModel(config)\n","        #The output layer that takes the [CLS] representation and gives an output\n","        self.regressor = nn.Sequential(\n","            nn.Dropout(p['dropout']),\n","            nn.Linear(768, 1))\n","\n","    def forward(self, input_ids, attention_mask):\n","        #Feed the input to Bert model to obtain contextualized representations\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        #Obtain the representations of [CLS] heads\n","        logits = outputs[1]\n","        output = self.regressor(logits)\n","        \n","        return output\n","\n"],"metadata":{"id":"SmOlriORubv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title train, eval fucnction { form-width: \"10%\" }\n","\n","from scipy import stats\n","\n","def evaluate(model, criterion, dataloader, device):\n","    model.eval()\n","    mean_acc, mean_loss, count = 0, 0, 0\n","    preds = []\n","    lst_label = []\n","    with torch.no_grad():\n","        for input_ids, attention_mask, target in (dataloader):\n","            \n","            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n","            output = model(input_ids, attention_mask)\n","            preds += output\n","            lst_label += target\n","            mean_loss += criterion(output, target.type_as(output)).item()\n","#             mean_err += get_rmse(output, target)\n","            count += 1\n","        predss = np.array([x.cpu().data.numpy().tolist() for x in preds]).squeeze()\n","        lst_labels = np.array([x.cpu().data.numpy().tolist() for x in lst_label]).squeeze()\n","        corr = stats.spearmanr(predss, lst_labels)\n","    return corr[0] #mean_loss/count\n","from tqdm import trange \n","def train(model, criterion, optimizer, train_loader, val_loader, epochs, device):\n","    best_acc = 0\n","    for epoch in trange(epochs, desc=\"Epoch\"):\n","        model.train()\n","        train_loss = 0\n","        for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):\n","            optimizer.zero_grad()  \n","            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n","            \n","            output = model(input_ids=input_ids, attention_mask=attention_mask)\n","            loss = criterion(output, target.type_as(output))\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","        \n","        print(f\"Training loss is {train_loss/len(train_loader)}\")\n","        val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n","        print(\"Epoch {} complete! Correlations : {}\".format(epoch, val_loss))\n","from transformers import AutoConfig, AutoTokenizer\n","import torch.nn as nn\n","from torch import optim\n","\n","\n","## Configuration loaded from AutoConfig \n","aconfig = AutoConfig.from_pretrained(p['model_name'])\n","## Tokenizer loaded from AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(p['model_name'])\n","## Creating the model from the desired transformer model\n","model = BertRegresser.from_pretrained(p['model_name'], config=aconfig)\n","\n","\n","#frezze all layers except regression head\n","'''\n","unfreeze_layers  = ['bert.pooler', 'cls_layer1.', 'ff1.', 'ff2']\n","\n","for name, params in model.named_parameters():\n","  params.requires_grad = False\n","  for ele in unfreeze_layers:\n","    if ele in name:\n","      params.requires_grad = True\n","      break\n","\n","for name, params in model.named_parameters():\n","  if params.requires_grad:\n","    print(name, params.size())\n","'''\n","\n","unfreeze_layers = ['bert.pooler', 'regressor.1']\n","for name, params in model.named_parameters():\n","  params.requires_grad = False\n","  for ele in unfreeze_layers:\n","    if ele in name:\n","      params.requires_grad = True\n","      break\n","\n","for name, params in model.named_parameters():\n","  if params.requires_grad:\n","    print(name, params.size())\n","\n","## GPU or CPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","## Putting model to device\n","model = model.to(device)\n","## Takes as the input the logits of the positive class and computes the binary cross-entropy \n","# criterion = nn.BCEWithLogitsLoss()\n","criterion = nn.MSELoss()\n","## Optimizer\n","optimizer = optim.Adam(params=model.parameters(), lr=p['lr'])\n","\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dGvKcMnwJ4G","executionInfo":{"status":"ok","timestamp":1651569451520,"user_tz":-120,"elapsed":3499,"user":{"displayName":"yichen xie","userId":"05370341903943803742"}},"outputId":"50de4e6b-0c14-403c-a625-6112e4ce2f1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertRegresser: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertRegresser from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertRegresser from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertRegresser were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['regressor.1.weight', 'regressor.1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["bert.pooler.dense.weight torch.Size([768, 768])\n","bert.pooler.dense.bias torch.Size([768])\n","regressor.1.weight torch.Size([1, 768])\n","regressor.1.bias torch.Size([1])\n"]}]},{"cell_type":"code","source":["\n"],"metadata":{"id":"Rv5CnpojwWYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title load data { form-width: \"10%\" }\n","\n","import pandas as pd\n","\n","text_map = pd.read_csv('/content/drive/MyDrive/Thesis/output/140_annotations.csv', index_col=0)\n","scores = pd.read_csv('/content/drive/MyDrive/Thesis/output/140_humanannotation_withoutunannotated.csv', index_col=0)\n","\n","abstract_df =text_map[['abstract']]\n","title_df = text_map[['title', 'bart_base', 'bart_cnn', 'bart_xsum', 't5_small', 'gpt2', 'pegasus_xsum']]\n","abstract_np = abstract_df.to_numpy()\n","scores_np = scores.to_numpy()\n","title_np = title_df.to_numpy()\n","idx_map = [map2index(row) for row in map_model[:140]]\n","title_np_picked = np.array([np.take(row1, np.sort(row2)) for row1, row2 in zip(title_np, idx_map)])\n","score_np_picked = np.array([np.take(row1, np.sort(row2)) for row1, row2 in zip(scores_np, idx_map)])\n","\n","pairs_np_picked = np.concatenate([abstract_np, title_np_picked,score_np_picked], axis=1)\n","pairs_np_picked_shuffled = np.random.permutation(pairs_np_picked)\n","\n","abstracs = pairs_np_picked_shuffled[:,:1]\n","titles = pairs_np_picked_shuffled[:,1:7]\n","scores = pairs_np_picked_shuffled[:,7:].astype(float)\n","scores = np.around(scores, 4)\n","\n","lst = []\n","\n","for ab, row1, row2 in zip(abstracs, titles, scores):\n","  assert len(row1) == len(row2)\n","  assert len(row1) == 6\n","  for t, s in zip(row1, row2):\n","    \n","    if np.isnan(s):\n","      print('found nan score')\n","    if t=='':\n","      print('found empty title')\n","    lst.append([ab[0] + '[SEP]' + t, s])\n","df = pd.DataFrame(np.array(lst))\n","\n","df.columns = ['excerpt', 'target']\n","#dataframe = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","dftrain = df[:600].reset_index(drop=True)\n","dfdev = df[600:660].reset_index(drop=True)\n","dftest = df[660:].reset_index(drop=True)\n","dftrain.to_csv('/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/robust_test/sciBert_shuffled_train.csv')\n","dfdev.to_csv('/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/robust_test/sciBert_shuffled_dev.csv')\n","dftest.to_csv('/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/robust_test/sciBert_shuffled_test.csv')\n","\n","dftrain = pd.read_csv('/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/robust_test/sciBert_shuffled_train.csv', index_col=0)\n","dfdev = pd.read_csv('/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/robust_test/sciBert_shuffled_dev.csv', index_col=0)\n","dftest = pd.read_csv('/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/robust_test/sciBert_shuffled_test.csv', index_col=0)\n","\n","\n","\n","## Training Dataset\n","train_set = Excerpt_Dataset(data=dftrain, maxlen=p['max_len'], tokenizer=tokenizer)\n","dev_set = Excerpt_Dataset(data=dfdev, maxlen=p['max_len'], tokenizer=tokenizer)\n","test_set = Excerpt_Dataset(data=dftest, maxlen=p['max_len'], tokenizer=tokenizer)\n","\n","## Data Loaders\n","train_loader = DataLoader(dataset=train_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)\n","dev_loader = DataLoader(dataset=dev_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)\n","test_loader = DataLoader(dataset=test_set, batch_size=p['batch_size'], num_workers=p['num_threads'], shuffle=True)\n"],"metadata":{"id":"9lrD5eHbFIKk","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" if p['do_train']:\n","  train(model=model, \n","      criterion=criterion,\n","      optimizer=optimizer, \n","      train_loader=train_loader,\n","      val_loader=dev_loader,\n","      epochs = p['epochs'],\n","     device = device)\n"],"metadata":{"id":"qMee8sQz2NV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#torch.save(model.state_dict(), MODEL_OUT_DIR + '.model')\n","\n","#load model\n","model_state, optimizer_state = torch.load(os.path.join('/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/ray_results/runner_2022-04-29_16-44-10/runner_980f0_00002_2_lr=4.0638e-05_2022-04-29_17-04-24/checkpoint_000017', \"checkpoint\"))\n","model.load_state_dict(model_state)\n","\n","#model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/output/sciBert_Kaggle/sciBert.model\"))\n","#model.eval()"],"metadata":{"id":"Wg9AtLg-axHn","executionInfo":{"status":"ok","timestamp":1651569528674,"user_tz":-120,"elapsed":808,"user":{"displayName":"yichen xie","userId":"05370341903943803742"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5721842f-ce46-4f0d-d0db-d14357090089"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["#@title prediction function\n","def predict(model, dataloader, device):\n","    predicted_label = []\n","    actual_label = []\n","    with torch.no_grad():\n","        for input_ids, attention_mask, target in (dataloader):\n","            \n","            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n","            output = model(input_ids, attention_mask)\n","                        \n","            predicted_label += output\n","            actual_label += target\n","            \n","    return predicted_label, actual_label"],"metadata":{"id":"iATEEkP7Narl","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","# **Display Correlation**\n","\n","\n","---\n"],"metadata":{"id":"q_iKX2VOG7ti"}},{"cell_type":"code","source":["output,GS_label = predict(model, train_loader, device)\n","cpu_output = np.array([x.cpu().data.numpy() for x in output]).squeeze()\n","cpu_target = np.array([x.cpu().data.numpy() for x in GS_label]).squeeze()\n","stats.spearmanr(cpu_output, cpu_target)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eN5JZ9ElGABX","executionInfo":{"status":"ok","timestamp":1651580282977,"user_tz":-120,"elapsed":11207,"user":{"displayName":"yichen xie","userId":"05370341903943803742"}},"outputId":"dc8ffb29-bbaf-4ffb-c332-1c067da3d7ce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3550826140350425"]},"metadata":{},"execution_count":250}]},{"cell_type":"code","source":["dev_output,dev_GS_label = predict(model, dev_loader, device)\n","cpu_dev_output = np.array([x.cpu().data.numpy() for x in dev_output]).squeeze()\n","cpu_dev_target = np.array([x.cpu().data.numpy() for x in dev_GS_label]).squeeze()\n","stats.spearmanr(cpu_dev_output, cpu_dev_target)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEV-es5KpREu","executionInfo":{"status":"ok","timestamp":1651581789481,"user_tz":-120,"elapsed":1447,"user":{"displayName":"yichen xie","userId":"05370341903943803742"}},"outputId":"e74cb0f1-a225-415a-eb5e-9fb0baff179a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3285384665862456"]},"metadata":{},"execution_count":258}]},{"cell_type":"code","source":["test_output,test_GS_label = predict(model, test_loader, device)\n","cpu_test_output = np.array([x.cpu().data.numpy() for x in test_output]).squeeze()\n","cpu_test_target = np.array([x.cpu().data.numpy() for x in test_GS_label]).squeeze()\n","stats.spearmanr(cpu_test_output, cpu_test_target)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcFR76sa53F7","executionInfo":{"status":"ok","timestamp":1651581792952,"user_tz":-120,"elapsed":3474,"user":{"displayName":"yichen xie","userId":"05370341903943803742"}},"outputId":"946e8150-82dd-4b0c-bf29-e54b648c7f98"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4278892603419838"]},"metadata":{},"execution_count":259}]},{"cell_type":"code","source":["dev_output,dev_GS_label = predict(model, dev_loader, device)\n","cpu_dev_output = np.array([x.cpu().data.numpy() for x in dev_output]).squeeze()\n","cpu_dev_target = np.array([x.cpu().data.numpy() for x in dev_GS_label]).squeeze()\n","stats.spearmanr(cpu_dev_output, cpu_dev_target)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lfX7ty-34BEo","executionInfo":{"status":"ok","timestamp":1651581812420,"user_tz":-120,"elapsed":1435,"user":{"displayName":"yichen xie","userId":"05370341903943803742"}},"outputId":"e731ea0f-e41d-4de9-82a9-a38dd3d12d77"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.23018991165838504"]},"metadata":{},"execution_count":262}]},{"cell_type":"code","source":["test_output,test_GS_label = predict(model, test_loader, device)\n","cpu_test_output = np.array([x.cpu().data.numpy() for x in test_output]).squeeze()\n","cpu_test_target = np.array([x.cpu().data.numpy() for x in test_GS_label]).squeeze()\n","stats.spearmanr(cpu_test_output, cpu_test_target)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l70-oVBm8oP7","executionInfo":{"status":"ok","timestamp":1651581818561,"user_tz":-120,"elapsed":3591,"user":{"displayName":"yichen xie","userId":"05370341903943803742"}},"outputId":"78cdc256-f6f2-4acf-9a52-b63e5b40c098"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3720043835305109"]},"metadata":{},"execution_count":263}]}]}